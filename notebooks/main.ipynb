{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a106fb84",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Small LLM with LoRA for Automated Paper Review Insights (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27cdf0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.30.0 (from -r requirements.txt (line 1))\n",
      "  Using cached transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\n",
      "Requirement already satisfied: datasets==2.15.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.15.0)\n",
      "Collecting torch==2.8.0 (from -r requirements.txt (line 3))\n",
      "  Using cached torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting bitsandbytes==0.38.1 (from -r requirements.txt (line 4))\n",
      "  Using cached bitsandbytes-0.38.1-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting peft==0.4.0 (from -r requirements.txt (line 5))\n",
      "  Using cached peft-0.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tqdm==4.65.0 (from -r requirements.txt (line 6))\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl.metadata (56 kB)\n",
      "Requirement already satisfied: scikit-learn in /home/disi/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/disi/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (1.15.3)\n",
      "Requirement already satisfied: bert-score==0.3.13 in /home/disi/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (0.3.13)\n",
      "Collecting pandas==2.0.1 (from -r requirements.txt (line 10))\n",
      "  Using cached pandas-2.0.1-cp312-cp312-linux_x86_64.whl\n",
      "Collecting openpyxl==3.1.2 (from -r requirements.txt (line 11))\n",
      "  Using cached openpyxl-3.1.2-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: huggingface-hub in /home/disi/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (0.32.2)\n",
      "Requirement already satisfied: filelock in /home/disi/miniconda3/lib/python3.12/site-packages (from transformers==4.30.0->-r requirements.txt (line 1)) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/disi/miniconda3/lib/python3.12/site-packages (from transformers==4.30.0->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from transformers==4.30.0->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/disi/miniconda3/lib/python3.12/site-packages (from transformers==4.30.0->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/disi/miniconda3/lib/python3.12/site-packages (from transformers==4.30.0->-r requirements.txt (line 1)) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/disi/miniconda3/lib/python3.12/site-packages (from transformers==4.30.0->-r requirements.txt (line 1)) (2.32.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.0->-r requirements.txt (line 1))\n",
      "  Using cached tokenizers-0.13.3.tar.gz (314 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /home/disi/miniconda3/lib/python3.12/site-packages (from transformers==4.30.0->-r requirements.txt (line 1)) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from datasets==2.15.0->-r requirements.txt (line 2)) (20.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/disi/miniconda3/lib/python3.12/site-packages (from datasets==2.15.0->-r requirements.txt (line 2)) (0.7)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from datasets==2.15.0->-r requirements.txt (line 2)) (0.3.7)\n",
      "Requirement already satisfied: xxhash in /home/disi/miniconda3/lib/python3.12/site-packages (from datasets==2.15.0->-r requirements.txt (line 2)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/disi/miniconda3/lib/python3.12/site-packages (from datasets==2.15.0->-r requirements.txt (line 2)) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.15.0->-r requirements.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/disi/miniconda3/lib/python3.12/site-packages (from datasets==2.15.0->-r requirements.txt (line 2)) (3.12.14)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from torch==2.8.0->-r requirements.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /home/disi/miniconda3/lib/python3.12/site-packages (from torch==2.8.0->-r requirements.txt (line 3)) (75.8.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/disi/miniconda3/lib/python3.12/site-packages (from torch==2.8.0->-r requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/disi/miniconda3/lib/python3.12/site-packages (from torch==2.8.0->-r requirements.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/disi/miniconda3/lib/python3.12/site-packages (from torch==2.8.0->-r requirements.txt (line 3)) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.8.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.8.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.8.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch==2.8.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.8.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.8.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.8.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.8.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.8.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch==2.8.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch==2.8.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.8.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.8.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.8.0->-r requirements.txt (line 3))\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch==2.8.0->-r requirements.txt (line 3))\n",
      "  Using cached triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: psutil in /home/disi/miniconda3/lib/python3.12/site-packages (from peft==0.4.0->-r requirements.txt (line 5)) (7.0.0)\n",
      "Requirement already satisfied: accelerate in /home/disi/miniconda3/lib/python3.12/site-packages (from peft==0.4.0->-r requirements.txt (line 5)) (1.9.0)\n",
      "Requirement already satisfied: matplotlib in /home/disi/miniconda3/lib/python3.12/site-packages (from bert-score==0.3.13->-r requirements.txt (line 9)) (3.10.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/disi/miniconda3/lib/python3.12/site-packages (from pandas==2.0.1->-r requirements.txt (line 10)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/disi/miniconda3/lib/python3.12/site-packages (from pandas==2.0.1->-r requirements.txt (line 10)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/disi/miniconda3/lib/python3.12/site-packages (from pandas==2.0.1->-r requirements.txt (line 10)) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in /home/disi/miniconda3/lib/python3.12/site-packages (from openpyxl==3.1.2->-r requirements.txt (line 11)) (2.0.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/disi/miniconda3/lib/python3.12/site-packages (from huggingface-hub->-r requirements.txt (line 12)) (1.1.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 7)) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 7)) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 2)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/disi/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 2)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/disi/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 2)) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 2)) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.15.0->-r requirements.txt (line 2)) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.15.0->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/disi/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.0.1->-r requirements.txt (line 10)) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/disi/miniconda3/lib/python3.12/site-packages (from requests->transformers==4.30.0->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/disi/miniconda3/lib/python3.12/site-packages (from requests->transformers==4.30.0->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/disi/miniconda3/lib/python3.12/site-packages (from requests->transformers==4.30.0->-r requirements.txt (line 1)) (2025.7.9)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.8.0->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from jinja2->torch==2.8.0->-r requirements.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/disi/miniconda3/lib/python3.12/site-packages (from matplotlib->bert-score==0.3.13->-r requirements.txt (line 9)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/disi/miniconda3/lib/python3.12/site-packages (from matplotlib->bert-score==0.3.13->-r requirements.txt (line 9)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from matplotlib->bert-score==0.3.13->-r requirements.txt (line 9)) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/disi/miniconda3/lib/python3.12/site-packages (from matplotlib->bert-score==0.3.13->-r requirements.txt (line 9)) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /home/disi/miniconda3/lib/python3.12/site-packages (from matplotlib->bert-score==0.3.13->-r requirements.txt (line 9)) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/disi/miniconda3/lib/python3.12/site-packages (from matplotlib->bert-score==0.3.13->-r requirements.txt (line 9)) (3.2.3)\n",
      "Using cached transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
      "Using cached torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
      "Using cached bitsandbytes-0.38.1-py3-none-any.whl (104.3 MB)\n",
      "Using cached peft-0.4.0-py3-none-any.whl (72 kB)\n",
      "Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Using cached openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[62 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /tmp/pip-build-env-rg_1_9sf/overlay/lib/python3.12/site-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         License :: OSI Approved :: Apache Software License\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   self._finalize_license_expression()\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/models/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/processors/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/visualizer.py -> build/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m running build_rust\n",
      "  \u001b[31m   \u001b[0m error: can't find Rust compiler\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m To update pip, run:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     pip install --upgrade pip\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m and then retry package installation.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build tokenizers\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mfailed-wheel-build-for-install\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Failed to build installable wheels for some pyproject.toml based projects\n",
      "\u001b[31m╰─>\u001b[0m tokenizers\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------- IMPORTS ----------------------\n",
    "%pip install -r requirements.txt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_absolute_error,root_mean_squared_error,r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from bert_score import score as bertscore\n",
    "from huggingface_hub import login\n",
    "login(\"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\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5791a14a",
   "metadata": {},
   "source": [
    "This cell loads and preprocesses the paper review dataset. It reads the Excel file, removes incomplete entries, cleans text fields, merges reviews for duplicate titles, and prepares the data for training and evaluation. Several utility functions are defined for cleaning and extracting relevant information from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30f1bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- LOAD & CLEAN DATA ----------------------\n",
    "\n",
    "df = pd.read_excel(\"../data/tp_2017conference.xlsx\")\n",
    "\n",
    "# Drop rows missing title/abstract/review\n",
    "df = df.dropna(subset=[\"title\", \"abstract\", \"review\"])\n",
    "\n",
    "# Clean abstract field\n",
    "df[\"abstract\"] = df[\"abstract\"].str.replace(\"Abstract:###\", \"\", regex=False).str.strip()\n",
    "\n",
    "# Deduplicate by title (merge reviews)\n",
    "grouped = df.groupby(\"title\").agg({\n",
    "    \"abstract\": \"first\",  # assume same abstract\n",
    "    \"review\": lambda r: \"\\n\\n\".join(r),  # concat reviews\n",
    "    \"rate\": list,\n",
    "    \"confidence\": list,\n",
    "    \"decision\": \"first\"\n",
    "}).reset_index()\n",
    "\n",
    "# CLEANING FUNCTIONS\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Rimuove caratteri non stampabili, tag HTML, codifiche Unicode e spazi extra'''\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"[\\x00-\\x1F\\x7F]\", \" \", text)\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "    text = re.sub(r\"_x[0-9a-fA-F]{4}_\", \" \", text)\n",
    "    text = re.sub(r\"\\\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_dataset(df):\n",
    "    '''Pulisce il DataFrame rimuovendo righe con campi essenziali mancanti e applicando clean_text'''\n",
    "    # Tieni solo righe con i campi essenziali\n",
    "    df = df.dropna(subset=[\"title\", \"abstract\", \"review\"]).reset_index(drop=True)\n",
    "\n",
    "    # Applica clean_text su tutte le colonne testuali\n",
    "    for col in [\"title\", \"abstract\", \"review\"]:\n",
    "        df[col] = df[col].map(clean_text)\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_response(text):\n",
    "    \"\"\"Rimuove token speciali e fallback in caso di errore\"\"\"\n",
    "    if \"<|start_header_id|>assistant<|end_header_id|>\\n\" in text:\n",
    "        text = text.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\", 1)[-1]\n",
    "    return text.replace(\"<|eot_id|>\", \"\").strip()\n",
    "\n",
    "def extract_number(text):\n",
    "    \"\"\"Estrae numero da rate/confidence tipo 'Rating:###7: ...' \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    match = re.search(r\"(\\d+)\", str(text))\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "# APPLY CLEANING\n",
    "grouped = clean_dataset(grouped)\n",
    "\n",
    "# Rimozione del campo decision (evita leakage)\n",
    "if \"decision\" in grouped.columns:\n",
    "    grouped = grouped.drop(columns=[\"decision\"])\n",
    "\n",
    "# Normalizzazione numerica rate/confidence\n",
    "grouped[\"rating_num\"] = grouped[\"rate\"].apply(lambda lst: extract_number(lst[0]) if isinstance(lst, list) and lst else None)\n",
    "grouped[\"confidence_num\"] = grouped[\"confidence\"].apply(lambda lst: extract_number(lst[0]) if isinstance(lst, list) and lst else None)\n",
    "\n",
    "# Split into train/val/test\n",
    "train_val, test = train_test_split(grouped, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train_val, test_size=0.1, random_state=42)\n",
    "\n",
    "# Save for future use\n",
    "train.to_csv(\"../data/train.csv\", index=False)\n",
    "val.to_csv(\"../data/val.csv\", index=False)\n",
    "test.to_csv(\"../data/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9643f16",
   "metadata": {},
   "source": [
    "This cell initializes the tokenizer and loads the Llama-3 model for causal language modeling. It sets up quantization with BitsAndBytes for efficient memory usage and configures the tokenizer for padding and device placement. The model is loaded with automatic device mapping to utilize available GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdabd9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a660381781204945b6ec0fac96c73f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     12\u001b[39m tokenizer.pad_token = tokenizer.eos_token\n\u001b[32m     13\u001b[39m tokenizer.padding_side = \u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# this will choose GPU\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:311\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    313\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:4839\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4830\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4832\u001b[39m     (\n\u001b[32m   4833\u001b[39m         model,\n\u001b[32m   4834\u001b[39m         missing_keys,\n\u001b[32m   4835\u001b[39m         unexpected_keys,\n\u001b[32m   4836\u001b[39m         mismatched_keys,\n\u001b[32m   4837\u001b[39m         offload_index,\n\u001b[32m   4838\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4839\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4845\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4848\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4854\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4855\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4857\u001b[39m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[32m   4858\u001b[39m model._tp_size = tp_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:5302\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5299\u001b[39m         args_list = logging.tqdm(args_list, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint shards\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5301\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[32m-> \u001b[39m\u001b[32m5302\u001b[39m         _error_msgs, disk_offload_index, cpu_offload_index = \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5303\u001b[39m         error_msgs += _error_msgs\n\u001b[32m   5305\u001b[39m \u001b[38;5;66;03m# Adjust offloaded weights name and save if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:933\u001b[39m, in \u001b[36mload_shard_file\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    931\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m     disk_offload_index, cpu_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index, cpu_offload_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:786\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[39m\n\u001b[32m    784\u001b[39m     param = file_pointer.get_slice(serialized_param_name)\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m786\u001b[39m     param = \u001b[43mempty_param\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_device\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# It is actually not empty!\u001b[39;00m\n\u001b[32m    788\u001b[39m to_contiguous, casting_dtype = _infer_parameter_dtype(\n\u001b[32m    789\u001b[39m     model,\n\u001b[32m    790\u001b[39m     param_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    793\u001b[39m     hf_quantizer,\n\u001b[32m    794\u001b[39m )\n\u001b[32m    796\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# In this case, the param is already on the correct device!\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ---------------------- TOKENIZER & MODEL LLAMA ----------------------\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",  # this will choose GPU\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92289294",
   "metadata": {},
   "source": [
    "This cell prints the device map used by the loaded Llama-3 model, showing how model layers are distributed across available hardware (such as GPUs or CPUs). This helps verify that the model is utilizing the intended devices for inference and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dac6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0}\n"
     ]
    }
   ],
   "source": [
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df96dee3",
   "metadata": {},
   "source": [
    "This cell defines the function used to build prompts for the language model. It formats the paper's title, abstract, and review into a structured input, instructing the model to generate strengths, weaknesses, a numeric rating, and a confidence score in a specific format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8709e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- PROMPT FUNCTION ----------------------\n",
    "def build_prompt(example):\n",
    "    title = clean_text(example[\"title\"])\n",
    "    abstract = clean_text(example[\"abstract\"])\n",
    "    review = clean_text(example[\"review\"])\n",
    "    return (\n",
    "        f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        f\"Title: {title}\\n\"\n",
    "        f\"Abstract: {abstract}\\n\"\n",
    "        f\"Review: {review}\\n\"\n",
    "        \"Please write a structured peer review with this exact format:\\n\\n\"\n",
    "        \"Strengths:\\n1. ...\\n2. ...\\n\\n\"\n",
    "        \"Weaknesses:\\n1. ...\\n2. ...<|eot_id|>\\n\"\n",
    "        \"Then, give a numeric rating (1-10) and a confidence score (1-5) in this format:\\n\"\n",
    "        \"Rating: <number>\\n\"\n",
    "        \"Confidence: <number><|eot_id|>\\n\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bc2657",
   "metadata": {},
   "source": [
    "This cell provides functions to generate structured peer reviews using the language model. It takes each paper, builds a prompt, and uses the model to produce strengths, weaknesses, rating, and confidence predictions. The second function applies this process to an entire DataFrame, saving the generated reviews and handling errors during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c0a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- GENERATE STRENGTHS AND WEAKNESSES (both zero-shot/ft) ----------------------\n",
    "def generate_review(model, tokenizer, paper, max_new_tokens=300):\n",
    "    prompt = build_prompt(paper)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    decoded = clean_response(decoded)\n",
    "    return decoded\n",
    "\n",
    "def generate_reviews_for_df(df, model, tokenizer, output_col=\"generated_review\", output_file=None):\n",
    "    preds = []\n",
    "\n",
    "    print(f\"Generating predictions for {len(df)} papers\")\n",
    "\n",
    "    model.eval()\n",
    "    model.config.use_cache = True\n",
    "    if hasattr(model, \"gradient_checkpointing_disable\"):\n",
    "        model.gradient_checkpointing_disable()\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            response = generate_review(model, tokenizer, row)\n",
    "            preds.append(response)\n",
    "        except Exception as e:\n",
    "            preds.append(f\"[ERROR: {e}]\")\n",
    "            print(f\"❌ Error on row {i}: {e}\")\n",
    "\n",
    "    df[output_col] = preds\n",
    "\n",
    "    if output_file:\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"✅ Saved to {output_file}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589a67c",
   "metadata": {},
   "source": [
    "These two cells generate structured peer reviews for the training and validation sets using the base language model. They clean the training and validation data, apply the review generation function, and save the resulting structured reviews to two CSV files for use as supervised targets in fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b3440d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for 350 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/350 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [1:18:33<00:00, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to train_structured.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- STRUCTURE REVIEW (Distillation) ----------------------\n",
    "\n",
    "# Generate structured reviews (this is your supervised target)\n",
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "\n",
    "#CLEANING\n",
    "train= clean_dataset(train)\n",
    "train_df = generate_reviews_for_df(train, model, tokenizer, output_file=\"train_structured.csv\")\n",
    "train_df.to_csv(\"../data/train_structured.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e63c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for 40 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [08:56<00:00, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to val_structured.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val = pd.read_csv(\"../data/val.csv\")\n",
    "val = clean_dataset(val)\n",
    "val_df = generate_reviews_for_df(val, model, tokenizer, output_file=\"val_structured.csv\")\n",
    "val_df.to_csv(\"../data/val_structured.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63282d9d",
   "metadata": {},
   "source": [
    "This cell configures the model for parameter-efficient fine-tuning using LoRA (Low-Rank Adaptation). It prepares the model for k-bit training and sets up the LoRA configuration, specifying target modules and hyperparameters. The LoRA adapter is then applied to the model to enable efficient adaptation during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b12e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- LoRA CONFIG ----------------------\n",
    "\n",
    "model_ft = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model_ft = get_peft_model(model_ft, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4cbf38",
   "metadata": {},
   "source": [
    "This cell prepares the training and validation datasets for fine-tuning. It builds prompt-response pairs from the generated reviews, tokenizes the data for the language model, and removes unnecessary columns to create datasets suitable for supervised training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744762e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154310f57b1a40c2ac5bd4123e751d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0109b422de254b048eb19bf683aac2e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40fe0807e6394e1ab90042e27324c1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22e7ae881594ef5a85b173e0f4f5769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------- FINE-TUNING PREP ----------------------\n",
    "def ft_prompt(example):\n",
    "    prompt = build_prompt(example)\n",
    "    response = example[\"generated_review\"].strip() + \"\\n<|eot_id|>\"\n",
    "    return {\"prompt\": prompt, \"response\": response}\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df).map(ft_prompt)\n",
    "val_dataset = Dataset.from_pandas(val_df).map(ft_prompt)\n",
    "\n",
    "def tokenize(example):\n",
    "    tokens= tokenizer(\n",
    "        example[\"prompt\"] + example[\"response\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=300\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize)\n",
    "val_dataset = val_dataset.map(tokenize)\n",
    "\n",
    "drop_cols = list(set(train_df.columns) | {\"prompt\", \"response\"})\n",
    "train_dataset = train_dataset.remove_columns([c for c in drop_cols if c in train_dataset.column_names])\n",
    "val_dataset = val_dataset.remove_columns([c for c in drop_cols if c in val_dataset.column_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec5356",
   "metadata": {},
   "source": [
    "This cell sets up and runs the training loop for fine-tuning the model using the Hugging Face Trainer API. It defines training arguments such as batch size, evaluation strategy, number of epochs, and learning rate, then starts the training process and saves the fine-tuned model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f46b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3176/2580500642.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/disi/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='525' max='525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [525/525 03:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.752500</td>\n",
       "      <td>2.618532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.585500</td>\n",
       "      <td>2.538239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.651300</td>\n",
       "      <td>2.521123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.545100</td>\n",
       "      <td>2.514292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.518000</td>\n",
       "      <td>2.511523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disi/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/disi/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/disi/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/disi/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/disi/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('finetuned-llama3-lora/tokenizer_config.json',\n",
       " 'finetuned-llama3-lora/special_tokens_map.json',\n",
       " 'finetuned-llama3-lora/chat_template.jinja',\n",
       " 'finetuned-llama3-lora/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------- TRAINING ----------------------\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned-llama3\",\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_ft,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "model_ft.save_pretrained(\"finetuned-llama3-lora\")\n",
    "tokenizer.save_pretrained(\"finetuned-llama3-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbf9501",
   "metadata": {},
   "source": [
    "This cell performs inference on the test set using the base language model. It cleans the test data, generates zero-shot structured peer reviews, and saves the predictions to a CSV file for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac57db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for 98 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/98 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [27:02<00:00, 16.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to zero_shot_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>review</th>\n",
       "      <th>rate</th>\n",
       "      <th>confidence</th>\n",
       "      <th>rating_num</th>\n",
       "      <th>confidence_num</th>\n",
       "      <th>zero_shot_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Training deep neural-networks using a noise ad...</td>\n",
       "      <td>The availability of large datsets has enabled ...</td>\n",
       "      <td>This paper looks at how to train if there are ...</td>\n",
       "      <td>['Rating:###5: Marginally below acceptance thr...</td>\n",
       "      <td>['Confidence:###4: The reviewer is confident b...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Strengths: 1. The paper addresses a very commo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deep Character-Level Neural Machine Translatio...</td>\n",
       "      <td>Neural machine translation aims at building a ...</td>\n",
       "      <td>* Summary: This paper proposes a neural machin...</td>\n",
       "      <td>['Rating:###6: Marginally above acceptance thr...</td>\n",
       "      <td>['Confidence:###4: The reviewer is confident b...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>Strengths: 1. The paper is well-written. 2. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Third Person Imitation Learning | OpenReview</td>\n",
       "      <td>Reinforcement learning (RL) makes it possible ...</td>\n",
       "      <td>This paper proposed a novel adversarial framew...</td>\n",
       "      <td>['Rating:###5: Marginally below acceptance thr...</td>\n",
       "      <td>['Confidence:###3: The reviewer is fairly conf...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Strengths: 1. The paper presents a novel appro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unsupervised Learning of State Representations...</td>\n",
       "      <td>We present an approach for learning state repr...</td>\n",
       "      <td>This paper is about learning unsupervised stat...</td>\n",
       "      <td>['Rating:###6: Marginally above acceptance thr...</td>\n",
       "      <td>['Confidence:###4: The reviewer is confident b...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>Strengths:\\n1. The paper is clearly written an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Neural Noisy Channel | OpenReview</td>\n",
       "      <td>We formulate sequence to sequence transduction...</td>\n",
       "      <td>This paper proposes to use an SSNT model of p(...</td>\n",
       "      <td>['Rating:###7: Good paper, accept', 'Rating:##...</td>\n",
       "      <td>['Confidence:###4: The reviewer is confident b...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Strengths: 1. The paper proposes a novel appro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>OMG: Orthogonal Method of Grouping With Applic...</td>\n",
       "      <td>Training a classifier with only a few examples...</td>\n",
       "      <td>This paper proposes a k-shot learning framewor...</td>\n",
       "      <td>['Rating:###4: Ok but not good enough - reject...</td>\n",
       "      <td>['Confidence:###4: The reviewer is confident b...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Strengths: 1. The paper proposes a novel appro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Rethinking Numerical Representations for Deep ...</td>\n",
       "      <td>With ever-increasing computational demand for ...</td>\n",
       "      <td>The paper studies the impact of using customiz...</td>\n",
       "      <td>['Rating:###6: Marginally above acceptance thr...</td>\n",
       "      <td>['Confidence:###3: The reviewer is fairly conf...</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>Strengths: 1. The paper is well written and ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Calibrating Energy-based Generative Adversaria...</td>\n",
       "      <td>In this paper, we propose to equip Generative ...</td>\n",
       "      <td>This paper addresses one of the major shortcom...</td>\n",
       "      <td>['Rating:###8: Top 50% of accepted papers, cle...</td>\n",
       "      <td>['Confidence:###4: The reviewer is confident b...</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>Strengths: 1. The paper presents a novel appro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Machine Solver for Physics Word Problems | Ope...</td>\n",
       "      <td>We build a machine solver for word problems on...</td>\n",
       "      <td>The authors describe a system for solving phys...</td>\n",
       "      <td>['Rating:###4: Ok but not good enough - reject...</td>\n",
       "      <td>['Confidence:###4: The reviewer is confident b...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Strengths: 1. The paper is well-written and ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Shift Aggregate Extract Networks | OpenReview</td>\n",
       "      <td>The Shift Aggregate Extract Network SAEN is an...</td>\n",
       "      <td>the paper proposed a method mainly for graph c...</td>\n",
       "      <td>['Rating:###5: Marginally below acceptance thr...</td>\n",
       "      <td>['Confidence:###3: The reviewer is fairly conf...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Strengths: 1. The paper proposes a novel appro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Training deep neural-networks using a noise ad...   \n",
       "1   Deep Character-Level Neural Machine Translatio...   \n",
       "2        Third Person Imitation Learning | OpenReview   \n",
       "3   Unsupervised Learning of State Representations...   \n",
       "4               The Neural Noisy Channel | OpenReview   \n",
       "..                                                ...   \n",
       "93  OMG: Orthogonal Method of Grouping With Applic...   \n",
       "94  Rethinking Numerical Representations for Deep ...   \n",
       "95  Calibrating Energy-based Generative Adversaria...   \n",
       "96  Machine Solver for Physics Word Problems | Ope...   \n",
       "97      Shift Aggregate Extract Networks | OpenReview   \n",
       "\n",
       "                                             abstract  \\\n",
       "0   The availability of large datsets has enabled ...   \n",
       "1   Neural machine translation aims at building a ...   \n",
       "2   Reinforcement learning (RL) makes it possible ...   \n",
       "3   We present an approach for learning state repr...   \n",
       "4   We formulate sequence to sequence transduction...   \n",
       "..                                                ...   \n",
       "93  Training a classifier with only a few examples...   \n",
       "94  With ever-increasing computational demand for ...   \n",
       "95  In this paper, we propose to equip Generative ...   \n",
       "96  We build a machine solver for word problems on...   \n",
       "97  The Shift Aggregate Extract Network SAEN is an...   \n",
       "\n",
       "                                               review  \\\n",
       "0   This paper looks at how to train if there are ...   \n",
       "1   * Summary: This paper proposes a neural machin...   \n",
       "2   This paper proposed a novel adversarial framew...   \n",
       "3   This paper is about learning unsupervised stat...   \n",
       "4   This paper proposes to use an SSNT model of p(...   \n",
       "..                                                ...   \n",
       "93  This paper proposes a k-shot learning framewor...   \n",
       "94  The paper studies the impact of using customiz...   \n",
       "95  This paper addresses one of the major shortcom...   \n",
       "96  The authors describe a system for solving phys...   \n",
       "97  the paper proposed a method mainly for graph c...   \n",
       "\n",
       "                                                 rate  \\\n",
       "0   ['Rating:###5: Marginally below acceptance thr...   \n",
       "1   ['Rating:###6: Marginally above acceptance thr...   \n",
       "2   ['Rating:###5: Marginally below acceptance thr...   \n",
       "3   ['Rating:###6: Marginally above acceptance thr...   \n",
       "4   ['Rating:###7: Good paper, accept', 'Rating:##...   \n",
       "..                                                ...   \n",
       "93  ['Rating:###4: Ok but not good enough - reject...   \n",
       "94  ['Rating:###6: Marginally above acceptance thr...   \n",
       "95  ['Rating:###8: Top 50% of accepted papers, cle...   \n",
       "96  ['Rating:###4: Ok but not good enough - reject...   \n",
       "97  ['Rating:###5: Marginally below acceptance thr...   \n",
       "\n",
       "                                           confidence  rating_num  \\\n",
       "0   ['Confidence:###4: The reviewer is confident b...           5   \n",
       "1   ['Confidence:###4: The reviewer is confident b...           6   \n",
       "2   ['Confidence:###3: The reviewer is fairly conf...           5   \n",
       "3   ['Confidence:###4: The reviewer is confident b...           6   \n",
       "4   ['Confidence:###4: The reviewer is confident b...           7   \n",
       "..                                                ...         ...   \n",
       "93  ['Confidence:###4: The reviewer is confident b...           4   \n",
       "94  ['Confidence:###3: The reviewer is fairly conf...           6   \n",
       "95  ['Confidence:###4: The reviewer is confident b...           8   \n",
       "96  ['Confidence:###4: The reviewer is confident b...           4   \n",
       "97  ['Confidence:###3: The reviewer is fairly conf...           5   \n",
       "\n",
       "    confidence_num                                   zero_shot_review  \n",
       "0                4  Strengths: 1. The paper addresses a very commo...  \n",
       "1                4  Strengths: 1. The paper is well-written. 2. Th...  \n",
       "2                3  Strengths: 1. The paper presents a novel appro...  \n",
       "3                4  Strengths:\\n1. The paper is clearly written an...  \n",
       "4                4  Strengths: 1. The paper proposes a novel appro...  \n",
       "..             ...                                                ...  \n",
       "93               4  Strengths: 1. The paper proposes a novel appro...  \n",
       "94               3  Strengths: 1. The paper is well written and ea...  \n",
       "95               4  Strengths: 1. The paper presents a novel appro...  \n",
       "96               4  Strengths: 1. The paper is well-written and ea...  \n",
       "97               3  Strengths: 1. The paper proposes a novel appro...  \n",
       "\n",
       "[98 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------- INFERENCE ON TEST SET ----------------------\n",
    "# Generate zero-shot reviews\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "test_df = clean_dataset(test_df)\n",
    "\n",
    "generate_reviews_for_df(\n",
    "    df=test_df,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    output_col=\"zero_shot_review\",\n",
    "    output_file=\"zero_shot_predictions.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d1779",
   "metadata": {},
   "source": [
    "This cell generates structured peer reviews for the test set using the fine-tuned model. It cleans the test data, applies the review generation function, and saves the resulting predictions to a CSV file for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188ad05",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Generate reviews using fine-tuned model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m test_df = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m\"\u001b[39m\u001b[33m../data/test.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m test_df = clean_dataset(test_df)\n\u001b[32m      6\u001b[39m generate_reviews_for_df(\n\u001b[32m      7\u001b[39m     df=test_df, \n\u001b[32m      8\u001b[39m     model=model_ft, \n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     output_file=\u001b[33m\"\u001b[39m\u001b[33mfinetuned_predictions.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate reviews using fine-tuned model\n",
    "\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "test_df = clean_dataset(test_df)\n",
    "\n",
    "generate_reviews_for_df(\n",
    "    df=test_df, \n",
    "    model=model_ft, \n",
    "    tokenizer=tokenizer, \n",
    "    output_col=\"fine_tuned_review\", \n",
    "    output_file=\"finetuned_predictions.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f757f",
   "metadata": {},
   "source": [
    "This cell evaluates the zero-shot and fine-tuned model predictions using regression metrics. It extracts numeric ratings and confidence scores from the generated reviews, computes metrics such as MAE, RMSE, R², Pearson, and Spearman correlations, and calculates BERTScore to assess the textual similarity between generated and reference reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd4e4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot usable samples: 71 / 98\n",
      "Fine-tuned usable samples: 65 / 98\n",
      "Zero-shot Rating Metrics: {'MAE': 1.2394366197183098, 'RMSE': 1.5833642201089195, 'R2': -0.050365691489361764, 'Pearson': 0.44940425697779995, 'Spearman': 0.49971386386603706}\n",
      "Fine-tuned Rating Metrics: {'MAE': 1.1846153846153846, 'RMSE': 1.4728308691872156, 'R2': -0.03489159891598925, 'Pearson': 0.4272394633667977, 'Spearman': 0.42107337422528374}\n",
      "Zero-shot Confidence Metrics: {'MAE': 0.8873239436619719, 'RMSE': 1.1196075771271663, 'R2': -1.256785714285714, 'Pearson': 0.05224773559977741, 'Spearman': 0.048965873759822386}\n",
      "Fine-tuned Confidence Metrics: {'MAE': 0.8769230769230769, 'RMSE': 1.1024448355290233, 'R2': -0.9018518518518521, 'Pearson': 0.16858544608470485, 'Spearman': 0.20658396443117913}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadced41c55747368a3cdf1f37645a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb88fd32a7f4adab4523cc4908b5192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd57fd0c79b442ecaf82a819c1350551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0235d8feb6ee4537872c5589438463d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b642a7d8ff4be293bc1fee698e2532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197a4fb1b8b74586b9dd1b0e76954aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned BERTScore F1: 0.8384987711906433\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- EVALUATION ----------------------\n",
    "import re\n",
    "\n",
    "# Carica predizioni\n",
    "zero = pd.read_csv(\"zero_shot_predictions.csv\")\n",
    "ft = pd.read_csv(\"finetuned_predictions.csv\")\n",
    "\n",
    "# Funzione per estrarre numeri dal testo\n",
    "def extract_pred_numbers(text):\n",
    "    rating, confidence = None, None\n",
    "    if isinstance(text, str):\n",
    "        match_r = re.search(r\"Rating:\\s*(\\d+)\", text)\n",
    "        match_c = re.search(r\"Confidence:\\s*(\\d+)\", text)\n",
    "        if match_r:\n",
    "            rating = int(match_r.group(1))\n",
    "        if match_c:\n",
    "            confidence = int(match_c.group(1))\n",
    "    return rating, confidence\n",
    "\n",
    "# Aggiungi colonne predette\n",
    "zero[[\"rating_pred\", \"confidence_pred\"]] = zero[\"zero_shot_review\"].apply(\n",
    "    lambda x: pd.Series(extract_pred_numbers(x))\n",
    ")\n",
    "ft[[\"rating_pred\", \"confidence_pred\"]] = ft[\"fine_tuned_review\"].apply(\n",
    "    lambda x: pd.Series(extract_pred_numbers(x))\n",
    ")\n",
    "\n",
    "# Funzione per regression metrics\n",
    "def regression_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    pr, _ = pearsonr(y_true, y_pred)\n",
    "    sr, _ = spearmanr(y_true, y_pred)\n",
    "    return dict(MAE=mae, RMSE=rmse, R2=r2, Pearson=pr, Spearman=sr)\n",
    "\n",
    "# Drop rows with missing predictions\n",
    "zero_eval = zero.dropna(subset=[\"rating_pred\", \"confidence_pred\"])\n",
    "ft_eval = ft.dropna(subset=[\"rating_pred\", \"confidence_pred\"])\n",
    "\n",
    "print(\"Zero-shot usable samples:\", len(zero_eval), \"/\", len(zero))\n",
    "print(\"Fine-tuned usable samples:\", len(ft_eval), \"/\", len(ft))\n",
    "\n",
    "# Rating\n",
    "print(\"Zero-shot Rating Metrics:\", regression_metrics(zero_eval[\"rating_num\"], zero_eval[\"rating_pred\"]))\n",
    "print(\"Fine-tuned Rating Metrics:\", regression_metrics(ft_eval[\"rating_num\"], ft_eval[\"rating_pred\"]))\n",
    "\n",
    "# Confidence\n",
    "print(\"Zero-shot Confidence Metrics:\", regression_metrics(zero_eval[\"confidence_num\"], zero_eval[\"confidence_pred\"]))\n",
    "print(\"Fine-tuned Confidence Metrics:\", regression_metrics(ft_eval[\"confidence_num\"], ft_eval[\"confidence_pred\"]))\n",
    "\n",
    "# BERTScore su review testuali\n",
    "P, R, F1 = bertscore(\n",
    "    cands=ft[\"fine_tuned_review\"].fillna(\"\").tolist(),\n",
    "    refs=ft[\"review\"].fillna(\"\").tolist(),\n",
    "    lang=\"en\"\n",
    ")\n",
    "print(\"Fine-tuned BERTScore F1:\", F1.mean().item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
