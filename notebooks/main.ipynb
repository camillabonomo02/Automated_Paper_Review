{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a106fb84",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Small LLM with LoRA for Automated Paper Review Insights (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f97212",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27cdf0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers<5.0.0,>=4.41.0 (from -r requirements.txt (line 1))\n",
      "  Downloading transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: datasets<3.0.0,>=2.15.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.15.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.18.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (0.32.2)\n",
      "Requirement already satisfied: peft>=0.8.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn>=1.2.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (1.6.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (2.2.2)\n",
      "Requirement already satisfied: tqdm in /home/disi/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (4.67.1)\n",
      "Requirement already satisfied: bert-score in /home/disi/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (0.3.13)\n",
      "Requirement already satisfied: pandas in /home/disi/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (2.3.1)\n",
      "Requirement already satisfied: scipy in /home/disi/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (1.15.3)\n",
      "Requirement already satisfied: openpyxl in /home/disi/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (3.1.5)\n",
      "Requirement already satisfied: filelock in /home/disi/miniconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->-r requirements.txt (line 1)) (3.18.0)\n",
      "Collecting huggingface-hub<1.0.0,>=0.18.0 (from -r requirements.txt (line 3))\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/disi/miniconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/disi/miniconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/disi/miniconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->-r requirements.txt (line 1)) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/disi/miniconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->-r requirements.txt (line 1)) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->-r requirements.txt (line 1))\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/disi/miniconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->-r requirements.txt (line 1)) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from datasets<3.0.0,>=2.15.0->-r requirements.txt (line 2)) (20.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/disi/miniconda3/lib/python3.12/site-packages (from datasets<3.0.0,>=2.15.0->-r requirements.txt (line 2)) (0.7)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from datasets<3.0.0,>=2.15.0->-r requirements.txt (line 2)) (0.3.7)\n",
      "Requirement already satisfied: xxhash in /home/disi/miniconda3/lib/python3.12/site-packages (from datasets<3.0.0,>=2.15.0->-r requirements.txt (line 2)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/disi/miniconda3/lib/python3.12/site-packages (from datasets<3.0.0,>=2.15.0->-r requirements.txt (line 2)) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets<3.0.0,>=2.15.0->-r requirements.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/disi/miniconda3/lib/python3.12/site-packages (from datasets<3.0.0,>=2.15.0->-r requirements.txt (line 2)) (3.12.14)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/disi/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0.0,>=0.18.0->-r requirements.txt (line 3)) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0.0,>=0.18.0->-r requirements.txt (line 3))\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: psutil in /home/disi/miniconda3/lib/python3.12/site-packages (from peft>=0.8.0->-r requirements.txt (line 4)) (7.0.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from peft>=0.8.0->-r requirements.txt (line 4)) (1.9.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from scikit-learn>=1.2.0->-r requirements.txt (line 5)) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from scikit-learn>=1.2.0->-r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: sympy in /home/disi/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/disi/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/disi/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/disi/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/disi/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/disi/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/disi/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/disi/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/disi/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/disi/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/disi/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/disi/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/disi/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/disi/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 6)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/disi/miniconda3/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->-r requirements.txt (line 6)) (12.6.85)\n",
      "Requirement already satisfied: matplotlib in /home/disi/miniconda3/lib/python3.12/site-packages (from bert-score->-r requirements.txt (line 8)) (3.10.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/disi/miniconda3/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 9)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/disi/miniconda3/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 9)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/disi/miniconda3/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 9)) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in /home/disi/miniconda3/lib/python3.12/site-packages (from openpyxl->-r requirements.txt (line 11)) (2.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets<3.0.0,>=2.15.0->-r requirements.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets<3.0.0,>=2.15.0->-r requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets<3.0.0,>=2.15.0->-r requirements.txt (line 2)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/disi/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets<3.0.0,>=2.15.0->-r requirements.txt (line 2)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/disi/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets<3.0.0,>=2.15.0->-r requirements.txt (line 2)) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets<3.0.0,>=2.15.0->-r requirements.txt (line 2)) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets<3.0.0,>=2.15.0->-r requirements.txt (line 2)) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets<3.0.0,>=2.15.0->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/disi/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 9)) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/disi/miniconda3/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/disi/miniconda3/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/disi/miniconda3/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->-r requirements.txt (line 1)) (2025.7.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 6)) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/disi/miniconda3/lib/python3.12/site-packages (from matplotlib->bert-score->-r requirements.txt (line 8)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/disi/miniconda3/lib/python3.12/site-packages (from matplotlib->bert-score->-r requirements.txt (line 8)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from matplotlib->bert-score->-r requirements.txt (line 8)) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/disi/miniconda3/lib/python3.12/site-packages (from matplotlib->bert-score->-r requirements.txt (line 8)) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /home/disi/miniconda3/lib/python3.12/site-packages (from matplotlib->bert-score->-r requirements.txt (line 8)) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/disi/miniconda3/lib/python3.12/site-packages (from matplotlib->bert-score->-r requirements.txt (line 8)) (3.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/disi/miniconda3/lib/python3.12/site-packages (from sympy->torch>=2.0.0->-r requirements.txt (line 6)) (1.3.0)\n",
      "Downloading transformers-4.57.0-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hf-xet, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: hf-xet\n",
      "\u001b[2K    Found existing installation: hf-xet 1.1.2\n",
      "\u001b[2K    Uninstalling hf-xet-1.1.2:\n",
      "\u001b[2K      Successfully uninstalled hf-xet-1.1.2\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.32.2\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.32.2:\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.32.2\n",
      "\u001b[2K  Attempting uninstall: tokenizers90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Found existing installation: tokenizers 0.15.2━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Uninstalling tokenizers-0.15.2:━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [huggingface-hub]\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.15.2━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/4\u001b[0m [huggingface-hub]\n",
      "\u001b[2K  Attempting uninstall: transformersm╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [tokenizers]]\n",
      "\u001b[2K    Found existing installation: transformers 4.39.3━━━━━━━━━━\u001b[0m \u001b[32m2/4\u001b[0m [tokenizers]\n",
      "\u001b[2K    Uninstalling transformers-4.39.3:━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.39.390m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.10 huggingface-hub-0.35.3 tokenizers-0.22.1 transformers-4.57.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------- IMPORTS ----------------------\n",
    "%pip install -r requirements.txt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_absolute_error,root_mean_squared_error,r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from bert_score import score as bertscore\n",
    "from huggingface_hub import login\n",
    "login(\"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\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5791a14a",
   "metadata": {},
   "source": [
    "This cell loads and preprocesses the paper review dataset. It reads the Excel file, removes incomplete entries, cleans text fields, merges reviews for duplicate titles, and prepares the data for training and evaluation. Several utility functions are defined for cleaning and extracting relevant information from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30f1bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- LOAD & CLEAN DATA ----------------------\n",
    "\n",
    "df = pd.read_excel(\"../data/tp_2017conference.xlsx\")\n",
    "\n",
    "# Drop rows missing title/abstract/review\n",
    "df = df.dropna(subset=[\"title\", \"abstract\", \"review\"])\n",
    "\n",
    "# Clean abstract field\n",
    "df[\"abstract\"] = df[\"abstract\"].str.replace(\"Abstract:###\", \"\", regex=False).str.strip()\n",
    "\n",
    "# Deduplicate by title (merge reviews)\n",
    "grouped = df.groupby(\"title\").agg({\n",
    "    \"abstract\": \"first\",  # assume same abstract\n",
    "    \"review\": lambda r: \"\\n\\n\".join(r),  # concat reviews\n",
    "    \"rate\": list,\n",
    "    \"confidence\": list,\n",
    "    \"decision\": \"first\"\n",
    "}).reset_index()\n",
    "\n",
    "# CLEANING FUNCTIONS\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Rimuove caratteri non stampabili, tag HTML, codifiche Unicode e spazi extra'''\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"[\\x00-\\x1F\\x7F]\", \" \", text)\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "    text = re.sub(r\"_x[0-9a-fA-F]{4}_\", \" \", text)\n",
    "    text = re.sub(r\"\\\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_dataset(df):\n",
    "    '''Pulisce il DataFrame rimuovendo righe con campi essenziali mancanti e applicando clean_text'''\n",
    "    # Tieni solo righe con i campi essenziali\n",
    "    df = df.dropna(subset=[\"title\", \"abstract\", \"review\"]).reset_index(drop=True)\n",
    "\n",
    "    # Applica clean_text su tutte le colonne testuali\n",
    "    for col in [\"title\", \"abstract\", \"review\"]:\n",
    "        df[col] = df[col].map(clean_text)\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_response(text):\n",
    "    \"\"\"Rimuove token speciali e fallback in caso di errore\"\"\"\n",
    "    if \"<|start_header_id|>assistant<|end_header_id|>\\n\" in text:\n",
    "        text = text.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\", 1)[-1]\n",
    "    return text.replace(\"<|eot_id|>\", \"\").strip()\n",
    "\n",
    "def extract_number(text):\n",
    "    \"\"\"Estrae numero da rate/confidence tipo 'Rating:###7: ...' \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    match = re.search(r\"(\\d+)\", str(text))\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "# APPLY CLEANING\n",
    "grouped = clean_dataset(grouped)\n",
    "\n",
    "# Rimozione del campo decision (evita leakage)\n",
    "if \"decision\" in grouped.columns:\n",
    "    grouped = grouped.drop(columns=[\"decision\"])\n",
    "\n",
    "# Normalizzazione numerica rate/confidence\n",
    "grouped[\"rating_num\"] = grouped[\"rate\"].apply(lambda lst: extract_number(lst[0]) if isinstance(lst, list) and lst else None)\n",
    "grouped[\"confidence_num\"] = grouped[\"confidence\"].apply(lambda lst: extract_number(lst[0]) if isinstance(lst, list) and lst else None)\n",
    "\n",
    "# Split into train/val/test\n",
    "train_val, test = train_test_split(grouped, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train_val, test_size=0.1, random_state=42)\n",
    "\n",
    "# Save for future use\n",
    "train.to_csv(\"../data/train.csv\", index=False)\n",
    "val.to_csv(\"../data/val.csv\", index=False)\n",
    "test.to_csv(\"../data/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9643f16",
   "metadata": {},
   "source": [
    "This cell initializes the tokenizer and loads the Llama-3 model for causal language modeling. It sets up quantization with BitsAndBytes for efficient memory usage and configures the tokenizer for padding and device placement. The model is loaded with automatic device mapping to utilize available GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdabd9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- TOKENIZER & MODEL LLAMA ----------------------\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",  # this will choose GPU\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92289294",
   "metadata": {},
   "source": [
    "This cell prints the device map used by the loaded Llama-3 model, showing how model layers are distributed across available hardware (such as GPUs or CPUs). This helps verify that the model is utilizing the intended devices for inference and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dac6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0}\n"
     ]
    }
   ],
   "source": [
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df96dee3",
   "metadata": {},
   "source": [
    "This cell defines the function used to build prompts for the language model. It formats the paper's title, abstract, and review into a structured input, instructing the model to generate strengths, weaknesses, a numeric rating, and a confidence score in a specific format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8709e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- PROMPT FUNCTION ----------------------\n",
    "def build_prompt(example):\n",
    "    title = clean_text(example[\"title\"])\n",
    "    abstract = clean_text(example[\"abstract\"])\n",
    "    review = clean_text(example[\"review\"])\n",
    "    return (\n",
    "        f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        f\"Title: {title}\\n\"\n",
    "        f\"Abstract: {abstract}\\n\"\n",
    "        f\"Review: {review}\\n\"\n",
    "        \"Please write a structured peer review with this exact format:\\n\\n\"\n",
    "        \"Strengths:\\n1. ...\\n2. ...\\n\\n\"\n",
    "        \"Weaknesses:\\n1. ...\\n2. ...<|eot_id|>\\n\"\n",
    "        \"Then, give a numeric rating (1-10) and a confidence score (1-5) in this format:\\n\"\n",
    "        \"Rating: <number>\\n\"\n",
    "        \"Confidence: <number><|eot_id|>\\n\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bc2657",
   "metadata": {},
   "source": [
    "This cell provides functions to generate structured peer reviews using the language model. It takes each paper, builds a prompt, and uses the model to produce strengths, weaknesses, rating, and confidence predictions. The second function applies this process to an entire DataFrame, saving the generated reviews and handling errors during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c0a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- GENERATE STRENGTHS AND WEAKNESSES (both zero-shot/ft) ----------------------\n",
    "def generate_review(model, tokenizer, paper, max_new_tokens=300):\n",
    "    prompt = build_prompt(paper)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    decoded = clean_response(decoded)\n",
    "    return decoded\n",
    "\n",
    "def generate_reviews_for_df(df, model, tokenizer, output_col=\"generated_review\", output_file=None):\n",
    "    preds = []\n",
    "\n",
    "    print(f\"Generating predictions for {len(df)} papers\")\n",
    "\n",
    "    model.eval()\n",
    "    model.config.use_cache = True\n",
    "    if hasattr(model, \"gradient_checkpointing_disable\"):\n",
    "        model.gradient_checkpointing_disable()\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            response = generate_review(model, tokenizer, row)\n",
    "            preds.append(response)\n",
    "        except Exception as e:\n",
    "            preds.append(f\"[ERROR: {e}]\")\n",
    "            print(f\"❌ Error on row {i}: {e}\")\n",
    "\n",
    "    df[output_col] = preds\n",
    "\n",
    "    if output_file:\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"✅ Saved to {output_file}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589a67c",
   "metadata": {},
   "source": [
    "These two cells generate structured peer reviews for the training and validation sets using the base language model. They clean the training and validation data, apply the review generation function, and save the resulting structured reviews to two CSV files for use as supervised targets in fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b3440d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for 350 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/350 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [1:18:33<00:00, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to train_structured.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- STRUCTURE REVIEW (Distillation) ----------------------\n",
    "\n",
    "# Generate structured reviews (this is your supervised target)\n",
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "\n",
    "#CLEANING\n",
    "train= clean_dataset(train)\n",
    "train_df = generate_reviews_for_df(train, model, tokenizer, output_file=\"train_structured.csv\")\n",
    "train_df.to_csv(\"../data/train_structured.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e63c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for 40 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [08:56<00:00, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to val_structured.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val = pd.read_csv(\"../data/val.csv\")\n",
    "val = clean_dataset(val)\n",
    "val_df = generate_reviews_for_df(val, model, tokenizer, output_file=\"val_structured.csv\")\n",
    "val_df.to_csv(\"../data/val_structured.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63282d9d",
   "metadata": {},
   "source": [
    "This cell configures the model for parameter-efficient fine-tuning using LoRA (Low-Rank Adaptation). It prepares the model for k-bit training and sets up the LoRA configuration, specifying target modules and hyperparameters. The LoRA adapter is then applied to the model to enable efficient adaptation during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b12e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- LoRA CONFIG ----------------------\n",
    "\n",
    "model_ft = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model_ft = get_peft_model(model_ft, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4cbf38",
   "metadata": {},
   "source": [
    "This cell prepares the training and validation datasets for fine-tuning. It builds prompt-response pairs from the generated reviews, tokenizes the data for the language model, and removes unnecessary columns to create datasets suitable for supervised training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744762e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- FINE-TUNING PREP ----------------------\n",
    "def ft_prompt(example):\n",
    "    prompt = build_prompt(example)\n",
    "    response = example[\"generated_review\"].strip() + \"\\n<|eot_id|>\"\n",
    "    return {\"prompt\": prompt, \"response\": response}\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df).map(ft_prompt)\n",
    "val_dataset = Dataset.from_pandas(val_df).map(ft_prompt)\n",
    "\n",
    "def tokenize(example):\n",
    "    tokens= tokenizer(\n",
    "        example[\"prompt\"] + example[\"response\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=300\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize)\n",
    "val_dataset = val_dataset.map(tokenize)\n",
    "\n",
    "drop_cols = list(set(train_df.columns) | {\"prompt\", \"response\"})\n",
    "train_dataset = train_dataset.remove_columns([c for c in drop_cols if c in train_dataset.column_names])\n",
    "val_dataset = val_dataset.remove_columns([c for c in drop_cols if c in val_dataset.column_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec5356",
   "metadata": {},
   "source": [
    "This cell sets up and runs the training loop for fine-tuning the model using the Hugging Face Trainer API. It defines training arguments such as batch size, evaluation strategy, number of epochs, and learning rate, then starts the training process and saves the fine-tuned model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- TRAINING ----------------------\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned-llama3\",\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_ft,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "model_ft.save_pretrained(\"finetuned-llama3-lora\")\n",
    "tokenizer.save_pretrained(\"finetuned-llama3-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbf9501",
   "metadata": {},
   "source": [
    "This cell performs inference on the test set using the base language model. It cleans the test data, generates zero-shot structured peer reviews, and saves the predictions to a CSV file for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac57db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for 98 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/98 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [27:02<00:00, 16.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to zero_shot_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>review</th>\n",
       "      <th>rate</th>\n",
       "      <th>confidence</th>\n",
       "      <th>rating_num</th>\n",
       "      <th>confidence_num</th>\n",
       "      <th>zero_shot_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Training deep neural-networks using a noise ad...</td>\n",
       "      <td>The availability of large datsets has enabled ...</td>\n",
       "      <td>This paper looks at how to train if there are ...</td>\n",
       "      <td>['Rating:###5: Marginally below acceptance thr...</td>\n",
       "      <td>['Confidence:###4: The reviewer is confident b...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Strengths: 1. The paper addresses a very commo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deep Character-Level Neural Machine Translatio...</td>\n",
       "      <td>Neural machine translation aims at building a ...</td>\n",
       "      <td>* Summary: This paper proposes a neural machin...</td>\n",
       "      <td>['Rating:###6: Marginally above acceptance thr...</td>\n",
       "      <td>['Confidence:###4: The reviewer is confident b...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>Strengths: 1. The paper is well-written. 2. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Third Person Imitation Learning | OpenReview</td>\n",
       "      <td>Reinforcement learning (RL) makes it possible ...</td>\n",
       "      <td>This paper proposed a novel adversarial framew...</td>\n",
       "      <td>['Rating:###5: Marginally below acceptance thr...</td>\n",
       "      <td>['Confidence:###3: The reviewer is fairly conf...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Strengths: 1. The paper presents a novel appro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unsupervised Learning of State Representations...</td>\n",
       "      <td>We present an approach for learning state repr...</td>\n",
       "      <td>This paper is about learning unsupervised stat...</td>\n",
       "      <td>['Rating:###6: Marginally above acceptance thr...</td>\n",
       "      <td>['Confidence:###4: The reviewer is confident b...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>Strengths:\\n1. The paper is clearly written an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Neural Noisy Channel | OpenReview</td>\n",
       "      <td>We formulate sequence to sequence transduction...</td>\n",
       "      <td>This paper proposes to use an SSNT model of p(...</td>\n",
       "      <td>['Rating:###7: Good paper, accept', 'Rating:##...</td>\n",
       "      <td>['Confidence:###4: The reviewer is confident b...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Strengths: 1. The paper proposes a novel appro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>OMG: Orthogonal Method of Grouping With Applic...</td>\n",
       "      <td>Training a classifier with only a few examples...</td>\n",
       "      <td>This paper proposes a k-shot learning framewor...</td>\n",
       "      <td>['Rating:###4: Ok but not good enough - reject...</td>\n",
       "      <td>['Confidence:###4: The reviewer is confident b...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Strengths: 1. The paper proposes a novel appro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Rethinking Numerical Representations for Deep ...</td>\n",
       "      <td>With ever-increasing computational demand for ...</td>\n",
       "      <td>The paper studies the impact of using customiz...</td>\n",
       "      <td>['Rating:###6: Marginally above acceptance thr...</td>\n",
       "      <td>['Confidence:###3: The reviewer is fairly conf...</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>Strengths: 1. The paper is well written and ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Calibrating Energy-based Generative Adversaria...</td>\n",
       "      <td>In this paper, we propose to equip Generative ...</td>\n",
       "      <td>This paper addresses one of the major shortcom...</td>\n",
       "      <td>['Rating:###8: Top 50% of accepted papers, cle...</td>\n",
       "      <td>['Confidence:###4: The reviewer is confident b...</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>Strengths: 1. The paper presents a novel appro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Machine Solver for Physics Word Problems | Ope...</td>\n",
       "      <td>We build a machine solver for word problems on...</td>\n",
       "      <td>The authors describe a system for solving phys...</td>\n",
       "      <td>['Rating:###4: Ok but not good enough - reject...</td>\n",
       "      <td>['Confidence:###4: The reviewer is confident b...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Strengths: 1. The paper is well-written and ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Shift Aggregate Extract Networks | OpenReview</td>\n",
       "      <td>The Shift Aggregate Extract Network SAEN is an...</td>\n",
       "      <td>the paper proposed a method mainly for graph c...</td>\n",
       "      <td>['Rating:###5: Marginally below acceptance thr...</td>\n",
       "      <td>['Confidence:###3: The reviewer is fairly conf...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Strengths: 1. The paper proposes a novel appro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Training deep neural-networks using a noise ad...   \n",
       "1   Deep Character-Level Neural Machine Translatio...   \n",
       "2        Third Person Imitation Learning | OpenReview   \n",
       "3   Unsupervised Learning of State Representations...   \n",
       "4               The Neural Noisy Channel | OpenReview   \n",
       "..                                                ...   \n",
       "93  OMG: Orthogonal Method of Grouping With Applic...   \n",
       "94  Rethinking Numerical Representations for Deep ...   \n",
       "95  Calibrating Energy-based Generative Adversaria...   \n",
       "96  Machine Solver for Physics Word Problems | Ope...   \n",
       "97      Shift Aggregate Extract Networks | OpenReview   \n",
       "\n",
       "                                             abstract  \\\n",
       "0   The availability of large datsets has enabled ...   \n",
       "1   Neural machine translation aims at building a ...   \n",
       "2   Reinforcement learning (RL) makes it possible ...   \n",
       "3   We present an approach for learning state repr...   \n",
       "4   We formulate sequence to sequence transduction...   \n",
       "..                                                ...   \n",
       "93  Training a classifier with only a few examples...   \n",
       "94  With ever-increasing computational demand for ...   \n",
       "95  In this paper, we propose to equip Generative ...   \n",
       "96  We build a machine solver for word problems on...   \n",
       "97  The Shift Aggregate Extract Network SAEN is an...   \n",
       "\n",
       "                                               review  \\\n",
       "0   This paper looks at how to train if there are ...   \n",
       "1   * Summary: This paper proposes a neural machin...   \n",
       "2   This paper proposed a novel adversarial framew...   \n",
       "3   This paper is about learning unsupervised stat...   \n",
       "4   This paper proposes to use an SSNT model of p(...   \n",
       "..                                                ...   \n",
       "93  This paper proposes a k-shot learning framewor...   \n",
       "94  The paper studies the impact of using customiz...   \n",
       "95  This paper addresses one of the major shortcom...   \n",
       "96  The authors describe a system for solving phys...   \n",
       "97  the paper proposed a method mainly for graph c...   \n",
       "\n",
       "                                                 rate  \\\n",
       "0   ['Rating:###5: Marginally below acceptance thr...   \n",
       "1   ['Rating:###6: Marginally above acceptance thr...   \n",
       "2   ['Rating:###5: Marginally below acceptance thr...   \n",
       "3   ['Rating:###6: Marginally above acceptance thr...   \n",
       "4   ['Rating:###7: Good paper, accept', 'Rating:##...   \n",
       "..                                                ...   \n",
       "93  ['Rating:###4: Ok but not good enough - reject...   \n",
       "94  ['Rating:###6: Marginally above acceptance thr...   \n",
       "95  ['Rating:###8: Top 50% of accepted papers, cle...   \n",
       "96  ['Rating:###4: Ok but not good enough - reject...   \n",
       "97  ['Rating:###5: Marginally below acceptance thr...   \n",
       "\n",
       "                                           confidence  rating_num  \\\n",
       "0   ['Confidence:###4: The reviewer is confident b...           5   \n",
       "1   ['Confidence:###4: The reviewer is confident b...           6   \n",
       "2   ['Confidence:###3: The reviewer is fairly conf...           5   \n",
       "3   ['Confidence:###4: The reviewer is confident b...           6   \n",
       "4   ['Confidence:###4: The reviewer is confident b...           7   \n",
       "..                                                ...         ...   \n",
       "93  ['Confidence:###4: The reviewer is confident b...           4   \n",
       "94  ['Confidence:###3: The reviewer is fairly conf...           6   \n",
       "95  ['Confidence:###4: The reviewer is confident b...           8   \n",
       "96  ['Confidence:###4: The reviewer is confident b...           4   \n",
       "97  ['Confidence:###3: The reviewer is fairly conf...           5   \n",
       "\n",
       "    confidence_num                                   zero_shot_review  \n",
       "0                4  Strengths: 1. The paper addresses a very commo...  \n",
       "1                4  Strengths: 1. The paper is well-written. 2. Th...  \n",
       "2                3  Strengths: 1. The paper presents a novel appro...  \n",
       "3                4  Strengths:\\n1. The paper is clearly written an...  \n",
       "4                4  Strengths: 1. The paper proposes a novel appro...  \n",
       "..             ...                                                ...  \n",
       "93               4  Strengths: 1. The paper proposes a novel appro...  \n",
       "94               3  Strengths: 1. The paper is well written and ea...  \n",
       "95               4  Strengths: 1. The paper presents a novel appro...  \n",
       "96               4  Strengths: 1. The paper is well-written and ea...  \n",
       "97               3  Strengths: 1. The paper proposes a novel appro...  \n",
       "\n",
       "[98 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------- INFERENCE ON TEST SET ----------------------\n",
    "# Generate zero-shot reviews\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "test_df = clean_dataset(test_df)\n",
    "\n",
    "generate_reviews_for_df(\n",
    "    df=test_df,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    output_col=\"zero_shot_review\",\n",
    "    output_file=\"zero_shot_predictions.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d1779",
   "metadata": {},
   "source": [
    "This cell generates structured peer reviews for the test set using the fine-tuned model. It cleans the test data, applies the review generation function, and saves the resulting predictions to a CSV file for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188ad05",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Generate reviews using fine-tuned model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m test_df = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m\"\u001b[39m\u001b[33m../data/test.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m test_df = clean_dataset(test_df)\n\u001b[32m      6\u001b[39m generate_reviews_for_df(\n\u001b[32m      7\u001b[39m     df=test_df, \n\u001b[32m      8\u001b[39m     model=model_ft, \n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     output_file=\u001b[33m\"\u001b[39m\u001b[33mfinetuned_predictions.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate reviews using fine-tuned model\n",
    "\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "test_df = clean_dataset(test_df)\n",
    "\n",
    "generate_reviews_for_df(\n",
    "    df=test_df, \n",
    "    model=model_ft, \n",
    "    tokenizer=tokenizer, \n",
    "    output_col=\"fine_tuned_review\", \n",
    "    output_file=\"finetuned_predictions.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f757f",
   "metadata": {},
   "source": [
    "This cell evaluates the zero-shot and fine-tuned model predictions using regression metrics. It extracts numeric ratings and confidence scores from the generated reviews, computes metrics such as MAE, RMSE, R², Pearson, and Spearman correlations, and calculates BERTScore to assess the textual similarity between generated and reference reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd4e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- EVALUATION ----------------------\n",
    "import re\n",
    "\n",
    "# Carica predizioni\n",
    "zero = pd.read_csv(\"zero_shot_predictions.csv\")\n",
    "ft = pd.read_csv(\"finetuned_predictions.csv\")\n",
    "\n",
    "# Funzione per estrarre numeri dal testo\n",
    "def extract_pred_numbers(text):\n",
    "    rating, confidence = None, None\n",
    "    if isinstance(text, str):\n",
    "        match_r = re.search(r\"Rating:\\s*(\\d+)\", text)\n",
    "        match_c = re.search(r\"Confidence:\\s*(\\d+)\", text)\n",
    "        if match_r:\n",
    "            rating = int(match_r.group(1))\n",
    "        if match_c:\n",
    "            confidence = int(match_c.group(1))\n",
    "    return rating, confidence\n",
    "\n",
    "# Aggiungi colonne predette\n",
    "zero[[\"rating_pred\", \"confidence_pred\"]] = zero[\"zero_shot_review\"].apply(\n",
    "    lambda x: pd.Series(extract_pred_numbers(x))\n",
    ")\n",
    "ft[[\"rating_pred\", \"confidence_pred\"]] = ft[\"fine_tuned_review\"].apply(\n",
    "    lambda x: pd.Series(extract_pred_numbers(x))\n",
    ")\n",
    "\n",
    "# Funzione per regression metrics\n",
    "def regression_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    pr, _ = pearsonr(y_true, y_pred)\n",
    "    sr, _ = spearmanr(y_true, y_pred)\n",
    "    return dict(MAE=mae, RMSE=rmse, R2=r2, Pearson=pr, Spearman=sr)\n",
    "\n",
    "# Drop rows with missing predictions\n",
    "zero_eval = zero.dropna(subset=[\"rating_pred\", \"confidence_pred\"])\n",
    "ft_eval = ft.dropna(subset=[\"rating_pred\", \"confidence_pred\"])\n",
    "\n",
    "print(\"Zero-shot usable samples:\", len(zero_eval), \"/\", len(zero))\n",
    "print(\"Fine-tuned usable samples:\", len(ft_eval), \"/\", len(ft))\n",
    "\n",
    "# Rating\n",
    "print(\"Zero-shot Rating Metrics:\", regression_metrics(zero_eval[\"rating_num\"], zero_eval[\"rating_pred\"]))\n",
    "print(\"Fine-tuned Rating Metrics:\", regression_metrics(ft_eval[\"rating_num\"], ft_eval[\"rating_pred\"]))\n",
    "\n",
    "# Confidence\n",
    "print(\"Zero-shot Confidence Metrics:\", regression_metrics(zero_eval[\"confidence_num\"], zero_eval[\"confidence_pred\"]))\n",
    "print(\"Fine-tuned Confidence Metrics:\", regression_metrics(ft_eval[\"confidence_num\"], ft_eval[\"confidence_pred\"]))\n",
    "\n",
    "# BERTScore su review testuali\n",
    "P, R, F1 = bertscore(\n",
    "    cands=ft[\"fine_tuned_review\"].fillna(\"\").tolist(),\n",
    "    refs=ft[\"review\"].fillna(\"\").tolist(),\n",
    "    lang=\"en\"\n",
    ")\n",
    "print(\"Fine-tuned BERTScore F1:\", F1.mean().item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
