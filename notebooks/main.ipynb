{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a106fb84",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Small LLM with LoRA for Automated Paper Review Insights (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f97212",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cdf0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------- IMPORTS ----------------------\n",
    "%pip install -r requirements.txt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_absolute_error,root_mean_squared_error,r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from bert_score import score as bertscore\n",
    "#from huggingface_hub import login\n",
    "#login(\"your_token_here\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5791a14a",
   "metadata": {},
   "source": [
    "This cell loads and preprocesses the paper review dataset. It reads the Excel file, removes incomplete entries, cleans text fields, merges reviews for duplicate titles, and prepares the data for training and evaluation. Several utility functions are defined for cleaning and extracting relevant information from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f30f1bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- LOAD & CLEAN DATA ----------------------\n",
    "\n",
    "df = pd.read_excel(\"./data/tp_2017conference.xlsx\")\n",
    "\n",
    "# Drop rows missing title/abstract/review\n",
    "df = df.dropna(subset=[\"title\", \"abstract\", \"review\"])\n",
    "\n",
    "# Clean abstract field\n",
    "df[\"abstract\"] = df[\"abstract\"].str.replace(\"Abstract:###\", \"\", regex=False).str.strip()\n",
    "\n",
    "# Deduplicate by title (merge reviews)\n",
    "grouped = df.groupby(\"title\").agg({\n",
    "    \"abstract\": \"first\",  # assume same abstract\n",
    "    \"review\": lambda r: \"\\n\\n\".join(r),  # concat reviews\n",
    "    \"rate\": list,\n",
    "    \"confidence\": list,\n",
    "    \"decision\": \"first\"\n",
    "}).reset_index()\n",
    "\n",
    "# CLEANING FUNCTIONS\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Rimuove caratteri non stampabili, tag HTML, codifiche Unicode e spazi extra'''\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"[\\x00-\\x1F\\x7F]\", \" \", text)\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "    text = re.sub(r\"_x[0-9a-fA-F]{4}_\", \" \", text)\n",
    "    text = re.sub(r\"\\\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_dataset(df):\n",
    "    '''Pulisce il DataFrame rimuovendo righe con campi essenziali mancanti e applicando clean_text'''\n",
    "    # Tieni solo righe con i campi essenziali\n",
    "    df = df.dropna(subset=[\"title\", \"abstract\", \"review\"]).reset_index(drop=True)\n",
    "\n",
    "    # Applica clean_text su tutte le colonne testuali\n",
    "    for col in [\"title\", \"abstract\", \"review\"]:\n",
    "        df[col] = df[col].map(clean_text)\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_response(text):\n",
    "    \"\"\"Rimuove token speciali e fallback in caso di errore\"\"\"\n",
    "    if \"<|start_header_id|>assistant<|end_header_id|>\\n\" in text:\n",
    "        text = text.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\", 1)[-1]\n",
    "    return text.replace(\"<|eot_id|>\", \"\").strip()\n",
    "\n",
    "def extract_number(text):\n",
    "    \"\"\"Estrae numero da rate/confidence tipo 'Rating:###7: ...' \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    match = re.search(r\"(\\d+)\", str(text))\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "# APPLY CLEANING\n",
    "grouped = clean_dataset(grouped)\n",
    "\n",
    "# Rimozione del campo decision (evita leakage)\n",
    "if \"decision\" in grouped.columns:\n",
    "    grouped = grouped.drop(columns=[\"decision\"])\n",
    "\n",
    "# Normalizzazione numerica rate/confidence\n",
    "grouped[\"rating_num\"] = grouped[\"rate\"].apply(lambda lst: extract_number(lst[0]) if isinstance(lst, list) and lst else None)\n",
    "grouped[\"confidence_num\"] = grouped[\"confidence\"].apply(lambda lst: extract_number(lst[0]) if isinstance(lst, list) and lst else None)\n",
    "\n",
    "# Split into train/val/test\n",
    "train_val, test = train_test_split(grouped, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train_val, test_size=0.1, random_state=42)\n",
    "\n",
    "# Save for future use\n",
    "train.to_csv(\"./data/train.csv\", index=False)\n",
    "val.to_csv(\"./data/val.csv\", index=False)\n",
    "test.to_csv(\"./data/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9643f16",
   "metadata": {},
   "source": [
    "This cell initializes the tokenizer and loads the Llama-3 model for causal language modeling. It sets up quantization with BitsAndBytes for efficient memory usage and configures the tokenizer for padding and device placement. The model is loaded with automatic device mapping to utilize available GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdabd9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- TOKENIZER & MODEL LLAMA ----------------------\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",  # this will choose GPU\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92289294",
   "metadata": {},
   "source": [
    "This cell prints the device map used by the loaded Llama-3 model, showing how model layers are distributed across available hardware (such as GPUs or CPUs). This helps verify that the model is utilizing the intended devices for inference and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dac6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0}\n"
     ]
    }
   ],
   "source": [
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df96dee3",
   "metadata": {},
   "source": [
    "This cell defines the function used to build prompts for the language model. It formats the paper's title, abstract, and review into a structured input, instructing the model to generate strengths, weaknesses, a numeric rating, and a confidence score in a specific format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8709e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- PROMPT FUNCTION ----------------------\n",
    "def build_prompt(example):\n",
    "    title = clean_text(example[\"title\"])\n",
    "    abstract = clean_text(example[\"abstract\"])\n",
    "    review = clean_text(example[\"review\"])\n",
    "    return (\n",
    "        f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        f\"Title: {title}\\n\"\n",
    "        f\"Abstract: {abstract}\\n\"\n",
    "        f\"Review: {review}\\n\"\n",
    "        \"Please write a structured peer review with this exact format:\\n\\n\"\n",
    "        \"Strengths:\\n1. ...\\n2. ...\\n\\n\"\n",
    "        \"Weaknesses:\\n1. ...\\n2. ...<|eot_id|>\\n\"\n",
    "        \"Then, give a numeric rating (1-10) and a confidence score (1-5) in this format:\\n\"\n",
    "        \"Rating: <number>\\n\"\n",
    "        \"Confidence: <number><|eot_id|>\\n\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bc2657",
   "metadata": {},
   "source": [
    "This cell provides functions to generate structured peer reviews using the language model. It takes each paper, builds a prompt, and uses the model to produce strengths, weaknesses, rating, and confidence predictions. The second function applies this process to an entire DataFrame, saving the generated reviews and handling errors during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c0a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- GENERATE STRENGTHS AND WEAKNESSES (both zero-shot/ft) ----------------------\n",
    "def generate_review(model, tokenizer, paper, max_new_tokens=300):\n",
    "    prompt = build_prompt(paper)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    decoded = clean_response(decoded)\n",
    "    return decoded\n",
    "\n",
    "def generate_reviews_for_df(df, model, tokenizer, output_col=\"generated_review\", output_file=None):\n",
    "    preds = []\n",
    "\n",
    "    print(f\"Generating predictions for {len(df)} papers\")\n",
    "\n",
    "    model.eval()\n",
    "    model.config.use_cache = True\n",
    "    if hasattr(model, \"gradient_checkpointing_disable\"):\n",
    "        model.gradient_checkpointing_disable()\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            response = generate_review(model, tokenizer, row)\n",
    "            preds.append(response)\n",
    "        except Exception as e:\n",
    "            preds.append(f\"[ERROR: {e}]\")\n",
    "            print(f\"❌ Error on row {i}: {e}\")\n",
    "\n",
    "    df[output_col] = preds\n",
    "\n",
    "    if output_file:\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"✅ Saved to {output_file}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589a67c",
   "metadata": {},
   "source": [
    "These two cells generate structured peer reviews for the training and validation sets using the base language model. They clean the training and validation data, apply the review generation function, and save the resulting structured reviews to two CSV files for use as supervised targets in fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b3440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- STRUCTURE REVIEW (Distillation) ----------------------\n",
    "\n",
    "# Generate structured reviews (this is your supervised target)\n",
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "\n",
    "#CLEANING\n",
    "train= clean_dataset(train)\n",
    "train_df = generate_reviews_for_df(train, model, tokenizer, output_file=\"train_structured.csv\")\n",
    "train_df.to_csv(\"./data/train_structured.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e63c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv(\"./data/val.csv\")\n",
    "val = clean_dataset(val)\n",
    "val_df = generate_reviews_for_df(val, model, tokenizer, output_file=\"val_structured.csv\")\n",
    "val_df.to_csv(\"./data/val_structured.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63282d9d",
   "metadata": {},
   "source": [
    "This cell configures the model for parameter-efficient fine-tuning using LoRA (Low-Rank Adaptation). It prepares the model for k-bit training and sets up the LoRA configuration, specifying target modules and hyperparameters. The LoRA adapter is then applied to the model to enable efficient adaptation during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b12e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- LoRA CONFIG ----------------------\n",
    "\n",
    "model_ft = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model_ft = get_peft_model(model_ft, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4cbf38",
   "metadata": {},
   "source": [
    "This cell prepares the training and validation datasets for fine-tuning. It builds prompt-response pairs from the generated reviews, tokenizes the data for the language model, and removes unnecessary columns to create datasets suitable for supervised training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744762e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- FINE-TUNING PREP ----------------------\n",
    "def ft_prompt(example):\n",
    "    prompt = build_prompt(example)\n",
    "    response = example[\"generated_review\"].strip() + \"\\n<|eot_id|>\"\n",
    "    return {\"prompt\": prompt, \"response\": response}\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df).map(ft_prompt)\n",
    "val_dataset = Dataset.from_pandas(val_df).map(ft_prompt)\n",
    "\n",
    "def tokenize(example):\n",
    "    tokens= tokenizer(\n",
    "        example[\"prompt\"] + example[\"response\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=300\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize)\n",
    "val_dataset = val_dataset.map(tokenize)\n",
    "\n",
    "drop_cols = list(set(train_df.columns) | {\"prompt\", \"response\"})\n",
    "train_dataset = train_dataset.remove_columns([c for c in drop_cols if c in train_dataset.column_names])\n",
    "val_dataset = val_dataset.remove_columns([c for c in drop_cols if c in val_dataset.column_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec5356",
   "metadata": {},
   "source": [
    "This cell sets up and runs the training loop for fine-tuning the model using the Hugging Face Trainer API. It defines training arguments such as batch size, evaluation strategy, number of epochs, and learning rate, then starts the training process and saves the fine-tuned model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- TRAINING ----------------------\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned-llama3\",\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_ft,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "model_ft.save_pretrained(\"finetuned-llama3-lora\")\n",
    "tokenizer.save_pretrained(\"finetuned-llama3-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbf9501",
   "metadata": {},
   "source": [
    "This cell performs inference on the test set using the base language model. It cleans the test data, generates zero-shot structured peer reviews, and saves the predictions to a CSV file for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac57db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- INFERENCE ON TEST SET ----------------------\n",
    "# Generate zero-shot reviews\n",
    "test_df = pd.read_csv(\"./data/test.csv\")\n",
    "test_df = clean_dataset(test_df)\n",
    "\n",
    "generate_reviews_for_df(\n",
    "    df=test_df,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    output_col=\"zero_shot_review\",\n",
    "    output_file=\"zero_shot_predictions.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d1779",
   "metadata": {},
   "source": [
    "This cell generates structured peer reviews for the test set using the fine-tuned model. It cleans the test data, applies the review generation function, and saves the resulting predictions to a CSV file for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reviews using fine-tuned model\n",
    "\n",
    "test_df = pd.read_csv(\"./data/test.csv\")\n",
    "test_df = clean_dataset(test_df)\n",
    "\n",
    "generate_reviews_for_df(\n",
    "    df=test_df, \n",
    "    model=model_ft, \n",
    "    tokenizer=tokenizer, \n",
    "    output_col=\"fine_tuned_review\", \n",
    "    output_file=\"finetuned_predictions.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f757f",
   "metadata": {},
   "source": [
    "This cell evaluates the zero-shot and fine-tuned model predictions using regression metrics. It extracts numeric ratings and confidence scores from the generated reviews, computes metrics such as MAE, RMSE, R², Pearson, and Spearman correlations, and calculates BERTScore to assess the textual similarity between generated and reference reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd4e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- EVALUATION ----------------------\n",
    "\n",
    "# Carica predizioni\n",
    "zero = pd.read_csv(\"./notebooks/zero_shot_predictions.csv\")\n",
    "ft = pd.read_csv(\"./notebooks/finetuned_predictions.csv\")\n",
    "\n",
    "# Funzione per estrarre numeri dal testo\n",
    "def extract_pred_numbers(text):\n",
    "    rating, confidence = None, None\n",
    "    if isinstance(text, str):\n",
    "        match_r = re.search(r\"Rating:\\s*(\\d+)\", text)\n",
    "        match_c = re.search(r\"Confidence:\\s*(\\d+)\", text)\n",
    "        if match_r:\n",
    "            rating = int(match_r.group(1))\n",
    "        if match_c:\n",
    "            confidence = int(match_c.group(1))\n",
    "    return rating, confidence\n",
    "\n",
    "# Aggiungi colonne predette\n",
    "zero[[\"rating_pred\", \"confidence_pred\"]] = zero[\"zero_shot_review\"].apply(\n",
    "    lambda x: pd.Series(extract_pred_numbers(x))\n",
    ")\n",
    "ft[[\"rating_pred\", \"confidence_pred\"]] = ft[\"fine_tuned_review\"].apply(\n",
    "    lambda x: pd.Series(extract_pred_numbers(x))\n",
    ")\n",
    "\n",
    "# Funzione per regression metrics\n",
    "def regression_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    pr, _ = pearsonr(y_true, y_pred)\n",
    "    return dict(MAE=mae, RMSE=rmse, R2=r2, Pearson=pr)\n",
    "\n",
    "# Drop rows with missing predictions\n",
    "zero_eval = zero.dropna(subset=[\"rating_pred\", \"confidence_pred\"])\n",
    "ft_eval = ft.dropna(subset=[\"rating_pred\", \"confidence_pred\"])\n",
    "\n",
    "print(\"Zero-shot usable samples:\", len(zero_eval), \"/\", len(zero))\n",
    "print(\"Fine-tuned usable samples:\", len(ft_eval), \"/\", len(ft))\n",
    "\n",
    "# Rating\n",
    "print(\"Zero-shot Rating Metrics:\", regression_metrics(zero_eval[\"rating_num\"], zero_eval[\"rating_pred\"]))\n",
    "print(\"Fine-tuned Rating Metrics:\", regression_metrics(ft_eval[\"rating_num\"], ft_eval[\"rating_pred\"]))\n",
    "\n",
    "# Confidence\n",
    "print(\"Zero-shot Confidence Metrics:\", regression_metrics(zero_eval[\"confidence_num\"], zero_eval[\"confidence_pred\"]))\n",
    "print(\"Fine-tuned Confidence Metrics:\", regression_metrics(ft_eval[\"confidence_num\"], ft_eval[\"confidence_pred\"]))\n",
    "\n",
    "# BERTScore su review testuali\n",
    "P, R, F1 = bertscore(\n",
    "    cands=ft[\"fine_tuned_review\"].fillna(\"\").tolist(),\n",
    "    refs=ft[\"review\"].fillna(\"\").tolist(),\n",
    "    lang=\"en\"\n",
    ")\n",
    "print(\"Fine-tuned BERTScore F1:\", F1.mean().item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
