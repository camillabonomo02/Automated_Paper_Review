title,abstract,review,rate,confidence,rating_num,confidence_num,generated_review
Prototypical Networks for Few-shot Learning | OpenReview,"A recent approach to few-shot classification called matching networks has demonstrated the benefits of coupling metric learning with a training procedure that mimics test. This approach relies on a complicated fine-tuning procedure and an attention scheme that forms a distribution over all points in the support set, scaling poorly with its size. We propose a more streamlined approach, prototypical networks, that learns a metric space in which few-shot classification can be performed by computing Euclidean distances to prototype representations of each class, rather than individual points. Our method is competitive with state-of-the-art one-shot classification approaches while being much simpler and more scalable with the size of the support set. We empirically demonstrate the performance of our approach on the Omniglot and mini-ImageNet datasets. We further demonstrate that a similar idea can be used for zero-shot learning, where each class is described by a set of attributes, and achieve state-of-the-art results on the Caltech UCSD bird dataset.","The paper is an extension of the matching networks by Vinyals et al. in NIPS2016. Instead of using all the examples in the support set during test, the method represents each class by the mean of its learned embeddings. The training procedure and experimental setting are very similar to the original matching networks. I am not completely sure about its advantages over the original matching networks. It seems to me when dealing with 1-shot case, these two methods are identical since there is only one example seen in this class, so the mean of the embedding is the embedding itself. When dealing with 5-shot case, original matching networks compute the weighted average of all examples, but it is at most 5x cost. The experimental results reported for prototypical nets are only slightly better than matching networks. I think it is a simple, straightforward, novel extension, but I am not fully convinced its advantages.  This paper proposes an improved version of matching networks, with better scalability properties with respect to the support set of a few-shot classifier. Instead of considering each support point individually, they learn an embedding function that aggregates over items of each class within the support set (eq. 1). This is combined with episodic few-shot training with randomly-sampled partitions of the training set classes, so that the training and testing scenarios match closely. Although the idea is quite straightforward, and there are a great many prior works on zero-shot and few-shot learning, the proposed technique is novel to my knowledge, and achieves state-of-the-art results on several benchmark datasets. One addition that I think would improve the paper is a clearer description of the training algorithm (perhaps pseudocode). In its current form the paper a bit vague about this.  *** Paper Summary *** This paper simplify matching network by considering only a single prototype per class which is obtained as the average of the embedding of the training class samples. Empirical comparisons with matching networks are reported. *** Review *** The paper reads well and clearly motivate the work. This work of learning metric learning propose to simplify an earlier work (matching network) which is a great objective. However, I am not sure it achieve better results than matching networks. The space of learning embeddings to optimize nearest neighbor classification has been explored before, but the idea of averaging the propotypes is interesting (as a non-linear extension of Mensink et al 2013). I would suggest to improve the discussion of related work and to consolidate the results section to help distinguish between the methods you outperform and the one you do not. The related work section can be extended to include work on learning distance metric to optimize a nearest neighbor classification, see Weinberger et al, 2005 and subsequent work. Extensions to perform the same task with neural networks can be found in Min et al, 09 that purse a goal very close to yours. Regarding approaches pursuing similar goals with a different learning objective, you cite siamese network with pairwise supervision. The learning to rank (for websearch) litterature with triplet supervision or global ranking losses is also highly relevant, ie. one example *the query* defines the class and the embedding space need to be such that positive/relevant document are closer to the query than the others. I would suggest to start with Chris Burges 2010 tutorial. One learning class I am not sure the reported results correctly reflect the state of the art for all tasks. The results are positive on Omniglot but I feel that you should also report the better results of matching networks on miniImageNet with fine tuning and full contextual embeddings. It can be considered misleading not to report it. On Cub 200, I thought that the state-of-the-art was 50.1%, when using features from GoogLeNet (Akata et al 2015), could you comment on this? Overall, paper could greatly be improved, both in the discussion of related work and with a less partial reporting of prior empirical results. *** References *** Large Margin Nearest Neighbors. Weinberger et al, 2005 From RankNet to LambdaRank to LambdaMART: An Overview, Chris J.C. Burges, June 23, 2010 A Deep Non-linear Feature Mapping for Large-Margin kNN Classification, Min et al, 09","['Rating:###5: Marginally below acceptance threshold', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###4: Ok but not good enough - rejection']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",5,3,"Here is the structured peer review:

**Strengths:**

1. The paper proposes a novel approach to few-shot learning, which is a significant contribution to the field.
2. The authors provide a clear and concise explanation of the method, making it easy to understand for a broad audience.
3. The experimental results are well-organized and easy to compare with existing state-of-the-art methods.
4. The paper highlights the scalability of the proposed method, which is an important aspect of few-shot learning.

**Weaknesses:**

1. The paper does not provide a clear comparison with the original matching networks, which is a significant omission.
2. The discussion of related work is limited, and the authors could benefit from a more comprehensive review of the literature.
3. The experimental results are not entirely clear, and some results seem to be missing or inconsistent.
4. The paper could benefit from more detailed explanations of the training algorithm and hyperparameter tuning.
5. The authors' claim of state-of-the-art results on some datasets is not entirely supported by the provided evidence.

**Rating:** 7
**Confidence:** 3

Note: The rating is based on the overall quality of the paper, and the confidence score reflects the reviewer's confidence in the paper's claims. A rating of 7 indicates that the paper is of good quality, but with some significant weaknesses. A confidence score of 3 indicates that the reviewer is moderately confident in the paper's claims, but has some"
Program Synthesis for Character Level Language Modeling | OpenReview,"We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data - the process is done via counting, as in simple language models such as n-gram. Our experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.","This paper proposes an approach to character language modeling (CLMs) based on developing a domain specific language to represent CLMs. The experiments show mixed performance versus neural CLM approaches to modeling linux kernel data and wikipedia text, however the proposed DSL models are slightly more compact and fast to query as compared with neural CLMs. The proposed approach is difficult to understand overall and perhaps is aimed towards the sub-community already working on this sort of approach but lacks sufficient explanation for the ICLR audience. Critically the paper glosses over the major issues of demonstrating the proposed DSL is a valid probabilistic model and how training is performed to fit the model to data (there is clearly not a gradient-based training approach used). FInally the experiments feel incomplete without showing samples drawn from the generative model or analyzing the learned model to determine what it has learned. Overall I feel this paper does not describe the approach in enough depth for readers to understand or re-implement it. Almost all of the model section is devoted to exposition of the DSL without specifying how probabilities are computed using this model and how training is performed. How are probabilities actually encoded? The DSL description seems to have only discrete decisions rather than probabilities. Training is perhaps covered in previous papers but there needs to be some discussion of how it works here. Section 2.5 does not do enough to explain how training works or how any measure of optimality is achieved. Given this model is quite a different hypothesis space from neural models or n-grams, looking and samples drawn from the model seems critical. The current experiments show it can score utterances relatively well but it would be very interesting if the model can sample more structured samples than neural approaches (for example long-range syntax constraints like brackets)  The authors propose a method for language modeling by first generating a program from a DSL, then learning the count-based parameters of that program. Pros include: The proposed method is innovative and highly different from standard LSTM-based approaches of late. The model should also be much quicker to apply at query time. Strong empirical results are obtained on modeling code, though there is some gap between the synthesis method and neural methods on the Hutter task. A detailed description of the language syntax is provided. Cons/suggestions: - The synthesis procedure using MCMC is left very vague, even though being able to make this procedure efficient is one of the key questions. - The work builds on work from the PL literature; surely the related work could also be expanded and this work better put in context. - More compact/convincing examples of human interpretability would be helpful. Other comments - Training time evaluation in Table 1 should give basic information such as whether training was done on GPU/CPU, CPU specs, etc.  This paper introduces a novel method for language modeling which is suitable for both modeling programming language as well as natural language. The approach uses a program synthesis algorithm to search over program space and uses count-based estimation of the weights of the program. This is a departure from neural network-based approaches which rely on gradient descent, and thus are extremely slow to estimate. Count-based method such as regular n-gram models suffer because of their simplicity, i.e. not being able to model large context, and scaling badly as context increases. The proposed approach synthesizes programs using MCMC which learn context-sensitive probabilities using count-based estimation, and thus is both fast and able to model long-range context. Experiments on a programming language datasets, the linux kernel corpus, show that this method is vastly better than both LSTM and n-gram language models. Experiments on the Wikipedia corpus show that the method is competitive, but not better, to SOTA models. Both estimation and query time are significantly better than LSTM LMs, and competitive to n-gram LMs. It*s debatable whether this paper is suitable for ICLR, due to ICLR*s focus on neural network-based approaches. However, in the interest of diversity and novelty, such *outside* papers should be accepted to ICLR. This paper is likely to inspire more research into fusion of program synthesis and machine learning methods, which was a popular theme at NIPS 2016. *Pros* 1. Novel approach. 2. Good results. *Cons* 1. Some significant algorithmic details are not included in the paper. They should at least be included in an appendix for comprehensiveness. *Comments* 1. Please include n-gram results in the table for Wikipedia results.","['Rating:###5: Marginally below acceptance threshold', 'Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###8: Top 50% of accepted papers, clear accept']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",5,3,"Here is the structured peer review:

**Strengths:**

1. The paper introduces a novel approach to character-level language modeling, which is suitable for both programming languages and natural languages. This is a significant contribution to the field.
2. The approach uses a program synthesis algorithm to search over program space and uses count-based estimation of the weights of the program, which is a departure from neural network-based approaches.
3. The experiments show that the method is vastly better than both LSTM and n-gram language models, especially in terms of estimation and query time.
4. The paper provides a detailed description of the language syntax and the synthesis procedure using MCMC.

**Weaknesses:**

1. The paper lacks a clear explanation of how probabilities are computed using the proposed DSL, and how training is performed to fit the model to data. The training process is not clearly described, and it is unclear how the model is optimized.
2. The synthesis procedure using MCMC is left very vague, and it is unclear how to make this procedure efficient.
3. The paper does not provide enough context for the related work, and it would be beneficial to expand on the connections to the PL literature.
4. The paper could benefit from more compact and convincing examples of human interpretability, as the DSL description is not very intuitive.
5. The evaluation of the model on the Wikipedia corpus is not sufficient, as it is not clear how the model performs on more complex and diverse data.

**Rating"
Geometry of Polysemy | OpenReview,"Vector representations of words have heralded a transformational approach to classical problems in NLP; the most popular example is word2vec. However, a single vector does not suffice to model the polysemous nature of many (frequent) words, i.e., words with multiple meanings. In this paper, we propose a three-fold approach for unsupervised polysemy modeling: (a) context representations, (b) sense induction and disambiguation and (c) lexeme (as a word and sense pair) representations. A key feature of our work is the finding that a sentence containing a target word is well represented by a low-rank subspace, instead of a point in a vector space. We then show that the subspaces associated with a particular sense of the target word tend to intersect over a line (one-dimensional subspace), which we use to disambiguate senses using a clustering algorithm that harnesses the Grassmannian geometry of the representations. The disambiguation algorithm, which we call -Grassmeans, leads to a procedure to label the different senses of the target word in the corpus -- yielding lexeme vector representations, all in an unsupervised manner starting from a large (Wikipedia) corpus in English. Apart from several prototypical target (word,sense) examples and a host of empirical studies to intuit and justify the various geometric representations, we validate our algorithms on standard sense induction and disambiguation datasets and present new state-of-the-art results.","On the plus side, the paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold). On the minus side, the paper mostly ignores the long history of Word Sense Induction (WSI) and Word Sense Disambiguation (WSD), citing and comparing only some relatively recent papers. The experiments in this paper done on SemEval-2010 are not very persuasive. (It*s difficult to evaluate the experiments done on the 2016 data, since they are not directly comparable to published results). For example, going back to the SemEval-2010 WSI task in [1], the best system seems to be UoY [2]. The F-measure seems to be a poor metric: always assigning one sense to every word (*MFS*) yields the highest F-measure of 63.5%. The paper*s result with *2 clusters* (with an average of about 1.9) seems to be close to MFS. So I don*t think we can use F-measure to compare. The V-measure seems to be tilted towards systems that have high number of senses per word. UoY has V=15.7%, while the paper (with *5 clusters*) has 14.4%. That isn*t very convincing that the proposed method has captured the geometry of polysemy. In general, I have often wondered why people work on pure unsupervised WSI and WSD. The assessment is very difficult (as described above). More importantly, some very weakly supervised systems (with minimal labels) can work pretty well to bootstrap. See, e.g., the classic paper [3]. If the authors used the Grassmannian idea to solve higher-level NLP problems directly (such as analogies), that would be very persuasive. However, that*s a very different paper than what was submitted. For an example of application of Grassmannian manifolds to analogies, see [4]. References: 1. Manandhar, Suresh, et al. *SemEval-2010 task 14: Word sense induction & disambiguation.* Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010. 2. Korkontzelos, Ioannis, and Suresh Manandhar. *Uoy: Graphs of unambiguous vertices for word sense induction and disambiguation.* Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010. 3. Yarowsky, David. *Unsupervised word sense disambiguation rivaling supervised methods.* Proceedings of the 33rd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1995. 4. Mahadevan, Sridhar, and Sarath Chandar Reasoning about Linguistic Regularities in Word Embeddings using Matrix Manifolds https://arxiv.org/abs/1507.07636  This paper describe a new method to capture word polysemy with word embeddings. In order to disambiguate a word in a given sentence, the word is represented by the subspace spanned by the word vectors of the context in which it appears. This departs from a traditional approach were the context is represented as a (weighted) sum of the word vectors. A clustering algorithm (very similar to k-means), is then used to cluster the different usages of a given word, and discover the different senses (each sense corresponding to a cluster). The proposed method is evaluated on various word sense induction datasets. It is compared to other word embedding techniques which model word polysemy. The method proposed in the paper to represent words in context is really interesting, simple to apply and seems very effective, based on the strong experimental results reported in the paper. My main concern about this paper is the writing, which is sometimes a bit verbose, making it hard to follow the description of the method. Some of the justification (*intersection hypothesis*, *polysemy intersection hypothesis*) might feel a bit like hand waving. Overall, the work presented in the paper looks solid. Pros: - I really liked the idea of representing a word in context by a subspace (as opposed to a weighted sum). Indeed, such representations captures much more information than a single vector. - The proposed method also obtain very good results, compared to existing polysemous word embeddings. - It can be used with any word vectors, making its application very easy. Cons: - I felt that the paper is sometimes a bit verbose and some justifications might be a bit hand waving. - I am also wondering how much of the improvement over existing approaches is due to the quality of the word2vec embeddings, or due to the proposed approach. It would therefore be nice to have a comparison with a regular k-means approach, where context are represented as sum of word vectors using the same embeddings.  This paper presents a study of the spaces around existing word embeddings. I proposes something unorthodox: instead of representing a word token by a vector, represent it by the subspace spanned by embeddings of the context word types around that token. These subspaces are fairly low-dimensional and are shown to capture some notions of polysemy (subspaces for tokens of the same sense should all roughly intersect in the same direction). While thinking about the subspace spanned by the context is fairly similar to thinking about a linear combination of the context embeddings, the subspace picture allows for a little more information to be preserved which can improve downstream semantic tasks. The paper is a little dense reading at times, and some things are hard to understand, but the perspective is original enough and the results are good enough that I think it belongs in ICLR.","['Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",7,3,"Here is the structured peer review:

**Strengths:**

1. The paper proposes a novel approach to capturing polysemy in word embeddings, which is a significant contribution to the field of natural language processing.
2. The idea of representing a word in context by a subspace spanned by the context word vectors is interesting and seems to capture more information than traditional vector representations.
3. The experimental results show that the proposed method can achieve good performance on word sense induction and disambiguation tasks.
4. The paper provides a clear and concise description of the method, making it easy to follow and understand.

**Weaknesses:**

1. The paper ignores the long history of Word Sense Induction (WSI) and Word Sense Disambiguation (WSD) research, which is a significant omission.
2. The evaluation metrics used in the paper are not robust, as the F-measure metric is not well-suited for this task, and the V-measure metric may not accurately reflect the performance of the proposed method.
3. The paper does not provide a clear comparison to existing polysemous word embeddings, which makes it difficult to assess the relative performance of the proposed method.
4. The writing style is sometimes verbose, which can make it difficult to follow the description of the method.

**Rating:** 7/10
**Confidence:** 3/5

Note: The rating and confidence score reflect the reviewer's overall assessment of the paper's strengths and"
Demystifying ResNet | OpenReview,"We provide a theoretical explanation for the superb performance of ResNet via the study of deep linear networks and some nonlinear variants. We show that with or without nonlinearities, by adding shortcuts that have depth two, the condition number of the Hessian of the loss function at the zero initial point is depth-invariant, which makes training very deep models no more difficult than shallow ones. Shortcuts of higher depth result in an extremely flat (high-order) stationary point initially, from which the optimization algorithm is hard to escape. The 1-shortcut, however, is essentially equivalent to no shortcuts. Extensive experiments are provided accompanying our theoretical results. We show that initializing the network to small weights with 2-shortcuts achieves significantly better results than random Gaussian (Xavier) initialization, orthogonal initialization, and shortcuts of deeper depth, from various perspectives ranging from final loss, learning dynamics and stability, to the behavior of the Hessian along the learning process.","I think the write-up can be improved. The results of the paper also might be somewhat misleading. The behavior for when weights are 0 is not revealing of how the model works in general. I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model.  This paper studies the optimization issue of linear ResNet, and shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth. I skimmed through the proof but have not checked them carefully. This result is a nice observation for training deep linear networks. But I do not think the paper has fully resolved the linear vs nonlinear issue. Some question: 1. Though the revision has added some results using ReLU units, it seems it is only added to the mid positions of the network (sec 5.3), is this how it is typically done in ResNet? Moreover, ReLU is not differentiable at zero point, which does not satisfy the condition you had in Theorem 1. Why not use differentiable activations like sigmoid or tanh? 2. From equation (22) in the appendix, it seems for nonlinear activations, the condition number depends on the derivative sigma^prime at 0. Therefore, if we use tanh which has derivative 1 at zero, the condition number is the same for linear and tanh activations. But this probably is not enough to explain the bit difference in performance or optimization for linear and nonlinear networks, or how the situations evolve after learning the 0 point. 3. As for the success of ResNet (or convnets in general) in computer vision, I believe there are more types of nonlinearity such as pooling? Can the result here generalizes to pooling as well? Minor: - sec 1 last paragraph, low approximation error typically means more powerful model class and better training error, but not necessarily better test error - sec 4.1 what do you mean by *zero initialization with small random perturbations*? why not exactly zero initialization, how large is the random perturbation?  ResNet and other architectures that use shortcuts have shown empirical success in several domains and therefore, studying the optimization for such architectures is very valuable. This paper is an attempt to address some of the properties of networks that use shortcuts. Some of the experiments in the paper are interesting. However, there are two main issues with the current paper: 1- linear vs non-linear: I think studying linear networks is valuable but we should be careful not to extend the results to networks with non-linear activations without enough evidence. This is especially true for Hessian as the Hessian of non-linear networks have very large condition number (see the ICLR submission *Singularity of Hessian in Deep Learning*) even in cases where the optimization is not challenging. Therefore, I don*t agree with the claims in the paper on non-linear networks. Moreover, one plot on MNIST is not enough to claim that non-linear networks behave similar to linear networks. 2- Hessian at zero initial point: The explanation of why we should be interested in Hessain at zero initial point is not acceptable. The zero initial point is not interesting because it is a very particular point that cannot tell us about the Hessian during optimization.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###4: Ok but not good enough - rejection']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",4,4,"Here is a structured peer review of the paper:

**Strengths:**

1. The paper provides a theoretical explanation for the performance of ResNet, which is a valuable contribution to the field.
2. The authors provide a clear and concise explanation of the concept of shortcuts and their impact on the Hessian of the loss function.
3. The experiments provided in the paper are interesting and demonstrate the effectiveness of the proposed approach.

**Weaknesses:**

1. The paper's results may be misleading, as the behavior of the model at the zero initial point does not necessarily generalize to other initial conditions or non-linear networks.
2. The paper underestimates the effect of non-linearities on the learning dynamics of the model, as the Hessian of non-linear networks can have a large condition number even when the optimization is not challenging.
3. The paper does not fully address the issue of linear vs non-linear networks, as the results may not generalize to networks with non-linear activations.
4. The paper's explanation of why the Hessian at the zero initial point is interesting is not convincing, as this point is not representative of the Hessian during optimization.
5. The paper's claim that non-linear networks behave similarly to linear networks is not supported by the provided evidence, as a single plot on MNIST is not enough to make this claim.

**Rating:** 6
**Confidence:** 3

Note: The rating and confidence score reflect the reviewer's overall assessment of the paper"
Exploring Sparsity in Recurrent Neural Networks | OpenReview,"Recurrent neural networks (RNN) are widely used to solve a variety of problems and as the quantity of data and the amount of available compute have increased, so have model sizes. The number of parameters in recent state-of-the-art networks makes them hard to deploy, especially on mobile phones and embedded devices. The challenge is due to both the size of the model and the time it takes to evaluate it. In order to deploy these RNNs efficiently, we propose a technique to reduce the parameters of a network by pruning weights during the initial training of the network. At the end of training, the parameters of the network are sparse while accuracy is still close to the original dense neural network. The network size is reduced by 8× and the time required to train the model remains constant. Additionally, we can prune a larger dense network to achieve better than baseline performance while still reducing the total number of parameters significantly. Pruning RNNs reduces the size of the model and can also help achieve significant inference time speed-up using sparse GEMMs. Benchmarks show that using our technique model size can be reduced by 90% and speed-up is around 2× to 7×.","The paper proposes a method for pruning weights in neural networks during training to obtain sparse solutions. The approach is applied to an RNN-based system which is trained and evaluated on a speech recognition dataset. The results indicate that large savings in test-time computations can be obtained without affecting the task performance too much. In some cases the method can actually improve the evaluation performance. The experiments are done using a state-of-the-art RNN system and the methodology of those experiments seems sound. I like that the effect of the pruning is investigated for networks of very large sizes. The computational gains are clearly substantial. It is a bit unfortunate that all experiments are done using a private dataset. Even with private training data, it would have been nice to see an evaluation on a known test set like the HUB5 for conversational speech. It would also have been nice to see a comparison with some other pruning approaches given the similarity of the proposed method to the work by Han et al. [2] to verify the relative merit of the proposed pruning scheme. While single-stage training looks more elegant at first sight, it may not save much time if more experiments are needed to find good hyperparameter settings for the threshold adaptation scheme. Finally, the dense baseline would have been more convincing if it involved some model compression tricks like training on the soft targets provided by a bigger network. Overall, the paper is easy to read. The table and figure captions could be a bit more detailed but they are still clear enough. The discussion of potential future speed-ups of sparse recurrent neural networks and memory savings is interesting but not specific to the proposed pruning algorithm. The paper doesn’t motivate the details of the method very well. It’s not clear to me why the threshold has to ramp up after a certain period time for example. If this is based on preliminary findings, the paper should mention that. Sparse neural networks have been the subject of research for a long time and this includes recurrent neural networks (e.g., sparse recurrent weight matrices were standard for echo-state networks [1]). The proposed method is also very similar to the work by Han et al. [2], where a threshold is used to prune weights after training, followed by a retraining phase of the remaining weights. While I think that it is certainly more elegant to replace this three stage procedure with a single training phase, the proposed scheme still contains multiple regimes that resemble such a process by first training without pruning followed by pruning at two different rates and finally training without further pruning again. The main novelty of the work would be the application of such a scheme to RNNs, which are typically more tricky to train than feedforward nets. Improving scalability is an important driving force of the progress in neural network research. While I don’t think the paper presents much novelty in ideas or scientific insight, it does show that weight pruning can be successfully applied to large practical RNN systems without sacrificing much in performance. The fact that this is possible with such a simple heuristic is a result worth sharing. Pros: The proposed method is successful at reducing the number of parameters in RNNs substantially without sacrificing too much in performance. The experiments are done using a state-of-the-art system for a practical application. Cons: The proposed method is very similar to earlier work and barely novel. There is no comparison with other pruning methods. The data is private and this prevents others from replicating the results. [1] Jaeger, H. (2001). The “echo state” approach to analyzing and training recurrent neural networks-with an erratum note. Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148, 34. [2] Han, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for efficient neural networks. In Advances in Neural Information Processing Systems, 2015.  Summary: The paper presents a technique to convert a dense to sparse network for RNNs. The algorithm will increasingly set more weights to zero during the RNN training phase. This provides a RNN model with less storage requirement and higher inference rate. Pros: Proposes a pruning method that doesn’t need re-training and doesn’t affect the training phase of RNN. The method achieves 90% sparsity, and hence less number of parameters. Cons & Questions: Judiciously choosing hyper parameters for different models and different applications wouldn’t be cumbersome? In equation 1, is q the sparsity of final model? Is there a formula to know what is sparsity, number of parameters and accuracy of final model given a set of hyper parameters, before going through training? (Questions answered) In table3, we see a trade-off between number of units and sparsity to achieve better number of parameters or accuracy, or in table5 better speed. Good, but where are the results for GRU sparse big? I mean, accuracy must be similar and still get decent compression rate and speed up. Just like RNN Sparse medium compared with RNN Dense. I can’t see much advantage of pruning and getting high speed-up if you are sacrificing so much accuracy. (Issue fixed with updated data) Why sparsity for table3 and table5 are different? In text: “average sparsity of 88%” but in table5 is 95%? Are the models used in table3 different from table5? (Issue fixed)","['Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",6,4,"Here is the structured peer review:

**Strengths:**

1. The paper presents a practical and effective method for pruning weights in RNNs during training, resulting in significant reductions in model size and inference time.
2. The experiments are well-designed and demonstrate the effectiveness of the proposed method, with substantial savings in test-time computations without sacrificing much in performance.
3. The paper provides a clear and concise presentation of the method and its results, making it easy to follow and understand.
4. The authors provide a good discussion on the potential future speed-ups of sparse recurrent neural networks and memory savings, which is an interesting aspect of the research.

**Weaknesses:**

1. The proposed method is very similar to earlier work by Han et al. [2], and the paper could benefit from a more nuanced discussion on the novelty and contributions of the work.
2. The experiments are limited to a private dataset, which makes it difficult to replicate the results and evaluate the generalizability of the method.
3. The paper could benefit from a more detailed explanation of the threshold adaptation scheme and its rationale, as well as a clearer justification for the choice of hyperparameters.
4. The results in tables 3 and 5 are not consistent, with different sparsity values reported for the same model architecture, which may indicate an error or inconsistency in the data.
5. The paper could benefit from a more thorough comparison with other pruning methods, such as those using different thresholding schemes or pruning strategies"
Recurrent Batch Normalization | OpenReview,"We propose a reparameterization of LSTM that brings the benefits of batch normalization to recurrent neural networks. Whereas previous works only apply batch normalization to the input-to-hidden transformation of RNNs, we demonstrate that it is both possible and beneficial to batch-normalize the hidden-to-hidden transition, thereby reducing internal covariate shift between time steps. We evaluate our proposal on various sequential problems such as sequence classification, language modeling and question answering. Our empirical results show that our batch-normalized LSTM consistently leads to faster convergence and improved generalization.","The paper shows that BN, which does not work out of the box for RNNs, can be used with LSTM when the operator is applied to the hidden-to-hidden and the input-to-hidden contribution separately. Experiments are conducted to show that it leads to improved generalisation error and faster convergence. The paper is well written and the idea well presented. i) The data sets and consequently the statistical assumptions used are limited (e.g. no continuous data, only autoregressive generative modelling). ii) The hyper parameters are nearly constant over the experiments. It is ruled out that they have not been picked in favor of one of the methods. E.g. just judging from the text, a different learning rate could have lead to equally fast convergence for vanilla LSTM. Concluding, the experiments are flawed and do not sufficiently support the claim. An exhaustive search of the hyper parameter space could rule that out.  This paper extends batch normalization successfully to RNNs where batch normalization has previously failed or done poorly. The experiments and datasets tackled show definitively the improvement that batch norm LSTMs provide over standard LSTMs. They also cover a variety of examples, including character level (PTB and Text8), word level (CNN question-answering task), and pixel level (MNIST and pMNIST). The supplied training curves also quite clearly show the potential improvements in training time which is an important metric for consideration. The experiment on pMNIST also solidly shows the advantage of batch norm in the recurrent setting for establishing long term dependencies. I additionally also appreciated the gradient flow insight, specifically the impact of unit variance on tanh derivatives. Showing it not just for batch normalization but additionally the *toy task* (Figure 1b) was hugely useful. Overall I find this paper a useful additional contribution to the usage of batch normalization and would be necessary information for successfully employing it in a recurrent setting.  Contributions The paper presents an adaptation of batch normalization for RNNs in the case of LSTMs, along the horizontal depth. Contrary to previous work from (Laurent 2015; Amodei 2016), the work demonstrates that batch-normalizing the hidden states of RNNs can improve optimization, and argues with quantitative experiments that the key factor to making this work is proper initialization of parameters, in particular gamma. Experiments show some gain in performance over vanilla LSTMs on Sequential MNIST, PTB, Text8 and CNN Question-Answering. Novelty+Significance Batch normalization has been key for training deeper and deeper networks (e.g. ResNets) and it seems natural that we would want to extend it to RNNs. The paper shows that it is possible to do so with proper initialization of parameters, contrary to previous work from (Laurent 2015; Amodei 2016). Novelty comes from where to batch norm (i.e. not in the cell update) and in the per-time step statistics. Adding batch normalization to LSTMs incurs additional computational cost and bookkeeping; for training speed comparisons (e.g. Figure 2) the paper only compares LSTM and BN-LSTM by iteration count; given the additional complexity of the BN-LSTM I would have also liked to see a wall-clock comparison. As RNNs are used across many tasks, this work is of interest to many. However, the results gains are generally minor and require several tricks to work in practice. Also, this work doesn’t address a question about batch normalization that it seems natural that it helps with faster training, but why would it also improve generalization? Clarity The paper is overall very clear and well-motivated. The model is well described and easy to understand, and the plots illustrate the points clearly. Summary Interesting though relatively incremental adaptation, but shows batch normalization to work for RNNs where previous works have not succeeded. Comprehensive set of experiments though it is questionable if the empirical gains are significant enough to justify the increased model complexity as well as computational overhead. Pros - Shows batch normalization to work for RNNs where previous works have not succeeded - Good empirical analysis of hyper-parameter choices and of the activations - Experiments on multiple tasks - Clarity Cons - Relatively incremental - Several ‘hacks’ for the method (per-time step statistics, adding noise for exploding variance, sequence-wise normalization) - No mention of computational overhead - Only character or pixel-level tasks, what about word-level?","['Rating:###7: Good paper, accept', 'Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###7: Good paper, accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",7,4,"Here is the structured peer review:

**Strengths:**

1. The paper presents a novel and effective way to apply batch normalization to LSTMs, which is a significant contribution to the field of deep learning.
2. The authors provide a clear and well-motivated explanation of the benefits of batch normalization for LSTMs, and their experiments demonstrate the effectiveness of their approach.
3. The paper covers a comprehensive set of experiments on various tasks, including character-level, word-level, and pixel-level tasks, which provides a good understanding of the applicability of the approach.
4. The authors provide a clear and concise writing style, making the paper easy to understand and follow.
5. The paper includes a useful discussion on the gradient flow insight and the impact of unit variance on tanh derivatives, which adds depth to the understanding of the topic.

**Weaknesses:**

1. The paper relies on a limited set of datasets and experiments, which may not be representative of the broader applicability of the approach.
2. The authors do not provide a thorough analysis of the hyperparameter choices and their impact on the results, which may limit the generalizability of the findings.
3. The paper does not address the question of why batch normalization improves generalization, which is an important aspect of understanding the underlying mechanisms.
4. The paper only covers character-level and pixel-level tasks, and does not explore the applicability of the approach to word-level tasks or other domains.
5. The"
Tree-Structured Variational Autoencoder | OpenReview,"Many kinds of variable-sized data we would like to model contain an internal hierarchical structure in the form of a tree, including source code, formal logical statements, and natural language sentences with parse trees. For such data it is natural to consider a model with matching computational structure. In this work, we introduce a variational autoencoder-based generative model for tree-structured data. We evaluate our model on a synthetic dataset, and a dataset with applications to automated theorem proving. By learning a latent representation over trees, our model can achieve similar test log likelihood to a standard autoregressive decoder, but with the number of sequentially dependent computations proportional to the depth of the tree instead of the number of nodes in the tree.","The method overall seems to be a very interesting structural approach to variational autoencoders, however it seems to lack motivation as well as the application areas sufficient to prove its effectiveness. I see the attractiveness of using structural information in this context and I find it more intuitive than using a flat sequence representation, especially when there is a clear structure in the data. However experimental results seem to fail to be convincing in that regard. One issue is the lack of a variety of applications in general, the experiments seem to be very limited in that regard, considering that the paper itself speaks about natural language applications. It would be interesting to use the latent representations learned with the model for some other end task and see how much it impacts the success of that end task compared to various baselines. In my opinion, the paper has a potentially strong idea however in needs stronger results (and possibly in a wider variety of applications) as a proof of concept.  The authors propose a variational autoencoder for a specific form of tree-generating model. The generative model for trees seems reasonable but is not fully motivated. If no previous references suggest this tree specification, then clear motivation for e.g. the extension beyond CFG should be given beyond the one sentence provided. Given the tree model it may be natural to specify a tree model encoder, but the posterior distribution does not respect the structure of the prior (as the posterior distribution couples tree-distant variables), so there is in fact no good reason for this form, and a more general network could be compared with. The approach provides sensible differentiable functions for encoding the network. The tests are indicative, but the results are very similar to the tested approaches, and it is not clear what the best evaluation metric ought to be. Significance: the work may well be significant in the future, but is currently somewhat preliminary, lacks motivation, chooses a tree structured encoder without particular motivation, and is lacking in wider comparisons. There is also some lack of current motivation for the model, and no comparison with tractable models that do not need a variational autoencoder. Originality: original, but at the moment it is not clear such originality is necessary. Clarity: Good. Experiments: Sensible, but not extensive or conclusive.  This paper introduces a novel extension of the variational autoencoder to arbitrary tree-structured outputs. Experiments are conducted on a synthetic arithmetic expression dataset and a first-order logic proof clause dataset in order to evaluate its density modeling performance. Pros: + The paper is clear and well-written. + The tree-structure definition is sufficiently complete to capture a wide variety of tree types found in real-world situations. + The tree generation and encoding procedure is elegant and well-articulated. + The experiments, though limited in scope, are relatively thorough. The use of IWAE to obtain a better estimate of log likelihoods is a particularly nice touch. Cons: - The performance gain over a baseline sequential model is marginal. - The experiments are limited in scope, both in the datasets considered and in the evaluation metrics used to compare the model with other approaches. Specifically: (a) there is only one set of results on a real-world dataset and in that case the proposed model performs worse than the baseline, and (b) there is no evaluation of the learned latent representation with respect to other tasks such as classification. - The ability of the model to generate trees in time proportional to the depth of the tree is proposed as a benefit of the approach, though this is not empirically validated in the experiments. The procedures to generate and encode trees are clever in their repeated use of common operations. The weight sharing and gating operations seem important for this model to perform well but it is difficult to assess their utility without an ablation (in Table 1 and 2 these modifications are not evaluated side-by-side). Experiments in another domain (such as modeling source code, or parse trees conditioned on a sentence) would help in demonstrating the utility of this model. Overall the model seems promising and applicable to a variety of data but the lack of breadth in the experiments is a concern. * Section 3.1: *We distinguish three types* => two * Section 3.6: The exposition of the variable-sized latent state is slightly confusing because the issue of how many z*s to generate is not discussed. * Section 4.2-4.3: When generating the datasets, did you verify that the test set is disjoint from the training set? * Table 1: Is there a particular reason why the variable latent results are missing for the depth 11 trees?","['Rating:###3: Clear rejection', 'Rating:###3: Clear rejection', 'Rating:###4: Ok but not good enough - rejection']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",3,4,"Here is the structured peer review:

**Strengths:**

1. The paper introduces a novel extension of the variational autoencoder to arbitrary tree-structured outputs, which is a significant contribution to the field.
2. The tree-structure definition is sufficiently complete to capture a wide variety of tree types found in real-world situations.
3. The tree generation and encoding procedure is elegant and well-articulated.
4. The use of IWAE to obtain a better estimate of log likelihoods is a particularly nice touch.

**Weaknesses:**

1. The performance gain over a baseline sequential model is marginal, which may indicate that the model is not as effective as claimed.
2. The experiments are limited in scope, both in the datasets considered and in the evaluation metrics used to compare the model with other approaches.
3. The lack of evaluation of the learned latent representation with respect to other tasks such as classification is a significant limitation.
4. The model's ability to generate trees in time proportional to the depth of the tree is not empirically validated, which may indicate that this is not a unique benefit of the approach.
5. The weight sharing and gating operations are not thoroughly evaluated, and it is unclear how important they are to the model's performance.

**Rating:** 7
**Confidence:** 3

Note: The rating is based on the overall quality of the paper, with 10 being the highest. The confidence score reflects the reviewer's confidence in the paper's claims"
Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights | OpenReview,"This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization (a variable-length encoding: 1 bit for representing zero value, and the remaining 4 bits represent at most 16 different values for the powers of two), our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices. The code will be made publicly available.","There is a great deal of ongoing interest in compressing neural network models. One line of work has focused on using low-precision representations of the model weights, even down to 1 or 2 bits. However, so far these approaches have been accompanied by a significant impact on accuracy. The paper proposes an iterative quantization scheme, in which the network weights are quantized in stages---the largest weights (in absolute value) are quantized and fixed, while unquantized weights can adapt to compensate for any resulting error. The experimental results show this is extremely effective, yielding models with 4 bit or 3 bit weights with essentially no reduction in accuracy. While at 2 bits the accuracy decreases slightly, the results are substantially better than those achieved with other quantization approaches. Overall this paper is clear, the technique is as far as I am aware novel, the experiments are thorough and the results are very compelling, so I recommend acceptance. The paper could use another second pass for writing style and grammar. Also, the description of the pruning-inspired partitioning strategy could be clarified somewhat... e.g., the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.  The idea of this paper is reasonable - gradually go from original weights to compressed weights by compressing a part of them and fine-tuning the rest. Everything seems fine, results look good, and my questions have been addressed. To improve the paper: 1) It would be good to incorporate some of the answers into the paper, mainly the results with pruning + this method as that can be compared fairly to Han et al. and outperforms it. 2) It would be good to better explain the encoding method (my question 4) as it is not that clear from the paper (e.g. made me make a mistake in question 5 for the computation of n2). The *5 bits* is misleading as in fact what is used is variable length encoding (which is on average close to 5 bits) where: - 0 is represented with 1 bit, e.g. 0 - other values are represented with 5 bits, where the first bit is needed to distinguish from 0, and the remaining 4 bits represent the 16 different values for the powers of 2.  Nice idea but not complete, model size is not reduced by the large factors found in one of your references (Song 2016), where they go to 5 bits, but this is ontop of pruning which gives overall 49X reduction in model size of VGG (without loss of accuracy). You may achieve similar reductions with inclusion of pruning (or better since you go to 4 bits with no loss) but we should see this in the paper, so at the moment it is difficult to compare","['Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",8,4,"Here is the structured peer review:

**Strengths:**

1. The paper presents a novel approach to incremental network quantization, which is a significant contribution to the field of deep learning.
2. The experimental results show that the proposed method is effective in achieving high accuracy with low-precision weights, which is a major advantage over existing methods.
3. The authors provide a clear and concise explanation of the proposed technique, making it easy to understand for the reader.
4. The paper includes a thorough evaluation of the proposed method on various deep CNN architectures, which demonstrates its applicability and effectiveness.

**Weaknesses:**

1. The paper could benefit from a more detailed explanation of the encoding method used in variable-length quantization, as it is not clearly explained in the paper.
2. The description of the pruning-inspired partitioning strategy could be clarified, as it is not fully explained in the paper.
3. The paper could include more comparisons with existing methods, such as Han et al.'s method, to demonstrate the superiority of the proposed approach.
4. The results with pruning + INQ could be included to provide a more comprehensive evaluation of the proposed method.

**Rating:** 9
**Confidence:** 4

Note: I deducted 1 point from the rating because the paper could benefit from more detailed explanations and comparisons with existing methods. However, the overall quality of the paper is high, and the results are compelling. The confidence score is 4 because I believe that"
Soft Weight-Sharing for Neural Network Compression | OpenReview,"The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression. Recent work by Han et al. (2016) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates. In this paper, we show that competitive compression rates can be achieved by using a version of *soft weight-sharing* (Nowlan & Hinton, 1991). Our method achieves both quantization and pruning in one simple (re-)training procedure. This point of view also exposes the relation between compression and the minimum description length (MDL) principle.","This paper proposes to use an empirical Bayesian approach to learn the parameters of a neural network, and their priors. A mixture model prior over the weights leads to a clustering effect in the weight posterior distributions (which are approximated with delta peaks). This clustering effect can exploited for parameter quantisation and compression of the network parameters. The authors show that this leads to compression rates and predictive accuracy comparable to related approaches. Earlier work [Han et al. 2015] is based on a three-stage process of pruning small magnitude weights, clustering the remaining ones, and updating the cluster centres to optimise performance. The current work provides a more principled approach that does not have such an ad-hoc multi-stage structure, but a single iterative optimisation process. A first experiment, described in section 6.1 shows that an empirical Bayes’ approach, without the use of hyper priors, already leads to a pronounced clustering effect and to setting many weights to zero. In particular a compression rate of 64.2 is obtained on the LeNet300-100 model. In section 6.1 the text refers to figure C, I suppose this should be figure 1. Section 6.2 describes an experiment where hyper-priors are used, and the parameters of these distributions, as well as other hyper-parameters such as the learning rates, are being optimised using Spearmint (Snoek et al., 2012). Figure 2 shows the performance of the different points in the hyper-parameter space that have been evaluated (each trained network gives an accuracy-compressionrate point in the graph). The text claims that best results lie on a line, this seems a little opportunistic interpretation given the limited data. Moreover, it would be useful to add a small discussion on whether such a linear relationship would be expected or not. Currently the results of this experiment lack interpretation. Section 6.3 describes results obtained for both CNN models and compares results to the recent results of (Han et al., 2015) and (Guo et al., 2016). Comparable results are obtained in terms of compression rate and accuracy. The authors state that their current algorithm is too slow to be useful for larger models such as VGG-19, but they do briefly report some results obtained for this model (but do not compare to related work). It would be useful here to explain what slows the training down with respect to standard training without the weight clustering approach, and how the proposed algorithm scales in terms of the relevant quantities of the data and the model. The contribution of this paper is mostly experimental, leveraging fairly standard ideas from empirical Bayesian learning to introduce weight clustering effects in CNN training. This being said, it is an interesting result that such a relatively straightforward approach leads to results that are on par with state-of-the-art, but more ad-hoc, network compression techniques. The paper could be improved by clearly describing the algorithm used for training, and how it scales to large networks and datasets. Another point that would deserve further discussion is how the hyper-parameter search is performed ( not using test data I assume), and how the compared methods dealt with the search over hyper-parameters to determine the accuracy-compression tradeoff. Ideally, I think, methods should be evaluated across different points on this trade-off.  The authors propose a method to compress neural networks by retraining them while putting a mixture of Gaussians prior on the weights with learned means and variances which then can be used to compress the neural network by first setting all weights to the mean of their infered mixture component (resulting in a possible loss of precision) and storing the network in a format which saves only the fixture index and exploits the sparseness of the weights that was enforced in training. Quality: Of course it is a serious drawback that the method doesn*t seem to work on VGG which would render the method unusable for production (as it is right now, maybe this can be improved). I guess AlexNet takes too long to process, too, otherwise this might be a very valuable addition. In Figure 2 I am noticing two things: On the left, there is a large number of points with improved accuracy which is not the case for LeNet5-Caffe. Is there any intuition for why that*s the case? Additionally regarding the spearmint optimization: Do they authors have found any clues about which hyperparameter settings worked well? This might be helpful for other people trying to apply this method. I really like Figure 7 in it*s latest version. Clarity: Especially section 2 on MDL is written very well and gives a nice theoretic introduction. Sections 4, 5 and 6 are very short but seem to contain most relevant information. It might be helpful to have at least some more details about the used models in the paper (maybe the number of layers and the number of parameters). In 6.1 the authors claim *Even though most variances seem to be reasonable small there are some that are large*. From figure 1 this is very hard to assess, especially as the vertical histogram essentially shows only the zero component. It might be helpful to have either a log histogram or separate histograms for each componenent. What are the large points in Figure 2 as opposed to the smaller ones? They seem to have a very good compression/accuracy loss ratio, is that it? Some other points are listed below originality: While there has been some work on compressing neural networks by using a reduced number of bits to store the parameters and exploiting sparsity structure, I like the idea to directly learn the quantization by means of a gaussian mixture prior in retraining which seems to be more principled than other approaches significance: The method achievs state-of-the-art performance on the two shown examples on MNIST, however these networks are far from the deep networks used in state-of-the-art models. This obviously is a drawback for the practical usability of the methods and therefor it*s significance. If the method could be made to work on more state-of-the-art networks like VGG or ResNet, I would consider this a contribution of high significance. Minor issues: page 1: There seems to be a space in front of the first author*s name page 3: *in this scenario, pi_0 may be fixed...*. Missing backslash in TeX? page 6: 6.2: two wrong blanks in *the number of components_,  au_.* page 6, 6.3: *in experiences with VGG*: In experiments? page 12: *Figure C*: Figure 7?  This paper revives a classic idea involving regularization for purposes of compression for modern CNN models on resource constrained devices. Model compression is hot and we*re in the midst of lots of people rediscovering old ideas in this area so it is nice to have a paper that explicitly draws upon classic approaches from the early 90s to obtain competitive results on standard benchmarks. There*s not too much to say here: this study is an instance of a simple idea applied effectively to an important problem, written up in an illuminating manner with appropriate references to classic approaches. The addition of the filter visualizations enhances the contribution.","['Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",7,3,"Here is the structured peer review:

**Strengths**

1. The paper presents a novel approach to neural network compression by leveraging soft weight-sharing, which is a classic idea from the early 90s. The authors demonstrate that this approach can achieve competitive compression rates and predictive accuracy comparable to state-of-the-art methods.
2. The paper provides a clear and concise introduction to the MDL principle and its relation to weight clustering, which is well-written and easy to follow.
3. The authors provide a thorough explanation of the algorithm and its components, making it easy to understand and replicate.
4. The paper includes visualizations of the filter weights and the clustering effect, which helps to illustrate the concept and provide insight into the results.
5. The authors acknowledge the limitations of their approach, specifically its inapplicability to larger models like VGG-19, and provide a clear explanation of why this is the case.

**Weaknesses**

1. The paper lacks a clear discussion of the algorithm's scalability to larger models and datasets. The authors mention that their approach is too slow for VGG-19, but do not provide a detailed explanation of why this is the case or how the algorithm can be modified to address this limitation.
2. The paper does not provide a thorough evaluation of the hyperparameter search process, including the methods used to optimize the hyperparameters and the trade-off between accuracy and compression.
3. The results presented in Figure 2 are not clearly interpreted, and the authors"
Implicit ReasoNet: Modeling Large-Scale Structured Relationships with Shared Memory | OpenReview,"Recent studies on knowledge base completion, the task of recovering missing relationships based on recorded relations, demonstrate the importance of learning embeddings from multi-step relations. However, due to the size of knowledge bases, learning multi-step relations directly on top of observed instances could be costly. In this paper, we propose Implicit ReasoNets (IRNs), which is designed to perform large-scale inference implicitly through a search controller and shared memory. Unlike previous work, IRNs use training data to learn to perform multi-step inference through the shared memory, which is also jointly updated during training. While the inference procedure is not operating on top of observed instances for IRNs, our proposed model outperforms all previous approaches on the popular FB15k benchmark by more than 5.7%.","In this paper, the authors proposed an implicit ResoNet model for knowledge base completion. The proposed model performs inference implicitly by a search controller and shared memory. The proposed approach demonstrates promising results on FB15k benchmark dataset. Pros: - The proposed approach demonstrates strong performance on FB15k dataset. - The idea of using shared memory for knowledge base completion is new and interesting. - The proposed approach is general and can be applied in various tasks. Cons: - There is no qualitative analysis on the results, and it is hard to see why the proposed approach works on the knowledge-base completion task. - The introduction section can be improved. Specifically, the authors should motivate *shared memory* more in the introduction and how it different from existing methods that using *unshared memory* for knowledge base completion. Similarly, the function of search controller is unclear in the introduction section as it is unclear what does search mean in the content of knowledge base completion. The concept of shared memory and search controller only make sense to me after reading through section 2.  This paper proposes a method for link prediction on Knowledge Bases. The method contains 2 main innovations: (1) an iterative inference process that allows the model to refine its predictions and (2) a shared memory component. Thanks to these 2 elements, the model introduced in the paper achieved remarkable results on two benchmarks. The paper is fairly written. The model is interesting and the experimental results are strikingly good. Still, I only rate for a weak accept for the following reasons. * The main problem with this paper is that there is little explanation of how and why the two new elements aforementioned are leading to such better results. For instance: - What are the performance without the shared memory? And when its size is grown? - How does the performance is impacted when one varies Tmax from 1 to 5 (which the chosen value for the experiments I assume)? This gives an indications of how often the termination gate works. - It would also be interesting to give the proportion of examples for which the inference is terminated before hitting Tmax. - What is the proportion of examples for which the prediction changed along several inference iterations? * A value of lambda set to 10 (Section 2) seems to indicate a low temperature for the softmax. Is the attention finally attending mostly at a single cell? How do the softmax activations change with the type of relationships? the entity type? * FB15k and WN18 are quite old overused benchmarks now. It would be interesting to test on larger conditions.  [Summary] This paper proposes a new way for knowledge base completion which highlights: 1) adopting an implicit shared memory, which makes no assumption about its structure and is completely learned during training; 2) modeling a multi-step search process that can decide when to terminate. The experimental results on WN18 and FB15k seem pretty good. The authors also perform an analysis on a shortest path synthetic task, and demonstrate that this model is better than standard seq2seq. The paper is well-written and it is easy to follow. [Major comments] I actually do like the idea and am also impressed that this model can work well. The main concern is that this paper presents too little analysis about how it works and whether it is sensitive to the hyper-parameters, besides that only reporting a final model on WN18 and FB15k. One key hyper-parameter I believe is the size of shared memory (using 64 for the experiments). I don’t think that this number should be fixed for all tasks, at least it should depend on the KB scale. Could you verify this in your experiments? Would it be even possible to make a memory structure with dynamic size? The RL setting (stochastic search process) is also one highlight of the paper, but could you demonstrate that how much it does really help? I think it is necessary to compare to the following: remove the termination gate and fix the number of inference steps and see how well the model does? Also show how the performance varies on # of steps? I appreciate your attempts on the shortest path synthetic task. However, I think it would be much better if you can demonstrate that under a real KB setting. You can still perform the shortest path analysis, but using KB (e.g., Freebase) entities and relations. [Minor comments] I am afraid that the output gate illustrated in Figure 1 is a bit confusing. There should be only one output, depending on when the search process is terminated.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",6,4,"Here is the structured peer review:

**Strengths:**

1. The paper proposes a novel approach to knowledge base completion using implicit shared memory and a search controller, which is a significant contribution to the field.
2. The experimental results on the popular FB15k benchmark are impressive, with a performance improvement of over 5.7% compared to previous approaches.
3. The paper provides a clear and concise writing style, making it easy to follow and understand.
4. The authors' use of a shortest path synthetic task to demonstrate the effectiveness of their approach is a good idea.

**Weaknesses:**

1. The introduction section could be improved to better motivate the use of shared memory and explain its significance in the context of knowledge base completion.
2. The paper lacks a clear explanation of how the search controller works and its role in the inference process.
3. The authors do not provide sufficient analysis on the impact of hyperparameters, such as the size of shared memory, on the performance of the model.
4. The paper relies heavily on the FB15k and WN18 benchmarks, which may not be representative of more complex knowledge bases.
5. The output gate illustrated in Figure 1 is confusing and should be clarified.

**Rating:** 8/10
**Confidence:** 4/5

Note: The rating is based on the overall quality of the paper, and the confidence score reflects my confidence in the paper's conclusions. The rating and confidence score are subjective and"
Rotation Plane Doubly Orthogonal Recurrent Neural Networks | OpenReview,"Recurrent Neural Networks (RNNs) applied to long sequences suffer from the well known vanishing and exploding gradients problem. The recently proposed Unitary Evolution Recurrent Neural Network (uRNN) alleviates the exploding gradient problem and can learn very long dependencies, but its nonlinearities make it still affected by the vanishing gradient problem and so learning can break down for extremely long dependencies. We propose a new RNN transition architecture where the hidden state is updated multiplicatively by a time invariant orthogonal transformation followed by an input modulated orthogonal transformation. There are no additive interactions and so our architecture exactly preserves forward hid-den state activation norm and backwards gradient norm for all time steps, and is provably not affected by vanishing or exploding gradients. We propose using the rotation plane parameterization to represent the orthogonal matrices. We validate our model on a simplified memory copy task and see that our model can learn dependencies as long as 5,000 timesteps.","My main objection with this work is that it operates under a hypothesis (that is becoming more and more popular in the literature) that all we need is to have gradients flow in order to solve long term dependency problems. The usual approach is then to enforce orthogonal matrices which (in absence of the nonlinearity) results in unitary jacobians, hence the gradients do not vanish and do not explode. However this hypothesis is taken for granted (and we don*t know it is true yet) and instead of synthetic data, we do not have any empirical evidence that is strong enough to convince us the hypothesis is true. My own issues with this way of thinking is: a) what about representational power; restricting to orthogonal matrices it means we can not represent the same family of functions as before (e.g. we can*t have complex attractors and so forth if we run the model forward without any inputs). You can only get those if you have eigenvalues larger than 1. It also becomes really hard to deal with noise (since you attempt to preserve every detail of the input, or rather every part of the input affects the output). Ideally you would want to preserve only what you need for the task given limited capacity. But you can*t learn to do that. My issue is that everyone is focused on solving this preserved issue without worrying of the side-effects. I would like one of these papers going for jacobians having eigenvalues of 1 show this helps in realistic scenarios, on complex datasets.  This paper discusses recurrent networks with an update rule of the form h_{t+1} = R_x R h_{t}, where R_x is an embedding of the input x into the space of orthogonal or unitary matrices, and R is a shared orthogonal or unitary matrix. While this is an interesting model, it is by no means a *new* model: the idea of using matrices to represent input objects (and multiplication to update state) is often used in the embedding-knowledge-bases or embedding-logic literature (e.g. Using matrices to model symbolic relationships by Ilya Sutskever and Geoffrey Hinton, or Holographic Embeddings of Knowledge Graphs by Maximillian Nickel et al.). I don*t think the experiments or analysis in this work add much to our understanding of it. In particular, the experiments are especially weak, consisting only of a very simplified version of the copy task (which is already very much a toy). I know several people who have played with this model in the setting of language modeling, and as the other reviewer notes, the inability of the model to forget is an actual annoyance. I think it is incumbent on the authors to show how this model can be really useful on a nontrivial task; as it is we should not accept this paper. Some questions: is there any reason to use the shared R instead of absorbing it into all the R_x? Can you find any nice ways of using the fact that the model is linear in h or linear in R_x ?  This is a nice proposal, and could lead to more efficient training of recurrent nets. I would really love to see a bit more experimental evidence. I asked a few questions already but didn*t get any answer so far. Here are a few other questions/concerns I have: - Is the resulting model still a universal approximator? (providing large enough hidden dimensions and number of layers) - More generally, can one compare the expressiveness of the model with the equivalent model without the orthogonal matrices? with the same number of parameters for instance? - The experiments are a bit disappointing as the number of distinct input/output sequences were in fact very small and as noted by the authr, training becomes unstable (I didn*t understand what *success* meant in this case). The authors point that the experiment section need to be expanded, but as far as I can tell they still haven*t unfortunately.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###4: Ok but not good enough - rejection', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",4,4,"Here is the structured peer review:

**Strengths:**

1. The authors propose a novel approach to addressing the vanishing and exploding gradient problem in RNNs by using orthogonal matrices to update the hidden state.
2. The idea of using a rotation plane parameterization to represent the orthogonal matrices is interesting and could lead to more efficient training of recurrent nets.

**Weaknesses:**

1. The authors operate under an unproven hypothesis that all we need is to have gradients flow in order to solve long-term dependency problems, and they do not provide empirical evidence to support this claim.
2. The model's restriction to orthogonal matrices limits its representational power, making it difficult to learn complex attractors and handle noise.
3. The experiments are limited and lack robustness, with only a simplified version of the copy task and unstable training.
4. The model's ability to learn long-term dependencies is not thoroughly tested, and the authors do not provide a clear comparison with the equivalent model without orthogonal matrices.
5. The paper does not address the issue of the model's capacity to be a universal approximator, and the number of parameters required to achieve this is not clearly defined.
6. The authors' claim that the model is not affected by vanishing or exploding gradients is not supported by rigorous analysis or empirical evidence.

**Rating:** 6
**Confidence:** 2

Note: The rating is based on the reviewer's assessment of the paper's strengths and weaknesses, and the"
A Compare-Aggregate Model for Matching Text Sequences | OpenReview,"Many NLP tasks including machine comprehension, answer selection and text entailment require the comparison between sequences. Matching the important units between sequences is a key to solve these problems. In this paper, we present a general *compare-aggregate* framework that performs word-level matching followed by aggregation using Convolutional Neural Networks. We particularly focus on the different comparison functions we can use to match two vectors. We use four different datasets to evaluate the model. We find that some simple comparison functions based on element-wise operations can work better than standard neural network and neural tensor network.","This paper proposed a compare-aggregate model for the NLP tasks that require semantically comparing the text sequences, such as question answering and textual entailment. The basic framework of this model is to apply a convolutional neural network (aggregation) after a element-wise operation (comparison) over the attentive outputs of the LSTMs. The highlighted part is the comparison, where this paper compares several different methods for matching text sequences, and the element-wise subtraction/multiplication operations are demonstrated to achieve generally better performance on four different datasets. While the weak point is that this is an incremental work and a bit lack of innovation. A qualitative evaluation about how subtraction, multiplication and other comparison functions perform on varied kinds of sentences would be more interesting.  This paper proposes a compare-aggregate framework that performs word-level matching followed by aggregation with convolutional neural networks. It compares six different comparison functions and evaluates them on four datasets. Extensive experimental results have been reported and compared against various published baselines. The paper is well written overall. A few detailed comments: * page 4, line5: including a some -> including some * What*s the benefit of the preprocessing and attention step? Can you provide the results without it? * Figure 2 is hard to read, esp. when on printed hard copy. Please enhance the quality.  The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model. This work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I*d strongly recommend acceptance. Detail: - The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections. - If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. *15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy). - Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)? - You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite. - Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept', 'Rating:###8: Top 50% of accepted papers, clear accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",6,4,"Here is the structured peer review:

**Strengths:**

1. The paper presents a clear and concise framework for comparing text sequences, which is a crucial step in many NLP tasks such as machine comprehension, answer selection, and text entailment.
2. The authors provide a thorough evaluation of the proposed framework using four different datasets, which demonstrates the effectiveness of the approach.
3. The paper is well-written and easy to follow, making it accessible to a wide range of readers.
4. The authors provide a detailed analysis of the comparison functions used in the framework, which is a valuable contribution to the field.

**Weaknesses:**

1. The paper may be considered incremental, as it builds upon existing work in the field. While it does contribute to the field, it may not be as innovative as it could be.
2. The paper could benefit from a more detailed discussion of the limitations of the approach, particularly in terms of its insensitivity to word order.
3. The comparison functions used in the framework may not be as effective for all types of sentences, and a more nuanced discussion of their strengths and weaknesses would be beneficial.
4. The paper could benefit from more detailed explanations of the preprocessing and attention steps, particularly in terms of their impact on the overall performance of the framework.

**Rating:** 8/10
**Confidence:** 4/5

Note: The rating and confidence score are subjective and based on my evaluation of the paper. The rating reflects the"
Surprisal-Driven Feedback in Recurrent Networks | OpenReview,"Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.","Summary: This paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network. Authors have shown a result on language modeling tasks. Contributions: The introduction of surprisal-driven feedback, which is just the feedback from the errors of the model from the previous time-steps. Questions: A point which is not fully clear from the paper is whether if you have used the ground-truth labels on the test set for the surprisal feedback part of the model? I assume that authors do that since they claim that they use the misprediction error as additional input. Criticisms: The paper is really badly written, authors should rethink the organization of the paper. Most of the equations presented in the paper, about BPTT are not necessary for the main-text and could be moved to Appendix. The justification is not convincing enough. Experimental results are lacking, only results on a single dataset are provided. Although the authors claim that they got SOTA on enwiki8, there are other papers such as the HyperNetworks that got better results (1.34) than the result they achieve. This claim is wrong. The model requires the ground-truth labels for the test-set, however, this assumption really limits the application of this technique to a very limited set of applications(more or less rules out most conditional language modeling tasks). High-level Review: Pros: - A simple modification of the model that seems to improve the results and it is an interesting modification. Cons: - The authors need to use test-set labels. - Writing of the paper is bad. - The authors assume that they have access to the ground-truth labels during the test-set. - Experimental results are lacking  This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function in order to enhance the modelling power of a dynamic system such as RNNs. -This paper makes an erroneous assumption: test label information is not given in most of the real world applications, except few applications. This means that the language modelling task, which is the only experiment of this paper, may not be the right task to test this approach. Also, comparing against the models that do not use test error signal at inference time is unfair. We cannot just say that the test label information is being observed, this only holds in online-prediction problems. -The experiment is only conducted on one dataset, reporting state-of-the-art result, but unfortunately this is not true. There are already more than four papers reporting better numbers than the one reported in this task, however the author did not cite them. I understand that this paper came before the other papers, but the manuscript should be updated before the final decision. -The model size is still missing and without this information, it is hard to judge the contribution of the proposed trick.  This paper proposes to leverage *surprisal* as top-down signal in RNN. More specifically author uses the error corresponding to the previous prediction as an extra input at the current timestep in a LSTM. The general idea of suprising-driven feedback is interesting for online prediction task. It is a simple enough idea that seems to bring some significant improvements. However, the paper in its current form has some important flaws. - Overall, the paper writing could be improved. In particular, section 2.4 and 2.5 is composed mostly by the equations of the forward and backward propagation of feedback RNN and feedback LSTM. However, author provides no analysis along with those equations. It is therefore not clear what insight the author tries to express in those sections. In addition, feedback RNN is not evaluated in the experimental section, so it is not clear why feedback RNN is described. - The experimental evaluation is limited. Only one dataset enwik8 is explored. I think it is necessary to try the idea on different datasets to see if feedback LSTM sees some consistent improvements. Also, author claims state-of-art on enwik8, but hypernetwork, already cited in the paper, achieves better results (1.34 BPC, table 4 in the hypernetworks paper). - Author only compares to methods that do not use last prediction error as extra signal. I would argue that a comparison with dynamic evaluation would be more fair. Feedback LSTM uses prediction error as extra input in the forward prop, while dynamic evaluation backprop it through the network and change the weight accordingly. Also they don*t propagate the prediction error in the same way, they both leverage *extra* supervised information through the prediction errors. In summary: Pros: - Interesting idea - Seems to improve performances Cons: - Paper writing - Weak evaluation (only one dataset) - Compare only with approaches that does not use the last-timestep error signal","['Rating:###4: Ok but not good enough - rejection', 'Rating:###3: Clear rejection', 'Rating:###3: Clear rejection']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",4,4,"Here is the structured peer review:

**Strengths:**

1. The idea of using surprisal-driven feedback in recurrent neural networks is interesting and has the potential to improve the performance of RNNs in predicting temporal data.
2. The authors propose a simple modification to the RNN architecture that can be easily implemented.

**Weaknesses:**

1. The paper is poorly written, with unclear and confusing sections, and the writing style is not engaging or easy to follow.
2. The experimental results are limited to a single dataset (enwik8) and do not provide a comprehensive evaluation of the proposed method.
3. The authors make an incorrect claim that their method is state-of-the-art, as there are other papers (e.g., HyperNetworks) that achieve better results.
4. The paper assumes that the test labels are available, which is not the case in most real-world applications, limiting the applicability of the proposed method.
5. The model size is not provided, making it difficult to assess the contribution of the proposed trick.
6. The paper does not provide a clear analysis of the feedback RNN architecture, making it unclear what insight the authors are trying to express.
7. The experimental evaluation is not fair, as the authors only compare to methods that do not use the last-timestep error signal.

**Rating:** 4/10
**Confidence:** 2/5

Note: The rating is low due to the significant weaknesses in the paper"
Improved Architectures for Computer Go | OpenReview,AlphaGo trains policy networks with both supervised and reinforcement learning and makes different policy networks play millions of games so as to train a value network. The reinforcement learning part requires massive ammount of computation. We propose to train networks for computer Go so that given accuracy is reached with much less examples. We modify the architecture of the networks in order to train them faster and to have better accuracy in the end.,"The paper tests various feedforward network architectures for supervised training to predict a human’s next move, given a board position. It trains on human play data taken from KGX, augmenting the data by considering all 8 rotations/reflections of each board position. The paper’s presentation is inefficient and muddled, and the results seem incremental. Presentation: The abstract and introduction point out that AlphaGo requires many RL iterations to train, and propose to improve this by swapping out the policy network with one that is more amenable to training. However, the paper only presents supervised learning results, not RL. While it’s not unreasonable to assume that a higher-capacity network that shows improvements in supervised learning will also yield dividends in RL, it’s still unsatisfying to be presented with SL improvements and be asked to assume that the RL improvements will be of a similar magnitude, whatever that may mean. It would’ve been more convincing to train both AlphaGo and this paper*s architectures on an equal number of RL self-play iterations, then have them play each other. (Both would be pre-trained using supervised training, as per the AlphaGo paper). It is not until section 3.3 that it is clearly stated this is strictly a supervised-learning paper. This should have been put front and center in the abstract and introduction. Fully 3 pages are spent on giant but simple architecture diagrams. This is both extravagant and muddles the exposition. It seems better to show just the architectures used in the experiments, and spend at most half a page doing so, so that they may be seen alongside one another. The results (Table 1, Figures 7 and 8) are hard to skim, as there is little information in the captions, and the graph axes are poorly labeled. For example, I assume “Examples” in figures 7 and 8 should be “Training examples”, and the number of training examples isn’t 0-50, but some large multiple thereof. Results: The take-home seems to be that that deeper networks do better, and residual architectures and spatial batch normalization each improve the results in this domain, as they are known to do in others. Furthermore, we are asked to assume that the improvements in an RL setting will be similar to the improvements in SL shown here. These results seem too incremental to justify an ICLR publication.  This paper reports new CNN architectures for playing Go. The results are better than previously reported, but there is no mention of computational time and efficiency, and relative metric of performance/flop or performance/flop/energy. Overall a good paper.  The paper trains slightly different network architectures on Computer Go, and provides an analysis of the accuracy as the number of training samples increases and the network architecture differs. The paper looks like a follow up paper of the author’s previous paper Cazenave (2016a), however, the contribution over the previous paper is not clear. A section should be added to state what the differences are. The paper states that the improvements that are obtained in this study are because of the changes in the training set, the input features and the architecture of the network. It is not clear what are the changes that were done to the training set and input features. How are they different than the previous work?  The paper investigates several different architectures for move prediction in computer Go. The main innovation seems to be the use of residual networks. The best proposed architecture outperforms previous results on KGS move prediction dataset. The network also reached amateur 3 dan level on KGS. I found the paper to be somewhat poorly written and lacking important details. Here are my main concerns: 1) This paper references a previous paper by the author(Cazenave 2016a) as having introduced residual network architectures to computer go. The overlap with this paper seems quite significant but I could not find it anywhere. What exactly is new? 2) The author claims the addition of batch norm to a residual architecture as the main architectural innovation, but the original ResNet paper was already using batch norm between the convolution and activation layers. Have you compared your architecture (ResNet with batch norm after ReLU) with the original ResNet architecture (batch norm before ReLU)? 3) It is not at all surprising that ResNets do slightly better than vanilla CNNs on move prediction. I don*t think this alone is enough for an ICLR paper. It would be good to see at least a comparison of several different variants of the network evaluated at actually playing Go, even if it*s against other bots like GnuGo, Pachi, and Fuego. 4) Are the differences between net_dark and your proposed networks after 20 iterations (Table 1) significant? Please also see my original questions.","['Rating:###3: Clear rejection', 'Rating:###7: Good paper, accept', 'Rating:###4: Ok but not good enough - rejection', 'Rating:###3: Clear rejection']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",3,4,"Here is the structured peer review:

**Strengths:**

1. The paper presents an interesting analysis of different network architectures for computer Go, and the results show that deeper networks and residual architectures can improve performance.
2. The paper provides a clear and concise presentation of the experimental results, making it easy to understand and compare the different architectures.

**Weaknesses:**

1. The paper's presentation is unclear and lacks a clear direction, making it difficult to understand the main contribution of the paper. The abstract and introduction do not clearly state that the paper is focused on supervised learning, and the results are not directly comparable to AlphaGo.
2. The paper does not provide enough context and background information on the previous work, making it difficult to understand the significance of the results. The reference to Cazenave (2016a) is not clearly explained, and the overlap between the current paper and the previous work is not clearly stated.
3. The paper does not provide enough details on the training process, computational time, and efficiency of the proposed architectures. This makes it difficult to evaluate the results and understand the significance of the improvements.
4. The paper does not provide a clear comparison of the proposed architectures with other state-of-the-art models, such as GnuGo, Pachi, and Fuego. This makes it difficult to evaluate the relative performance of the proposed architectures.

**Rating:** 6
**Confidence:** 3

Note: The rating is based on the overall quality"
Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling | OpenReview,"Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.","This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution. This is a nice setup since it can effectively smooth over the labels given as input. However, the construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in Eqs. 3.6 and 3.7. It is not obvious why the projection matrix L in Eq 3.6 (let*s rename it to L*) should be the same as that in Eq. 2.1. For example, L* could be obtained through word2vec embeddings trained on a large dataset or it could be learned as an additional set of parameters. In the case that L* is a new learned matrix, it seems the result in Eq 4.5 is to use an independent matrix for the output projection layer, as is usually done. The experimental results are good and provide support for the approximate derivation done in section 4, particularly the distance plots in figure 1. Minor comments: Third line in abstract: where model -> where the model Second line in section 7: into space -> into the space Shouldn*t the RHS in Eq 3.5 be sum  ilde{y_{t,i}}(frac{hat{y}_t}{ ilde{y_{t,i}}} - e_i) ?  This paper provides a theoretical framework for tying parameters between input word embeddings and output word representations in the softmax. Experiments on PTB shows significant improvement. The idea of sharing or tying weights between input and output word embeddings is not new (as noted by others in this thread), which I see as the main negative side of the paper. The proposed justification appears new to me though, and certainly interesting. I was concerned that results are only given on one dataset, PTB, which is now kind of old in that literature. I*m glad the authors tried at least one more dataset, and I think it would be nice to find a way to include these results in the paper if accepted. Have you considered using character or sub-word units in that context?  This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs. The argument uses an augmented loss function which spreads the output probability mass among words with close word-embedding. I see two main drawbacks from this framework: The augmented loss function has no trainable parameters and is used for only for regularization. This is not expected to give gains with large enough datasets. The augmented loss is heavily “engineered” to produce the desired result of parameter tying. It’s not clear what happens if you try to relax it a bit, by adding parameters, or estimating y~ in a different way. Nevertheless the argument is very interesting, and clearly written. The simulated results indeed validate the argument, and the PTB results seem promising. Minor comments: Section 3: Can you clarify if y~ is conditioned on the t example or on the entire history. Eq. 3.5: i is enumerated over V (not |V|)","['Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept', 'Rating:###8: Top 50% of accepted papers, clear accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",6,4,"Here is the structured peer review:

**Strengths:**

1. The paper provides a novel theoretical framework for tying word vectors and word classifiers in language modeling, which is a significant contribution to the field.
2. The authors provide a clear and concise explanation of the theoretical justification for the proposed framework, making it easy to follow and understand.
3. The experimental results on the Penn Treebank dataset are promising, showing significant improvement over existing language models.
4. The paper addresses the issue of parameter efficiency in language modeling, which is a common challenge in the field.

**Weaknesses:**

1. The construction of the estimate of the true data distribution (y~) seems engineered to produce the desired result of parameter tying, and it is not clear what happens if the augmented loss function is relaxed or estimated differently.
2. The paper relies on a single dataset (PTB) for evaluation, which may not be representative of the broader language modeling landscape.
3. The idea of tying word embeddings and output projection matrices is not new, and the paper does not provide a clear justification for why this approach is superior to existing methods.
4. The paper could benefit from more detailed analysis of the limitations and potential drawbacks of the proposed framework.

**Rating:** 8/10
**Confidence:** 4/5

Note: The rating is based on the overall quality of the paper, and the confidence score reflects the reviewer's confidence in the conclusions drawn from the paper. The confidence score is"
End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension | OpenReview,"This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions. DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top-ranked chunk as the answer. Experimental results show that DCR could achieve a 66.3% Exact match and 74.7% F1 score on the Stanford Question Answering Dataset.","SYNOPSIS: The paper proposes a new neural network-based model for reading comprehension (reading a passage of text and answering questions based on the passage). It is similar in spirit to several other recent models, with the main exception that it is able to predict answers of different lengths, as opposed to single words/tokens/entities. The authors compare their model on the Stanford Question Answering Dataset (SQuAD), and show improvements over the baselines, while apparently lagging quite far behind the current state of the art reported on the SQuAD leaderboard. THOUGHTS: The main novelty of the method is to be able to identify phrases of different lengths as possible answers to the question. However, both approaches considered -- using a POS pattern trie tree to filter out word sequences with POS tags matching those of answers in the training set, and brute-force enumeration of all phrases up to length N -- seem somewhat orthogonal to the idea of *learning end-to-end * an answer chunk extraction model. Furthermore, as other reviews have pointed out, it seems that the linguistic features actually contribute a lot to the final accuracy (Table 3). One could argue that these are easy to obtain using standard taggers, but it takes away even more from the idea of an *end-to-end trained* system. The paper is generally well written, but there are several crucial sections in parts describing the model where it was really hard for me to follow the descriptions. In particular, the attention mechanism seems fairly standard to me in a seq2seq sense (i.e. there is nothing architecturally novel about it, as is for instance the case with the Gated Attentive Reader). I may be missing something, but even after the clarification round I still don*t understand how it is novel compared to standard attention used in for instance seq2seq models. Finally, although the method is shown to outperform the baseline method reported in the original paper introducing the SQuAD dataset, it currently seems to be 12th (out of 15 systems) on the leaderboard (https://rajpurkar.github.io/SQuAD-explorer/). Of course, it may be that further training and hyperparameter optimizations may improve these results. Therefore, given the lack of model novelty (based on my understanding), and the lack of strong results (based on the leaderboard), I don*t feel the paper is ready in its current form to be accepted to the conference. Note: The GRU citation should be (Cho et al., 2014), not (Bengio et al., 2015).  The paper proposed an end-to-end machine learning model called dynamic reader for the machine reading comprehension task. Compared to earlier systems, the proposed model is able to extract and rank a set of answer candidates from a given document. There are many recent models focusing on building good question answering systems by extracting phrases from a given article. It seems that there are two different aspects that are unique in this work: 1. The use of convolution model, and 2. Dynamic chunking Convolution network is often only used for modeling character-based word embeddings so I am curious about its effectiveness on representing phrases. Therefore, I wish there could be more analysis on how effective it is, as the authors do not compare the convolution framework to other alternative approaches such as LSTM. The comparisons are important, as the authors uses uni-gram, bi-gram and tri-gram information in the convolution network, and it is not clear to me that if tri-gram information is still needed for LSTM models. The dynamic chunking is a good idea, and a very similar idea is proposed in some of the recent papers such as [Kenton et al, 16], which also targets at the same dataset. However, I would like to see more analysis on the dynamic chunking. Why this approach is a good approach for representing answer chunks? Given the representation of the chunk is constructed by the first and the end word representations generated by a convolution network, I am not sure about the ability of this representation to capture the long answer phrases. The authors do not use character base embedding but use some of the previous trained NLP models. It would be interesting if the authors could show what are the advantages and disadvantages of using linguistic features compared to character embeddings. In short, there are several good ideas proposed in the paper, but the lack of proper analysis make it difficult to judge how important the proposed techniques are.  SUMMARY. The paper propose a reading-comprehension question answering system for the recent QA task where answers of a question can be either single tokens or spans in the given text passage. The model first encodes the passage and the query using a recurrent neural network. With an attention mechanism the model calculates the importance of each word on the passage with respect to each word in the question. The encoded words in the passage are concatenated with the attention; the resulting vector is re-encoded with a further RNN. Three convolutional neural networks with different filter size (1,2,3-gram) are used to further capture local features. Candidate answers are selected either matching POS patterns of answers in the training set or choosing all possible text span until a certain length. Each candidate answer has three representations, one for each n-gram representation. The compatibility of these representation with the question representation is then calculated. The scores are combined linearly and used for calculating the probability of the candidate answer being the right answer for the question. The method is tested on the SQUAD dataset and outperforms the proposed baselines. ---------- OVERALL JUDGMENT The method presented in this paper is interesting but not very motivated in some points. For example, it is not explained why in the attention mechanism it is beneficial to concatenate the original passage encoding with the attention-weighted ones. The contributions of the paper are moderately novel proposing mainly the attention mechanism and the convolutional re-encoding. In fact, combining questions and passages and score their compatibility has became a fairly standard procedure in all QA models. ---------- DETAILED COMMENTS Equation (13) i should be s, not s^l. I still do not understand the sentence * the best function is to concatenate the hidden stat of the fist word in a chunk in forward RNN and that of the last word in backward RNN*. The RNN is over what all the words in the chunk? in the passage? The answer the authors gave in the response does not clarify this point.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",4,3,"Here is the structured peer review:

**Strengths:**

1. The paper proposes a novel approach to reading comprehension question answering, which is a significant contribution to the field.
2. The authors provide a clear and concise explanation of the proposed model's architecture and its components.
3. The paper presents a well-defined problem and a clear research question, which is a good starting point for a research paper.

**Weaknesses:**

1. The paper lacks a clear motivation for the proposed model's architecture. The authors do not provide a clear explanation of why the attention mechanism and convolutional re-encoding are beneficial, and how they contribute to the overall performance of the model.
2. The paper does not provide a thorough analysis of the effectiveness of the proposed model. The authors only compare their model to the baseline, but do not provide a detailed comparison to other state-of-the-art models in the field.
3. The paper does not provide a clear explanation of the choice of hyperparameters and the optimization procedure. The authors do not discuss the impact of hyperparameter tuning on the model's performance.
4. The paper does not provide a clear explanation of the linguistic features used in the model. The authors do not discuss the advantages and disadvantages of using linguistic features compared to character embeddings.
5. The paper lacks a clear evaluation of the model's performance on different types of questions. The authors only evaluate the model on the SQuAD dataset, but do not discuss its performance on other datasets or question"
Deep Biaffine Attention for Neural Dependency Parsing | OpenReview,"This paper builds off recent work from Kiperwasser & Goldberg (2016) using neural attention in a simple graph-based dependency parser. We use a larger but more thoroughly regularized parser than other recent BiLSTM-based approaches, with biaffine classifiers to predict arcs and labels. Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95.7% UAS and 94.1% LAS on the most popular English PTB dataset. This makes it the highest-performing graph-based parser on this benchmark—outperforming Kiperwasser & Goldberg (2016) by 1.8% and 2.2%—and comparable to the highest performing transition-based parser (Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over other graph-based approaches.","The paper proposes a new function for computing arc score between two words in a sentence for dependency parsing. The proposed function is biaffine in the sense that it*s a combination of a bilinear score function and a bias term playing a role as prior. The paper reports new state-of-the-art dependency parsing performances on both English PTB and Chinese TB. The paper is very well written with impressive experimental results and analysis. However, the idea is hardly novel regarding to the theme of the conference: the framework that the paper uses is from Kiperwasser & Goldberg (2016), the use of bilinear score function for attention is from Luong et al (2015). Projecting BiLSTM outputs into different spaces using MLPs is a trivial step to make the model *deeper*, whereas adding linear bias terms isn*t confirmed to work in the experiments (table 2 shows that diag bilinear has a close performance to biaffine). I think that this paper is more proper for NLP conferences.  The paper brings the new STOA in PTB dependency parsing. The numbers are very impressive. Built upon the framework of K&G parser, this improvement is achieved by mainly two things -- (1) the paper replace the original scorer using bilinear scorer and make a difference between the head of modifier representation (2) the hyperparameter tuning in the ADAM trainer. Although I think the bilinear modification make some sense intuitively, I don*t think this contribution alone is strong enough for a conference publication. The authors did not show a good explanation of why this approach works better in this case nor did the author show this modification is generally applicable in any other tasks.  This is primarily an engineering paper. The authors find a small architectural modification to prior work and some hyperparameter tuning which pushes up the state-of-the-art in dependency parsing in two languages. The architecture modification is a biaffine attention mechanism, which was inspired work in neural machine translation by Luong et al. (2015). The proposed attention model appears to be a win-win: better accuracy, reduced memory requirements, and fewer parameters. The performance of the model is impressive, but how the performance is achieved is not very impressive. I do not believe that there are novel insights in the paper that will generalize to other tasks, nor does the paper shed light on the dependency parsing tasks (e.g., does biaffine attention have a linguistic interpretation?).","['Rating:###5: Marginally below acceptance threshold', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",5,4,"Here is the structured peer review:

**Strengths:**

1. The paper presents impressive experimental results, achieving state-of-the-art performance on standard treebanks for six different languages.
2. The authors provide a clear and well-written explanation of their approach, making it easy to understand for non-experts.
3. The use of biaffine attention in dependency parsing is a novel application of this technique, and the authors demonstrate its effectiveness.
4. The paper provides a thorough analysis of the hyperparameter choices and their impact on parsing accuracy.

**Weaknesses:**

1. The idea of using biaffine attention in dependency parsing is not entirely novel, as it builds upon existing work by Kiperwasser & Goldberg (2016) and Luong et al. (2015).
2. The paper does not provide a clear explanation of why the biaffine attention mechanism is particularly effective in this context, or how it generalizes to other tasks.
3. The authors' claim that their approach is a ""win-win"" in terms of accuracy, memory requirements, and parameter count may be overstated, as the performance gains may be due to the specific hyperparameter tuning rather than the biaffine attention mechanism itself.
4. The paper does not provide any linguistic insights or theoretical justification for the use of biaffine attention in dependency parsing.

**Rating:** 7
**Confidence:** 3

Note: The rating is 7 because the paper"
Universality in halting time | OpenReview,"The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that remains unchanged even when the input is changed drastically. We observe two main classes, a Gumbel-like distribution that appears in Google searches, human decision times, QR factorization and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.","The authors explore whether the halting time distributions for various algorithms in various settings exhibit *universality*, i.e. after rescaling to zero mean and unit variance, the distribution does not depend on stopping parameter, dimensionality and ensemble. The idea of the described universality is very interesting. However I see several shortcomings in the paper: In order to be of practical relevance, the actual stopping time might be more relevant than the scaled one. The discussion of exponential tailed halting time distributions is a good start, but I am not sure how often this might be actually helpful. Still, the findings in the paper might be interesting from a theoretical point of view. Especially for ICLR, I think it would have been more interesting to look into comparisons between stochastic gradient descent, momentum, ADAM etc on different deep learning architectures. Over which of those parameters does universality hold?. How can different initializations influence the halting time distribution? I would expect a sensible initialization to cut of part of the right tail of the distribution. Additionally, I found the paper quite hard to read. Here are some clarity issues: - abstract: *even when the input is changed drastically*: From the abstract I*m not sure what *input* refers to, here - I. Introduction: *where the stopping condition is, essentially, the time to find the minimum*: this doesn*t seem to make sense, a condition is not a time. I guess the authors wanted to say that the stopping condition is that the minimum has been reached? - I.1 the notions of dimension N, epsilon and ensemble E are introduced without any clarification what they are. From the later parts of the paper I got some ideas and examples, but here it is very hard to understand what these parameters should be (just some examples would be already helpful) - I.3 *We use x^ell for ell in Z={1, dots, S} where Z is a random sample from of training samples* This formulation doesn*t make sense. Either Z is a random sample, or Z={1, ..., S}. - II.1 it took me a long time to find the meaning of M. As this parameter seems to be crucial for universality in this case, it would be very helpful to point out more explicitly what it refers to.  Summary For several algorithms, previous research has shown that the halting time follows a two-parameter distribution (the so-called universal property investigated by the authors). In this work, the authors extend the investigation to new algorithms (spin-glass, gradient descent in deep learning). An algorithm is considered to satisfy the universality property when the centered/scaled halting time fluctuations (empirical distribution of halting times) depend on the algorithm but do not depend on the target accuracy epsilon, an intrinsic measure of dimension N, the probability distribution/random ensemble. (This is clear from Eq 1 where on the left the empirical halting time distribution depends on epsilon, N, A, E and on the right, the approximation only depends on the algorithm) The authors argue that empirically, the universal property is observed when both algorithms (spin glass and deep learning) perform well and that it is not observed when they do not perform well. A moment-based indicator is introduced to assess whether universality is observed. Review This paper presents several problems. page 2: “[…] for sufficiently large N and eps = eps(N)” The dependence of epsilon on N is troubling. page 3: “Universality is a measure of stability in an algorithm […] For example […] halting time for the power method […] has infinite expectation and hence this type of universality is *not* present. One could use this to conclude that the power method is naive. Therefore the presence of universality is a desirable feature of a numerical method” No. An algorithm is naive if there are better ways to answer the problem. One could not conclude from a halting time with infinite expectation (e.g. solving a problem extremely quickly 99% of the time, and looping forever in 1% of cases) or infinite variance, that the algorithm is naive.? Moreover the universal property is more restrictive than having a finite halting time expectation. Even if in many specific cases, having a finite halting time expectation is a desirable property, showing that the presence of universality is desirable would require a demonstration that the other more restrictive aspects are also desirable.? Also, the paragraph only concerns one algorithm. why would the conclusions generalise to all numerical methods ?? Even if the universality property is arguably desirable (i.e. event if the conclusion of this paragraph is assumed correct), the paragraph does not support the given conclusion. Comparing Eq 1 and figures 2,3,4,5? From Eq 1, universality means that the centered/scaled halting time fluctuations (which depend on A, epsilon, N, E) can be approximated by a distribution that only depends on A (not on epsilon, N, E) but in the experiments only E varies (figures 2,3,4,5). The validity of the approximation with varying epsilon or N is never tested The ensembles/distributions parameter E (on which halting fluctuations should not depend) and the algorithm A (on which halting fluctuations are allowed to depend) are not well defined, especially w.r.t. the common use of the words. In the optimisation setting we are told that the functional form of the landscape function is part of A (in answer to the question of a reviewer) but what is part of the functional form ? what about computations where the landscape has no known functional form (black box) ? The conclusion claims that the paper “attempts to exhibit cases” where one can answer 5 questions in a robust and quantitative way. Question 1: “What are the conditions on the ensembles and the model that lead to such universality ?”?The only quantitative way would be to use the moments based indicator however there is only one example of universality not being observed which concerns only one algorithm (conjugate gradient) and one type of failure (when M = N). This does not demonstrate robustness of the method. Question 2: “What constitutes a good set of hyper parameters for a given algorithm ?” The proposed way to choose would be to test whether universality is observed. If it is then the hyper parameters are good, if not the hyper parameters are bad. The correspondance between bad hyper-parameters and observing no universality concerns only one algorithm and one type of failure. Other algorithms may fail in the universal regime or perform well in the non universal regime. The paper does not show how to answer this question in a robust way. Question 3: *How can we go beyond inspection when tuning a system ??* The question is too vague and general and there is probably no robust and quantitative way to answer it at all. Question 4: *How can we infer if an algorithm is a good match to the system at hand ??* The paper fails to demonstrate convincingly that universality is either a good or robust way to approach the very few studied algorithms. The suggested generalisation to all systems and algorithms is extremely far fetched. Question 5: *What is the connection between the universal regime and the structure of the landscape ?* ?Same as before, the question is extremely vague and cannot be answered in a robust or quantitative way at all. The fact that what corresponds to A and what corresponds to E is not clear does not help. In the conclusion it is written that the paper validates the claim that universality is present in all or nearly all sensible computation. It does not. The paper does not properly test whether universality is present (only 1 parameter in 3 that should not vary is tested). The paper does not properly test whether universality is lost when the computation is no longer sensible (only one failure case tested). Finally the experiments do not apply to all or nearly all computations but only to very few specific algorithms.  The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short: 1. Exactly one algorithm is shown for the deep learning example. It would have been more convincing to compare distributions with one or more algorithms. 2. The definition (1), and much of the work of Section 2.1 seems to have already been covered in Deift (2014), Section 1.3. In that paper, a number of different algorithms for the solution of linear systems are considered, and then the concept of universality becomes more plausible. I do not see enough of such algorithmic comparisons in this paper (same problem setup, different algorithms). 3. It seems to me that what practitioners might care about in practice are both the mean and variance in running times; these quantities are buried in (1). So I question how useful the distribution itself might be for algorithm tuning. At the least, many more empirical comparisons should be provided to convince me that the universality holds across a broad range of algorithms.","['Rating:###5: Marginally below acceptance threshold', 'Rating:###2: Strong rejection', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",5,3,"Here is the structured peer review:

**Strengths:**

1. The idea of universality in halting time is an interesting one, and the authors' empirical evidence for its existence is intriguing.
2. The paper presents a clear and concise introduction to the concept of universality and its implications for optimization algorithms.

**Weaknesses:**

1. The paper lacks clarity and concision in its writing style, making it difficult to follow the authors' arguments and understand the context.
2. The definition of universality is not well-defined, and the authors' attempt to quantify it using a moment-based indicator is problematic.
3. The paper relies too heavily on a single example (conjugate gradient) and fails to provide robust comparisons with other algorithms.
4. The authors' conclusion that universality is present in all or nearly all sensible computations is not supported by the limited number of experiments and the lack of robustness in the results.
5. The paper does not address the limitations and potential biases of the chosen algorithms and experimental setup.
6. The discussion of universality is not well-connected to the broader context of optimization algorithms and their applications.
7. The paper does not provide a clear and concise summary of the main findings and implications.

**Rating:** 6
**Confidence:** 2

Note: The rating and confidence score reflect the reviewer's overall assessment of the paper's strengths and weaknesses. A rating of 6 indicates that the paper has some redeeming qualities, but"
Quasi-Recurrent Neural Networks | OpenReview,"Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep’s computation on the previous timestep’s output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.","This paper introduces the Quasi-Recurrent Neural Network (QRNN) that dramatically limits the computational burden of the temporal transitions in sequence data. Briefly (and slightly inaccurately) model starts with the LSTM structure but removes all but the diagonal elements to the transition matrices. It also generalizes the connections from lower layers to upper layers to general convolutions in time (the standard LSTM can be though of as a convolution with a receptive field of 1 time-step). As discussed by the authors, the model is related to a number of other recent modifications of RNNs, in particular ByteNet and strongly-typed RNNs (T-RNN). In light of these existing models, the novelty of the QRNN is somewhat diminished, however in my opinion their is still sufficient novelty to justify publication. The authors present a reasonably solid set of empirical results that support the claims of the paper. It does indeed seem that this particular modification of the LSTM warrants attention from others. While I feel that the contribution is somewhat incremental, I recommend acceptance.  This paper introduces a novel RNN architecture named QRNN. QNNs are similar to gated RNN , however their gate and state update functions depend only on the recent input values, it does not depend on the previous hidden state. The gate and state update functions are computed through a temporal convolution applied on the input. Consequently, QRNN allows for more parallel computation since they have less operations in their hidden-to-hidden transition depending on the previous hidden state compared to a GRU or LSTM. However, they possibly loose in expressiveness relatively to those models. For instance, it is not clear how such a model deals with long-term dependencies without having to stack up several QRNN layers. Various extensions of QRNN, leveraging Zoneout, Densely-connected or seq2seq with attention, are also proposed. Authors evaluate their approach on various tasks and datasets (sentiment classification, world-level language modelling and character level machine translation). Overall the paper is an enjoyable read and the proposed approach is interesting, Pros: - Address an important problem - Nice empirical evaluation showing the benefit of their approach - Demonstrate up to 16x speed-up relatively to a LSTM Cons: - Somewhat incremental novelty compared to (Balduzizi et al., 2016) Few specific questions: - Is densely layer necessary to obtain good result on the IMDB task. How does a simple 2-layer QRNN compare with 2-layer LSTM? - How does the i-fo-ifo pooling perform comparatively? - How does QRNN deal with long-term time depency? Did you try on it on simple toy task such as the copy or the adding task?  The authors describe the use of convolutional layers with intermediate pooling layers to more efficiently model long-range dependencies in sequential data compared with recurrent architectures. Whereas the use of convolutional layers is related to the PixelCNN architecture (Oord et al.), the main novelty is to combine them with gated pooling layers to integrate information from previous time steps. Additionally, the authors describe extensions based on zone-out regularization, densely connected layers, and an efficient attention mechanism for encoder-decoder models. The authors report a striking speed-up over RNNs by up to a factor of 16, while achieving similar or even higher performances. Major comment ============= QRNNs are closely related to PixelCNNs, which leverage masked dilated convolutional layers to speed-up computations. However, the authors cite ByteNet, which builds upon PixelCNN, only at the end of their manuscript and do not include it in the evaluation. The authors should cite PixelCNN already when introducing QRNN in the methods sections, and include it in the evaluation. At the very least, QRNN should be compared with ByteNet for language translation. How well does a fully convolutional model without intermediate pooling layers perform, i.e. what is the effect to the introduced pooling layers? Are their performance difference between f, fo, and ifo pooling? Did the authors investigate dilated convolutional layers? Minor comments ============== 1. How does a model without dense connections perform, i.e. what is the effect of dense connections? To illustrate dense connections, the authors might draw them in figure 1 and refer to it in section 2.1. 2. The run-time results shown in figure 4 are very helpful, but as far as I understood, the breakdown shown on the left side was measured for language modeling (referred in 3.2), whereas the dependency on batch- and sequence size shown on the right side for sentiment classification (referred in 3.1). I suggest to consistently show the results for either sentiment classification or language modeling, or both. At the very least, the figure caption should describe the task explicitly. Labeling the left and right figure by a) and b) would further improve readability. 3. Section 3.1 describes a high speed-up for long sequences and small batch sizes. I suggest motivating why this is the case. While computations can be parallelized along the sequence length, it is less obvious why smaller batch sizes speed-up computations. 4. The proposed encoder-decoder attention is different from traditional attention in that attention vectors are not computed and used as input to the decoder sequentially, but on top of decoder output states in parallel. This should be described and motivated in the text. Sentiment classification ------------------------ 5. What was the size of the hold-out development set and how was it created? The text describes that data were split equally into training and test set, without describing the hold-out set. 6. What was the convolutional filter size? 7. What is the speed-up for the best hyper-parameters (batch size 24, sequence length 231)? 8. Figure 3 would be easier to interpret by actually showing the text on the y-axis. For the sake of space, one might use a smaller text passage, plot it along the x-axis, and show the activations of fewer neurons along the y-axis. Showing more examples in the appendix would make the authors’ claim that neurons are interpretable even more convincing. Language modeling ----------------- 9. What was the size of the training, test, and validation set? 10. What was the convolutional filter size, denoted as ‘k’? 11. Is it correct that a very high learning rate of 1 was used for six epochs at the beginning? 12. The authors should show learning curves for a models with and without zone-out. Translation ----------- 13. What was the size of the training, test, and validation set? 14. How does translation performance depend on k?  This paper points out that you can take an LSTM and make the gates only a function of the last few inputs - h_t = f(x_t, x_{t-1}, ...x_{t-T}) - instead of the standard - h_t = f(x_t, h_{t-1}) -, and that if you do so the networks can run faster and work better. You*re moving compute from a serial stream to a parallel stream and also making the serial stream more parallel. Unfortunately, this simple, effective and interesting concept is somewhat obscured by confusing language. - I would encourage the authors to improve the explanation of the model. - Another improvement might be to explicitly go over some of the big Oh calculations, or give an example of exactly where the speed improvements are coming from. - Otherwise the experiments seem adequate and I enjoyed this paper. This could be a high value contribution and become a standard neural network component if it can be replicated and if it turns out to work reliably in multiple settings.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###7: Good paper, accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",6,4,"**Structured Peer Review**

**Strengths:**

1. The paper introduces a novel approach to neural sequence modeling, specifically Quasi-Recurrent Neural Networks (QRNNs), which can significantly reduce the computational burden of temporal transitions in sequence data.
2. The authors provide a clear and concise explanation of the QRNN architecture and its benefits, including increased parallelism and faster training times.
3. The empirical results demonstrate the effectiveness of QRNNs on various sequence tasks, including language modeling, sentiment classification, and character-level machine translation.
4. The authors provide a thorough analysis of the model's performance and discuss potential extensions, such as zone-out regularization, densely connected layers, and efficient attention mechanisms.

**Weaknesses:**

1. The novelty of the QRNN architecture may be diminished by its similarity to existing models, such as ByteNet and PixelCNN.
2. The paper could benefit from a more detailed explanation of the model's theoretical underpinnings and the mathematical derivations of the proposed architecture.
3. The authors may want to provide more context and background information on the specific sequence tasks and datasets used in the experiments.
4. Some of the experimental results, such as the learning curves and speed-up values, could be more clearly presented and labeled for easier interpretation.
5. The paper could benefit from more discussion on the limitations and potential drawbacks of the QRNN architecture, such as its potential loss of expressiveness compared to traditional RNNs.

**Rating:** 8/10"
Semi-Supervised Detection of Extreme Weather Events in Large Climate Datasets | OpenReview,"The detection and identification of extreme weather events in large scale climate simulations is an important problem for risk management, informing governmental policy decisions and advancing our basic understanding of the climate system. Recent work has shown that fully supervised convolutional neural networks (CNNs) can yield acceptable accuracy for classifying well-known types of extreme weather events when large amounts of labeled data are available. However, there are many different types of spatially localized climate patterns of interest (including hurricanes, extra-tropical cyclones, weather fronts, blocking events, etc.) found in simulation data for which labeled data is not available at large scale for all simulations of interest. We present a multichannel spatiotemporal encoder-decoder CNN architecture for semi-supervised bounding box prediction and exploratory data analysis. This architecture is designed to fully model multi-channel simulation data, temporal dynamics and unlabelled data within a reconstruction and prediction framework so as to improve the detection of a wide range of extreme weather events. Our architecture can be viewed as a 3D convolutional autoencoder with an additional modified one-pass bounding box regression loss. We demonstrate that our approach is able to leverage temporal information and unlabelled data to improve localization of extreme weather events. Further, we explore the representations learned by our model in order to better understand this important data, and facilitate further work in understanding and mitigating the effects of climate change.","This paper applies convnet-based object detection techniques to detection of weather events from 3D climate data, additionally exploring the effect of using an unsupervised autoencoder-style objective term. Pros: The application of object detection techniques to extreme weather event detection problem is unique, to my knowledge. The paper is well-written and describes the method well, including a survey of the related work. The best model makes use of 3D convolutions and unsupervised learning, both of which are relatively unexplored in the detection literature. Both of these aspects are validated and shown to produce at least small performance improvements over a 2D and/or purely supervised approach. Cons: The benefits of the 3D convolutional architecture and unsupervised learning end up being a little underwhelming, with 52.92% mAP for the 3D+semi-sup result vs. 51.42% mAP for the 2D+sup result. It’s a bit strange that 3D+sup and 2D+semi-sup are each worse than the 2D+sup base result; I’d expect each aspect to give a slight improvement over the base result, given that using both together gives the best results -- perhaps there was not a thorough enough hyperparameter search for these cases. The paper does acknowledge this and provide potential explanations in Sec. 4.3, however. As other reviewers pointed out, the use of the 0.1 IoU criterion for true positives is very loose relative to the standard 0.5 criterion. On the other hand, if the results visualized in Figure 3 are typical, a 0.1 overlap criterion could be reasonable for this domain as the detector does seem to localize events well enough that the system could be used to expedite human review of the climate images for extreme events. Still, it would be useful to also report results at higher overlap thresholds. Minor: eq 6 should (probably) be the squared L2 norm (i.e. the sum of squares) rather than the L2 norm itself. Minor: table 4 -- shouldn’t the semi-supervised models have more parameters than the corresponding supervised ones due to the decoder layers? Overall, this paper is well-written and applies some interesting underutilized techniques to a relatively unique domain. The results aren*t striking, but the model is ablated appropriately and shown to be beneficial. For a final version, it would be nice to see results at higher overlap thresholds.  [EDIT: The thoughtful author responses addressed my major concerns. The github links for data and code will be really helpful for reproducing results (I haven*t looked carefully, but this is great). The revision addressed many issues, including the additional results. As such I am upgrading my rating from a 5 to a 6 and recommend acceptance of the paper.] The paper proposes to apply deep nets to perform detection and localization of extreme weather events in simulated weather data. The problem is related to object detection in computer vision in that the input is a 2D “image” (multichannel spatial weather data) or 3D “video” (temporal version of the data) and the output is a bounding box (spatial-temporal localization of a weather event) and class label (weather event type). It differs from standard object detection in that the input has multiple heterogenous channels and labeled data is scarce. A simple but quite reasonable deep net is proposed for the task based on similar approaches in computer vision. While proposal based systems are most popular in vision currently (in particular Faster-RCNN) the proposed approach is simple and a fine starting point. There is little innovation on the part of the detection system, but as noted, it is a valid application of ideas from computer vision to the task at hand. The authors propose both a supervised approach (only ground truth bounding box location/label is used) and a semi-supervised approach that additionally incorporates the reconstruction loss as a regularization. In all cases the losses are fairly standard and again, reasonable. The only confusing bit is that the “semi-supervised” loss actually has all the labels used for the “supervised” loss and additionally incorporates the reconstruction loss. Hence, the “semi-supervised” loss is actually stronger, which makes the terminology a bit confusing. The paper is easy to follow, but notation is sloppy. For example, above equation 5 it states that “the loss is a weighted combination of reconstruction error and bounding box regression loss”; actually it’s a combination of the supervised and unsupervised loss (Lsup and Lunsup), and Lrec is not defined (although I assume Lrec=Lunsup). The paper is fairly non-technical, but nevertheless these minor issues should be fixed. (E.g., see also reference to “figure 4 and 4”). The biggest concern w the paper though is experimental results. Only a single figure and table of results are shown (figure 4 and table 4). The metrics are not defined (what is mean average recall?). Only 2D versus 3D version of the model are shown, and supervised and semi-supervised. Moreover, numbers seem a bit all over the place, without consistent patterns (e.g., why is 2D supervised better than the seemingly much strong 3D semi-supervised?). One of the things that is unclear is how many events are actually in the training/testing data, and more importantly, how good are these results in absolute terms? Regardless, the experiments are fairly sparse and ablation studies and more discussion lacking. It is also unclear if future researchers will be able to reproduce the experimental setting (a commitment to open-source the data or a way to reproduce the experiments would be critical for future authors). Minor nit: the authors use both a classification loss and an “objectness” loss. I’ve never seen both used together like this (normally objectness is used in two-stage object proposal systems where in the first stage class-agnostic proposals are given and in the second stage these are cropped and a class-specific classifier is applied). I strongly suspect removing the objectness loss would not impact results since the classification loss should provide strictly stronger supervisory signal. Regardless, this is a fairly non-standard choice and should be justified (experimentally). Overall this is a borderline paper. I do believe that it is valuable to apply computer vision techniques to a domain that I’ve see little work on in our community. That being said, I have no expertise on this type of data -- it’s possible this deep learning techniques are now routinely used in the climate science literature (I suspect not, though). Overall, there is little novelty on the algorithmic side in this paper (the equations in section 3 are commonly used in the cv literature). The use of reconstruction loss to improve results in the data-sparse setting is interesting, but the experimental results are inconclusive. The experimental validation is generally insufficient. Reproducibility for future research is difficult unless the data is open-sourced. Overall, I think this paper is a good start, and with improved experiments and more careful writing I think could eventually make for a decent paper.  This work presents a novel 3D CNN architecture for climate event detection that combines an unsupervised auto-encoder reconstruction loss with YOLO like bounding box prediction. The approach is trained and evaluated on a large-scale, simulated climate dataset labeled by a costly heuristic approach called TECA. For the most part, the paper is nicely written (minor comments below) and addresses an important and well motivated problem. The authors provide sufficient model details to allow reproduction (although public code would be preferred). I find the experiments a bit unconvincing (see below) but appreciate the attention to model capacity (via number of parameter) when comparing the 2D and 3D model variants. I am concerned that the evaluation may be insufficient to assess the effectiveness of this method. An IoU threshold of 0.1 allows for many rather poor detections to count as true positives. If the goal of this work is to count the number of such instances this is defensible, but for localization this seems overly loose.Furthermore, the 3D CNN architecture (which is one of the the core novelties of this work compared to past efforts) does not seem capable of producing variable sized boxes (as noted in the last paragraph of page 7), which I imagine results in poorer performance at higher IoU thresholds (as many of the weather events appear small). The experiments also feel inconclusive about the effect of temporal modeling and semi-supervision. The temporal component does not seem to matter in the supervised settings (2D 51.45 mAP - 3D 51.00 mAP) but improves somewhat in the semi-supervised case (2D 51.11 mAP - 3D 52.92 mAP). Whereas the additional unlabeled data seems to hurt in the 2D case but improve results for the 3D model. Could the authors provide confidence intervals for these numbers? I would like to see further discussion of these trends especially with respect to the effect of the loss weights (alpha, beta, and gamma). I also note that it is not clear if both the 2D and 3D models were trained for equivalent time periods (seems like no from last paragraph of page 7). Could a plot of training and validation accuracy for each model be presented for comparison? Finally, is there any baseline approach the authors could report or compare too? Without one, it is difficult to evaluate the performance of the approach with respect to the difficulty of the problem. Preliminary Rating: I think this is an interesting paper that is well motivated but feel the experiments as presented do not seem adequate to support any conclusive trends. I would like to see the mAP trends across a wider range of IoU values and further discussion of training procedure, loss weight settings, and reasons for lack of bounding box variability in the 3D model (as stated above). Clarification: In the paper you say *While climate models are run on a 3D grid, with the vertical dimension corresponding to 30 levels; we only consider surface quantities (i.e. 2D data) in this study.* Could you elaborate on what the surface quantities correspond to? Is it the highest cloud level? Minor notes: Please provide years for Prabhat et al. references rather than a and b. Footnote in 4.2 could be inline text with similar space. 4.3 second paragraph the word table is not capitalized like elsewhere. 4.3 4th paragraph the word section is not capitalized like elsewhere. Edit: I appreciate the authors responding to my questions but still feel the relatively poor localization performance at stricter IoU thresholds fails to justify the complexity of the approach. I encourage the authors to continue pursuing this line of research.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###4: Ok but not good enough - rejection']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",6,4,"Here is the structured peer review:

**Strengths:**

1. The paper applies deep learning techniques to a unique problem in climate science, which is a valuable contribution to the field.
2. The authors provide a clear and concise description of the method, including the architecture and training procedure.
3. The paper is well-written and easy to follow, making it accessible to a broad audience.
4. The authors provide a thorough discussion of the experimental results and provide some insights into the performance of the model.

**Weaknesses:**

1. The experimental results are inconclusive, and the evaluation metrics are not well-defined, making it difficult to assess the effectiveness of the method.
2. The paper relies heavily on a costly heuristic approach to label the data, which may not be representative of real-world climate data.
3. The 3D CNN architecture does not seem to produce variable-sized bounding boxes, which may limit its performance at higher IoU thresholds.
4. The paper does not provide sufficient discussion of the effect of temporal modeling and semi-supervision on the performance of the model.
5. The evaluation metrics are not well-defined, and the paper does not provide confidence intervals for the results.
6. The paper does not provide a clear comparison to a baseline approach, making it difficult to evaluate the performance of the method.
7. The notation is sometimes sloppy, and the paper could benefit from more precise and consistent notation.

**Rating:** 6
**Confidence:** 3"
Online Bayesian Transfer Learning for Sequential Data Modeling | OpenReview,"We consider the problem of inferring a sequence of hidden states associated with a sequence of observations produced by an individual within a population. Instead of learning a single sequence model for the population (which does not account for variations within the population), we learn a set of basis sequence models based on different individuals. The sequence of hidden states for a new individual is inferred in an online fashion by estimating a distribution over the basis models that best explain the sequence of observations of this new individual. We explain how to do this in the context of hidden Markov models with Gaussian mixture models that are learned based on streaming data by online Bayesian moment matching. The resulting transfer learning technique is demonstrated with three real-word applications: activity recognition based on smartphone sensors, sleep classification based on electroencephalography data and the prediction of the direction of future packet flows between a pair of servers in telecommunication networks.","This paper proposes an online inference algorithm by using online Bayesian moment matching for HMM-GMM. The method uses transfer learning by utilizing individual sequence estimators to predict a target sequence based on a weighted combination of individual HMM-GMM. Online Bayesian moment matching has a benefit of updating HMM-GMM parameters frame-by-frame, and fits to this problem. The authors compare the proposed method with the other sequential modeling methods including RNN and EM, and show the effectiveness of the proposed method. The paper is well written overall. Comments: 1) Could you provide the average performance in table? It is difficult to compare the performance only with individual performance. Also, it seems that the EM performance is sometimes good 2) I’m curious how initialization and hyper-parameter settings affect the final performance. If you provide some information about it, that is great. 3) It would be better to provide a figure of describing the transfer-learning-based proposed methods, since this is a unique and a little bit complicated setup.  I*m bumping up my score to a 7 to acknowledge that the authors responded satisfactorily to reviewer feedback, and to indicate that I think that the updated manuscript is a strong paper and that I do not object to its acceptance to ICLR. However, I also would not fight for its acceptance. I still think that it is a better fit for a venue with an explicit interest in more traditional Bayesian latent variable models (ICML, UAI, NIPS). ----- This manuscript describes a novel Bayesian approach to transfer learning focused on online sequence modeling settings where he primary concern is less distribution drift in an ongoing sequence and more variability between individual sequences (since each sequence can be thought of as defining its own conditional distribution over subsequent states). They provide the example of human gait classification, where each individual*s gait may differ from others even while performing the same activity. In this setting we typically train the gait model on gait sequences from a set of *source* individuals but then apply it to previously unseen people. The core model is an HMM, which they give a full Bayesian treatment; the central problem this introduces is that each new observation that arrives introduces an additional product term to the posterior that is itself a product over M components (clusters or hidden states). This rapidly becomes intractable. This paper applies Bayesian moment matching (BMM) to this problem, in which the posterior is approximated via projection onto a more tractable distribution that is adjusted to match some moments of the posterior. The experimental results are quite promising Strengths: - The problem setting is very appropriately framed as online sequence prediction via Bayesian transfer learning. There is almost certainly individual variability between the training sequences (for which explanatory variables not be available in the inputs). The Bayesian approach gives a natural approach to performing transfer and handling low data regimes (common in all experiments). - The Bayesian formulation creates a computational challenge (the posterior becomes intractable). The proposed BMM approximation is both reasonable and relatively novel (particularly given the popularity of MCMC and variational methods). - With the caveat that I am not well-versed in recent work on Bayesian moment matching, a cursory literature search suggests this is a novel application of BMM. A lot of the related BMM work is roughly contemporaneous with this work, and none of it seems concerned with online transfer learning. - The overall results look quite strong: in activity recognition and flow prediction, the transfer learning approach is in general superior to the one-size-fits-all HMM, even when trained using BMM (which in turn is generally superior to the EM-based HMM). The proposed approach appears to beat the LSTM across all tasks (including sleep stage classification), possibly due to the lack of training data. Weaknesses: - The description of the LSTM training and architecture search is vague (and in one instance, contradictory), strongly implying that it was not fully tuned and may be an artificially weak baseline. While it is plausible that the proposed approach might excel given the small data sets used in the experiments, there is not sufficient evidence and detail to support this claim. The authors should provide more detail about architecture search, hyperparameter tuning, and most important, attempts at regularization, given the limited training data. In particular, the authors should experiment with a sufficiently rich set of settings for # hidden layers, # hidden units, weight decay, and dropout. - Given the emphasis on transfer learning and the use of a Bayesian framework, the decision to train source models independently is a little odd. Why not perform some kind of joint training? - The authors provide no analysis, analytical or empirical, of the proposed framework*s (storage and computational) complexity, especially at prediction time. At the top of page 10, they mention using only one EEG channel in order to *reduce complexity and processing time.* This is an ominous hint that the proposed framework may not scale practically. - Additionally, I have a meta-concern about this paper*s fit for ICLR. *Representation learning* -- the general theme of ICLR -- is not featured prominently in this work. Given the competitive nature of ICLR, we should consider seriously whether this paper is of interest to the wider ICLR community or whether it might be a better fit for a meeting such as AISTATS, ICML, or UAI. Comments: - The plots in Figures 1-3 are difficult to interpret. Putting patients along the X-axis is unintuitive since their order is arbitrary. Why not just make scatter plots of one vs. the other model*s accuracy. The shape of the scatter should hopefully make it clear if there is a general trend. - In Experimental Setup, the authors make conflicting claims about the LSTM architecture. They first state the single hidden layer has number of cells equal to the number of inputs. They then say that the number of LSTM units is finetuned based on empirical performance. - While generally well-written, the paper has several obscenely long paragraphs. The single paragraph *Experimental Setup* section, for example, takes up more than half of page 8. These should be broken up into shorter paragraphs to make them easier to read. A good rule of thumb is that no paragraph should take up more than ~1/6 of a page. In general, I like this work, but the vague details around the LSTM training raise serious red flags about their experimental results, at least the comparison vs. LSTMs, and I have concerns about how well it meets ICLR*s CFP. That said, my policy for interactive review is to carefully consider author responses with an open mind, so I will serious consider changing my score, if warranted.  This paper looks at transfer learning on sequence. First individual Bayesian moment matched algorithms are used on each individual. Then for a test setting, the probabilities for the new domain are a similarity weighted set of probabilities from the training setting. The similarity weighting is given by the (approximate) posterior probability of the observation (either complete or so far, depending on whether an online scheme is used) for each domain. A few comments on the model. First there is a discrepancy between train and test: in the test domain there is an assumption in some sort of relationship between individuals, but at training all individuals are treated independently. This contrasts with models that try to analyze between-subject and within-subject variation inherently in the training data. A discussion of these points would be valuable. Does this assume the data about individuals is extensive, so such sharing is not necessary? The Bayesian posterior on lambda and pi provides a means of model averaging. But weighted averaging of models is very different from weighted averaging of transition probabilities. Given this discrepancy, it would have been good to have a bit more discussion about this choice. Clearly it is pragmatic, but what do you lose and what do you gain? Does this fit into the moment matching interpretation? Of course parameterized sharing is common in multitask settings, but more could be said about this. Experiments: This seems to be the real point of the paper: the authors have an actual problem to solve and developed this as a method to do this. Yet from the title it feels like the authors felt they had to bill it as a methodological paper for submission to ICLR. Personally I think it is unfortunate that this was a perceived need. A demonstration of what can be transferred in a domain like this is as important as how it is done. The experiments are on a valuable real world problem that people widely care about. This is the real strength of this paper, and a focus on this demonstrative aspect, and a corresponding conclusion would strengthen the paper.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",6,3,"Here is the structured peer review:

**Strengths:**

1. The problem setting is very appropriately framed as online sequence prediction via Bayesian transfer learning. The Bayesian approach gives a natural approach to performing transfer and handling low data regimes (common in all experiments).
2. The Bayesian formulation creates a computational challenge (the posterior becomes intractable). The proposed BMM approximation is both reasonable and relatively novel (particularly given the popularity of MCMC and variational methods).
3. The overall results look quite strong: in activity recognition and flow prediction, the transfer learning approach is in general superior to the one-size-fits-all HMM, even when trained using BMM (which in turn is generally superior to the EM-based HMM).
4. The experimental results are on a valuable real-world problem that people widely care about.

**Weaknesses:**

1. The description of the LSTM training and architecture search is vague (and in one instance, contradictory), strongly implying that it was not fully tuned and may be an artificially weak baseline.
2. The authors provide no analysis, analytical or empirical, of the proposed framework's (storage and computational) complexity, especially at prediction time.
3. The decision to train source models independently is a little odd, and the authors should consider discussing the implications of this choice.
4. The paper has several obscenely long paragraphs, which should be broken up into shorter paragraphs to make them easier to read.
5. The vague details around the LSTM training raise"
PREDICTION OF POTENTIAL HUMAN INTENTION USING SUPERVISED COMPETITIVE LEARNING | OpenReview,"We propose a learning method to quantify human intention. Generally, a human being will imagine several potential actions for a given scene, but only one of these actions will subsequently be taken. This makes it difficult to quantify human intentions. To solve this problem, we apply competitive learning to human behavior prediction as supervised learning. In our approach, competitive learning generates several outputs that are then associated with several potential situations imagined by a human. We applied the proposed method to human driving behavior and extracted three potential driving patterns. Results showed a squared error is reduced to 1/25 that of a conventional method . We also found that competitive learning can distinguish valid data from disturbance data in order to train a model.","This paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future based on video and vehicle state input. The architecture allows several RNNs to compete to make the best predictions, with only the best prediction receiving back propagation training at each time step. Preliminary experimental results show that this scheme can yield reduced prediction error. It is not clear how the best-performing RNN is chosen for each time point at test time. That is, how is the “integrated prediction” obtained in Fig. 7? Is the prediction the one with minimum error over all of the output layers? If so, this means the prediction cannot be made until you already know the value to be predicted. It seems possible that a larger generic RNN might be able to generate accurate predictions. If I understand correctly, the competitive architectures have many more parameters than the baseline. Is the improved performance here due to the competitive scheme, or just a larger model? A large amount of additional work is required to sustain the claim that this scheme is successfully extracting driver ‘intentions’. It would be interesting to see if the scheme, suitably extended, can automatically infer the intention to stop at a stop sign vs slowing but not stopping due to a car in front, say, or to pass a car vs simply changing lanes. Adding labels to the dataset may enable this comparison more clearly. More generally, the intention of the driver seems more related to the goals they are pursuing at the moment; there is a fair amount of work in inverse reinforcement learning that examines this problem (some of it in the context of driving style as well).  This paper proposes a neural network architecture for car state prediction while driving based on competitive learning. Competitive learning creates several duplicates of the baseline neural architecture and during training only updates the architecture with minimum loss. The experiments compare the competitive learning approach to a single baseline architecture on a driving benchmark task. The paper is understandable but could benefit from some copy editing. The competitive learning approach seems rather adhoc and this paper feels quite incomplete without significant discussion and comparisons to ensembling. Much recent work has shown that duplicating and ensembling neural architectures can produce gains, and it’s not clear why competitive learning is better than ensembling, it seems less theoretically sound to me. There is a huge confound in the experiments due to the competitive learning architecture having many more free parameters than the baseline architecture. Again I think comparing to ensembling with the same number of architectures duplicated and perhaps comparing to a single baseline with larger hidden layers to make the total number of free parameters comparable is critical to validating the proposed approach. The graphical model of the driving process depicted in figure 1 seems nonsensical. If e is observed then all variables are known given the dependencies shown. Further, it is at best very poor notation to say that the driving action d decided at time t affects the vehicle state s at that same time. It should be that s_t depends on d_(t-1). Also, according to this figure the driving decision d does not depend on the observed vehicle state x which also seems invalid. Odd to have a paragraph break in abstract Figure 1 caption should include a brief explanation of the variables shown  Authors propose a competitive learning architecture that learn different RNN predictors independently, akin to a committee of experts which are chosen with a hard switch at run-time. This work is applied to the task of predictive different driving behaviors from human drivers, and combines behaviors at test time, often switching behaviors within seconds. Prediction loss is lower than the similar but non-competitive architecture used as a baseline. It is not very clear how to interpret the results, what is the real impact of the model. If behaviors switch very often, can this really be seen as choosing the best driving mode for a given situation? Maybe the motivation needs to be rephrased a little to be more convincing? The competitive approach presented is interesting but not really novel, thus the impact of this paper for a conference such as ICLR may be limited.","['Rating:###2: Strong rejection', 'Rating:###2: Strong rejection', 'Rating:###4: Ok but not good enough - rejection']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",2,4,"Here is the structured peer review:

**Strengths:**

1. The paper proposes a novel approach to predicting human intention using competitive learning, which is a timely and relevant topic in the field of human-computer interaction and artificial intelligence.
2. The authors provide a clear and concise explanation of the competitive learning architecture and its application to the task of predicting driving behaviors.
3. The experimental results show a significant reduction in prediction error compared to the baseline approach, which is a promising finding.

**Weaknesses:**

1. The paper lacks a clear theoretical foundation and justification for the use of competitive learning in this context. The authors do not provide a clear explanation of why competitive learning is better than ensembling or other machine learning approaches.
2. The experimental results are not fully interpretable, and it is unclear what the real impact of the model is. The authors do not provide a clear explanation of how the model is choosing the best driving behavior, and the results may be influenced by the switching behavior of the drivers.
3. The paper could benefit from more rigorous evaluation and comparison to other approaches. The authors only compare their approach to a single baseline architecture, and do not provide a clear comparison to other competitive learning approaches.
4. The notation and graphical model in the paper are unclear and confusing, which makes it difficult to understand the underlying assumptions and relationships between the variables.
5. The paper could benefit from more discussion and exploration of the limitations and potential biases of the competitive learning approach.

**Rating"
Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening | OpenReview,"We propose a novel training algorithm for reinforcement learning which combines the strength of deep Q-learning with a constrained optimization approach to tighten optimality and encourage faster reward propagation. Our novel technique makes deep reinforcement learning more practical by drastically reducing the training time. We evaluate the performance of our approach on the 49 games of the challenging Arcade Learning Environment, and report significant improvements in both training time and accuracy.","In this paper, the authors proposed a extension to the DQN algorithm by introducing both an upper and lower bound to the optimal Q function. The authors show experimentally that this approach improves the data efficiency quite dramatically such that they can achieve or even supersede the performance of DQN that is trained in 8 days. The idea is novel to the best of my knowledge and the improvement over DQN seems very significant. Recently, Remi et al have introduced the Retrace algorithm which can make use of multi-step returns to estimate Q values. As I suspect, some of the improvements that comes from the bounds is due to the fact that multi-step returns is used effectively. Therefore, I was wondering whether the authors have tried any approach like Retrace or Tree backup by Precup et al. and if so how do these methods stack up against the proposed method. The author have very impressive results and the paper proposes a very promising direction for future research and as a result I would like to make a few suggestions: First, it would be great if the authors could include a discussion about deterministic vs stochastic MDPs. Second, it would be great if the authors could include some kind of theoretically analysis about the approach. Finally, I would like to apologize for the late review.  In this paper, a Q-Learning variant is proposed that aims at *propagating* rewards faster by adding extra costs corresponding to bounds on the Q function, that are based on both past and future rewards. This leads to faster convergence, as shown on the Atari Learning Environment benchmark. The paper is well written and easy to follow. The core idea of using relaxed inequality bounds in the optimization problem is original to the best of my knowledge, and results seem promising. This submission however has a number of important shortcomings that prevent me from recommending it for publication at ICLR: 1. The theoretical justification and analysis is very limited. As far as I can tell the bounds as defined require a deterministic reward to hold, which is rarely the case in practice. There is also the fact that the bounds are computed using the so-called *target network* with different parameters theta-, which is another source of discrepancy. And even before that, the bounds hold for Q* but are applied on Q for which they may not be valid until Q gets close enough to Q*. It also looks weird to take the max over k in (1, ..., K) when the definition of L_j,k makes it look like the max has to be L_j,1 (or even L_j,0, but I am not sure why that one is not considered), since L*_j,0 >= L*_j,1 >= ... >= L*_j,K. Neither of these issues are discussed in the paper, and there is no theoretical analysis of the convergence properties of the proposed method. [Update: some of these concerns were addressed in OpenReview comments] 2. The empirical evaluation does not compensate, in my opinion, for the lack of theory. First, since there are two bounds introduced, I would have expected *ablative* experiments showing the improvement brought by each one independently. It is also unfortunate that the authors did not have time to let their algorithm run longer, since as shown in Fig. 1 there remain a significant amount of games where it performs worse compared to DQN. In addition, comparisons are limited to vanilla DQN and DDQN: I believe it would have been important to compare to other ways of incorporating longer-term rewards, like n-step Q-Learning or actor-critic. Finally, there is no experiment demonstrating that the proposed algorithm can indeed improve other existing DQN variants: I agree with the author when they say *We believe that our method can be readily combined with other techniques developed for DQN*, however providing actual results showing this would have made the paper much stronger. In conclusion, I do believe this line of research is worth pursuing, but also that additional work is required to really prove and understand its benefits. Minor comments: - Instead of citing the arxiv Wang et al (2015), it would be best to cite the 2016 ICML paper - The description of Q-Learning in section 3 says *The estimated future reward is computed based on the current state s or a series of past states s_t if available.* I am not sure what you mean by *a series of past states*, since Q is defined as Q(s, a) and thus can only take the current state s as input, when defined this way. - The introduction of R_j in Alg. 1 is confusing since its use is only explained later in the text (in section 5 *In addition, we also incorporate the discounted return R_j in the lower bound calculation to further stabilize the training*) - In Fig. S1 the legend should not say *10M* since the plot is from 1M to 10M  This paper proposes an improvement to the q-learning/DQN algorithm using constraint bounds on the q-function, which are implemented using quadratic penalties in practice. The proposed change is simple to implement and remarkably effective, enabling both significantly faster learning and better performance on the suite of Atari games. I have a few suggestions for improving the paper: The paper could be improved by including qualitative observations of the learning process with and without the proposed penalties, to better understand the scenarios in which this method is most useful, and to develop a better understanding of its empirical performance. It would also be nice to include zoomed-out versions of the learned curves in Figure 3, as the DQN has yet to converge. Error bars would also be helpful to judge stability over different random seeds. As mentioned in the paper, this method could be combined with D-DQN. It would be interesting to see this combination, to see if the two are complementary. Do you plan to do this in the final version? Also, a couple questions: - Do you think the performance of this method would continue to improve after 10M frames? - Could the ideas in this paper be extended to methods for continuous control like DDPG or NAF?","['Rating:###9: Top 15% of accepted papers, strong accept', 'Rating:###4: Ok but not good enough - rejection', 'Rating:###9: Top 15% of accepted papers, strong accept']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",9,3,"Here is the structured peer review:

**Strengths:**

1. The paper proposes a novel approach to improving the efficiency of deep reinforcement learning by introducing constraint bounds on the Q-function, which is a fresh perspective on the problem.
2. The authors provide a clear and concise explanation of the proposed method, making it easy to follow and understand.
3. The experimental results show significant improvements in training time and accuracy, which is a strong indication of the effectiveness of the proposed approach.

**Weaknesses:**

1. The theoretical justification and analysis of the proposed method is limited, which may raise concerns about the applicability and robustness of the results.
2. The empirical evaluation is limited, as the authors only compare their method to vanilla DQN and DDQN, and do not provide a comprehensive analysis of its performance on other variants of DQN or other reinforcement learning algorithms.
3. The paper could benefit from more detailed explanations of the learning process and the scenarios in which the proposed method is most useful.
4. The authors do not provide a clear answer to the question of whether the performance of the method would continue to improve after a certain number of frames, which may be an important consideration for practitioners.
5. The paper does not explore the extension of the proposed method to continuous control problems, which is an important area of research in reinforcement learning.

**Rating:** 7
**Confidence:** 3

Note: The rating is based on the overall quality of the paper, and the"
Efficient Representation of Low-Dimensional Manifolds using Deep Networks | OpenReview,"We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space. We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data. Specifically we show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional Euclidean space. Remarkably, the network can do this using an almost optimal number of parameters. We also show that this network projects nearby points onto the manifold and then embeds them with little error. Experiments demonstrate that training with stochastic gradient descent can indeed find efficient representations similar to the one presented in this paper.","The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Specifically, the paper focuses on what the authors call *monotonic chains of linear segments*, which are essentially sets of intersecting tangent planes. The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error analysis of the resulting construction. While the presented results are novel to the best of my knowledge, they are hardly surprising (1) given what we already know about the representational power of deep networks and (2) given that the study selects a deep network architecture and a data structure that are very *compatible*. In particular, I have three main concerns with respect to the results presented in this paper: (1) In the last decade, there has been quite a bit of work on learning data representations from sets of local tangent planes. Examples that spring to mind are local tangent space analysis of Zhang & Zha (2002), manifold charting by Brand (2002) and alignment of local models by Verbeek, Roweis, and Vlassis (2003). None of this work is referred to in related work, even though it seems highly relevant to the analysis presented here. For instance, it would be interesting to see how these old techniques compare to the deep network trained to produce the embedding of Figure 6. This may provide some insight into the inductive biases the deep net introduces: does it learn better representations that non-parametric techniques because it has better inductive biases, or does it learn worse representations because the loss being optimized is non-convex? (2) It is difficult to see how the analysis generalizes to more complex data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space, or how it generalizes to deep network architectures that are not pure ReLU networks. For instance, most modern networks use a variant of batch normalization; this already appears to break the presented analyses. (3) The error bound presented in Section 4 appears vacuous for any practical setting, as the upper bound on the error is exponential in the total curvature (a quantity that will be quite large in most practical settings). This is underlined by the analysis of the Swiss roll dataset, of which the authors state that the *bound for this case is very loose*. The fact that the bound is already so loose for this arguably very simple manifold makes that the error analysis may tell us very little about the representational power of deep nets. I would encourage the authors to address issue (1) in the revision of the paper. Issue (2) and (3) may be harder to address, but is essential that they are addressed for the line of work pioneered by this paper to have an impact on our understanding of deep learning. Minor comments: - In prior work, the authors only refer to fully supervised siamese network approaches. These approaches differ from that taken by the authors, as their approach is unsupervised. It should be noted that the authors are not the first to study unsupervised representation learners parametrized by deep networks: other important examples are deep autoencoders (Hinton & Salakhutdinov, 2006 and work on denoising autoencoders from Bengio*s group) and parametric t-SNE (van der Maaten, 2009). - What loss do the authors use in their experiments? Using *the difference between the ground truth distance ... and the distance computed by the network* seems odd, because it encourages the network to produce infinitely large distances (to get a loss of minus infinity). Is the difference squared?  Summary: In this paper, the authors look at the ability of neural networks to represent low dimensional manifolds efficiently e.g. embed them into a lower dimensional Euclidian space. They define a class of manifolds, monotonic chains (affine spaces that intersect, with hyperplanes separating monotonic intervals of spaces) and give a construction to embed such a chain with a neural network with one hidden layer. They also give a bound on the number of parameters required to do so, and examine what happens when the manifold is noisy. Experiments involve looking at embedding synthetic data from a monotonic chain using a distance preservation loss. This experiment supports the theoretical bound on number of parameters needed to embed the monotonic chain. Another experiment varies the elevation and azimuth of of faces, which are known to lie on a monotonic chain, on a regression loss. Comments: The direction of investigation in the paper (looking at what happens to manifolds in a neural network), is very compelling, and I strongly encourage the authors to continue exploring this direction. However, the current version of the paper could use some more work: The experiments are all with a regression loss and a shallow network, and as part of the reason for interest in this question is the very large, high dimensional datasets we use now, which require a deeper network, it seems important to address this case. It also seems important to confirm that embedding works well when *classification* loss is used, instead of regression The theory sections could do with being more clearly written -- I’m not as familiar with the literature in this area, and while the proof method used is relatively elementary, it was difficult to understand what exactly was being proved -- e.g. formally stating what could be expected of an embedding that “accurately and efficiently” preserves a monotonic chain, etc.  SUMMARY This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. PROS Interesting, easy to follow view on some of the capabilities of neural networks, highlighting the dimensionality reduction aspect, and pointing at possible directions for further investigation. CONS The paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less). COMMENTS It would be interesting to study the ramifications of the presented observations for the case of deep(er) networks. Also, to study to what extent the proposed picture describes the totality of functions that are representable by the networks. MINOR COMMENTS - Figure 1 could be referenced first in the text. - ``Color coded** where the color codes what? - Thank you for thinking about revising the points from my first questions. Note: Isometry on the manifold. - On page 5, mention how the orthogonal projection on S_k is realized in the network. - On page 6 ``divided into segments** here `segments* is maybe not the best word. - On page 6 ``The mean relative error is 0.98** what is the baseline here, or what does this number mean?","['Rating:###6: Marginally above acceptance threshold', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###7: Good paper, accept']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",6,3,"Here is the structured peer review:

**Strengths:**

1. The paper presents an interesting and novel approach to representing low-dimensional manifolds in high-dimensional spaces using deep neural networks. The idea of using a monotonic chain as a special type of piecewise linear manifold is well-motivated and well-defined.
2. The authors provide a clear and concise presentation of the problem and the proposed solution, making it easy to follow and understand.
3. The experiments are well-designed and provide a good indication of the effectiveness of the proposed approach.

**Weaknesses:**

1. The paper does not address the learning problem, which is a crucial aspect of deep learning. The authors present a construction that efficiently models a specific type of manifold, but do not discuss how to learn such representations in practice.
2. The analysis of the error bound is limited to a specific type of manifold (monotonic chains) and does not generalize well to more complex data or other types of manifolds.
3. The paper does not discuss the inductive biases of the deep network and how they may affect the learned representations. For example, the use of ReLU activation functions may not be optimal for all types of manifolds.
4. The paper does not provide a clear explanation of the loss function used in the experiments. The use of the difference between the ground truth distance and the distance computed by the network as the loss function is unclear and may not be optimal.
5. The paper could"
Energy-Based Spherical Sparse Coding | OpenReview,"In this paper, we explore an efficient variant of convolutional sparse coding with unit norm code vectors and reconstructions are evaluated using an inner product (cosine distance). To use these codes for discriminative classification, we describe a model we term Energy-Based Spherical Sparse Coding (EB-SSC) in which the hypothesized class label introduces a learned linear bias into the coding step. We evaluate and visualize performance of stacking this encoder to make a deep layered model for image classification.","The paper introduces an efficient variant of sparse coding and uses it as a building block in CNNs for image classification. The coding method incorporates both the input signal reconstruction objective as well as top down information from a class label. The proposed block is evaluated against the recently proposed CReLU activation block. Positives: The proposed method seems technically sound, and it introduces a new way to efficiently train a CNN layer-wise by combining reconstruction and discriminative objectives. Negatives: The performance gain (in terms of classification accuracy) over the previous state-of-the-art is not clear. Using only one dataset (CIFAR-10), the proposed method performs slightly better than the CRelu baseline, but the improvement is quite small (0.5% in the test set). The paper can be strengthened if the authors can demonstrate that the proposed method can be generally applicable to various CNN architectures and datasets with clear and consistent performance gains over strong CNN baselines. Without such results, the practical significance of this work seems unclear.  This paper proposes sparse coding problem with cosine-loss and integrated it as a feed-forward layer in a neural network as an energy based learning approach. The bi-directional extension makes the proximal operator equivalent to a certain non-linearity (CReLu, although unnecessary). The experiments do not show significant improvement against baselines. Pros: - Minimizing the cosine-distance seems useful in many settings where compute inner-product between features are required. - The findings that the bidirectional sparse coding is corresponding to a feed-forward net with CReLu non-linearity. Cons: - Unrolling sparse coding inference as a feed-foward network is not new. - The class-wise encoding makes the algorithm unpractical in multi-class cases, due to the requirement of sparse coding net for each class. - It does not show the proposed method could outperform baseslines in real-world tasks.  First, I*d like to thank the authors for their answers and clarifications. I find, the presentation of the multi-stage version of the model much clearer now. Pros: + The paper states a sparse coding problem using cosine loss, which allows to solve the problem in a single pass. + The energy-based formulation allows bi-directional coding that incorporates top-down and bottom-up information in the feature extraction process. Cons: + The cost of running the evaluation could be large in the multi-class setting, rendering the approach less attractive and the computational cost comparable to recurrent architectures. + While the model is competitive and improves over the baseline, the paper would be more convincing with other comparisons (see text). The experimental evaluation is limited (a single database and a single baseline) ------ The motivation of the sparse coding scheme is to perform inference in a feed forward manner. This property does not hold in the multi stage setting, thus optimization would be required (as clarified by the authors). Having an efficient way of performing a bi-directional coding scheme is very interesting. As the authors clarified, this could not necessarily be the case, as the model needs to be evaluated many times for performing a classification. Maybe an interesting combination would be to run the model without any class-specific bias, and evaluation only the top K predictions with the energy-based setting. Having said this, it would be good to include a discussion (if not direct comparisons) of the trade-offs of using a model as the one proposed by Cao et al. Eg. computational costs, performance. Using the bidirectional coding only on the top layers seems reasonable: one can get a good low level representation in a class agnostic way. This, however could be studied in more detail, for instance showing empirically the trade offs. If I understand correctly, now only one setting is being reported. Finally, the authors mention that one benefit of using the architecture derived from the proposed coding method is the spherical normalization scheme, which can lead to smoother optimization dynamics. Does the baseline (or model) use batch-normalization? If not, seems relevant to test. Minor comments: I find figure 2 (d) confusing. I would not plot this setting as it does not lead to a function (as the authors state in the text).","['Rating:###5: Marginally below acceptance threshold', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",5,4,"Here is the structured peer review:

**Strengths:**

1. The paper introduces a novel approach to sparse coding, which is a long-standing problem in computer vision. The use of cosine loss and bi-directional coding is an interesting twist on traditional sparse coding methods.
2. The authors provide a clear and concise explanation of the proposed method, making it easy to understand for non-experts.
3. The experimental evaluation is well-organized and easy to follow, with clear results and comparisons to baselines.

**Weaknesses:**

1. The performance gain of the proposed method over the baseline is not clear, and the results are limited to a single dataset (CIFAR-10). It would be beneficial to provide more comprehensive results across multiple datasets and architectures.
2. The bi-directional extension of the sparse coding scheme is not necessarily a new idea, and the connection to feed-forward networks with ReLU activation is not entirely clear.
3. The class-wise encoding makes the algorithm impractical in multi-class cases, which is a significant limitation.
4. The computational cost of the proposed method is not explicitly stated, but it may be comparable to or even higher than that of recurrent architectures, which could be a drawback.
5. The experimental evaluation is limited to a single database and a single baseline, which may not be representative of the method's performance in real-world scenarios.

**Rating:** 7
**Confidence:** 3

Note: The rating is based on the overall"
Hadamard Product for Low-rank Bilinear Pooling | OpenReview,"Bilinear models provide rich representations compared with linear models. They have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances taking advantage of the expanded representations. However, bilinear representations tend to be high-dimensional, limiting the applicability to computationally complex tasks. We propose low-rank bilinear pooling using Hadamard product for an efficient attention mechanism of multimodal learning. We show that our model outperforms compact bilinear pooling in visual question-answering tasks with the state-of-the-art results on the VQA dataset, having a better parsimonious property.","Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built. Strengths: 1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. 2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below). 3. The various design choices made in model development have been experimentally verified. Weaknesses/Suggestions: 1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA). 2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? 3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art. 4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper. 5. In the caption for Table 1, fix the following: “have not” -> “have no” Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling.  Results on the VQA task are good for this simple model, the ablation study of table 1 gives some insights as to what is important. Missing are some explanations about the language embedding and the importance in deciding embedding dimension and final output dimension, equivalent to deciding the projected dimension in the compact bilinear model. Since the main contribution of the paper seems to be slightly better performance with fairly large reduction in parameters vs. compact bilinear something should be said about choice of those hyper parameters. If you increase embedded and output dimensions to equalize parameters to the compact bilinear model are further gains possible? How is the question encoded? Is word order preserved in this encoding, the compact bilinear model compared to in table 1 mentions glove, the proposed model is using this as well? The meaning of visual attention in this model along with the number of glimpses should be tied to the sentence embedding, so now we are looking at particular spatial components when that part of the sentence is encoded, then we stack according to your equation 9?  This work proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product (element-wise product). This formulation is evaluated on the visual question answering (VQA) task together with several other model variants. Strength: 1. The paper discusses how the Hadamard product can be used to approximate the full outer product. 2. The paper provides an extensive experimental evaluation of other model aspect for VQA. 3. The full model archives a slight improvement over prior state-of-the-art on the challenging and large scale VQA challenge. Weaknesses: 1. Novelty: The paper presents only a new “interpretation” of the Hadamard product which has previously been widely used for pooling, including for VQA. 2. Experimental evaluation: 2.1. An experimental direct comparison with MCB missing. Although the evaluated model is similar to Fukui et al. several other changes have been made, including question encoding (GRU vs. LSTM), normalization (tanh vs. L2 vs. none). The small difference in performance (0.44% om Table 1) could easily be attributed to these differences. 2.2. An experimental comparison to the full outer product (e.g. for a lower dimension) is missing. It remains unclear how good the proposed approximation for the full outer product is. While a comparison to MCB is presented this seems insufficient as MCB is a very different model. 2.3. One of the most important hyper parameters for the Hadamard Product seems to be the dimension of the lower dimensional embedding d. What effect does changing this have? 2.4. Comparison with other pooling strategies, e.g. elementwise sum instead of elementwise product. 3. No theoretical analysis or properties of the approximation are presented. 4. The paper seems to be general at the beginning, but the claim of the benefit of the Hadamard product is only shown experimentally on the VQA dataset. 5. Related work: The comparison to the related works in the appendix should at least be mentioned in the main paper, even if the details are the supplemental. Minor - It is not clear why the Lu et al, 2015 is cited rather than the published paper from Antol et al. - Sect 2, first sentence: “every pairs” -> “every pair” Summary: While the paper provides a new best performance and an interesting interpretation of Hadamard product, to be a strong paper, either a more theoretical analysis of the properties of this approximation is required or a corresponding experimental evaluation. It is a bit unfortunate that most of the experimental evaluation is not about the main claim of the paper (the Hadamard product) but of unrelated aspects which are important to achieve high performance in the VQA challenge. To be more convincing I would like to see the following experiments - Comparison with Outer product in the identical model - Comparison with MCB in the identical model - Comparison with elementwise sum instead of elementwise product - One of the most important hyper parameters for the Hadamard Product seems to be the dimension of the lower dimensional embedding d. What effect does changing this have?","['Rating:###7: Good paper, accept', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",7,3,"Here is the structured peer review:

**Strengths:**

1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work.
2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%.
3. The various design choices made in model development have been experimentally verified.

**Weaknesses:**

1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse.
2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling, but it is not clear how does the reduction in number of parameters help experimentally?
3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%, but I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.
4. I would like the authors to explicitly mention the differences between MRN, MARN, and MLB.
5. In the caption for Table 1, fix the following: “have not” -> “have no”.
6. The paper seems to be general at the beginning, but the claim of the benefit"
Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer | OpenReview,"The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.","This paper proposes a method for significantly increasing the number of parameters in a single layer while keeping computation in par with (or even less than) current SOTA models. The idea is based on using a large mixture of experts (MoE) (i.e. small networks), where only a few of them are adaptively activated via a gating network. While the idea seems intuitive, the main novelty in the paper is in designing the gating network which is encouraged to achieve two objectives: utilizing all available experts (aka importance), and distributing computation fairly across them (aka load). Additionally, the paper introduces two techniques for increasing the batch-size passed to each expert, and hence maximizing parallelization in GPUs. Experiments applying the proposed approach on RNNs in language modelling task show that it can beat SOTA results with significantly less computation, which is a result of selectively using much more parameters. Results on machine translation show that a model with more than 30x number of parameters can beat SOTA while incurring half of the effective computation. I have the several comments on the paper: - I believe that the authors can do a better job in their presentation. The paper currently is at 11 pages (which is too long in my opinion), but I find that Section 3.2 (the crux of the paper) needs better motivation and intuitive explanation. For example, equation 8 deserves more description than currently devoted to it. Additional space can be easily regained by moving details in the experiments section (e.g. architecture and training details) to the appendix for the curious readers. Experiment section can be better organized by finishing on experiment completely before moving to the other one. There are also some glitches in the writing, e.g. the end of Section 3.1. - The paper is missing some important references in conditional computation (e.g. https://arxiv.org/pdf/1308.3432.pdf) which deal with very similar issues in deep learning. - One very important lesson from the conditional computation literature is that while we can in theory incur much less computation, in practice (especially with the current GPU architectures) the actual time does not match the theory. This can be due to inefficient branching in GPUs. It would be nice if the paper includes a discussion of how their model (and perhaps implementation) deal with this problem, and why it scales well in practice. - Table 1 and Table 3 contain repetitive information, and I think they should be combined in one (maybe moving Table 3 to appendix). One thing I do not understand is how does the number of ops/timestep relate to the training time. This also related to the pervious comment.  Paper Strengths: -- Elegant use of MoE for expanding model capacity and enabling training large models necessary for exploiting very large datasets in a computationally feasible manner -- The effective batch size for training the MoE drastically increased also -- Interesting experimental results on the effects of increasing the number of MoEs, which is expected. Paper Weaknesses: --- there are many different ways of increasing model capacity to enable the exploitation of very large datasets; it would be very nice to discuss the use of MoE and other alternatives in terms of computational efficiency and other factors.  This paper describes a method for greatly expanding network model size (in terms of number of stored parameters) in the context of a recurrent net, by applying a Mixture of Experts between recurrent net layers that is shared between all time steps. By process features from all timesteps at the same time, the effective batch size to the MoE is increased by a factor of the number of steps in the model; thus even for sparsely assigned experts, each expert can be used on a large enough sub-batch of inputs to remain computationally efficient. Another second technique that redistributes elements within a distributed model is also described, further increasing per-expert batch sizes. Experiments are performed on language modeling and machine translation tasks, showing significant gains by increasing the number of experts, compared to both SoA as well as explicitly computationally-matched baseline systems. An area that falls a bit short is in presenting plots or statistics on the real computational load and system behavior. While two loss terms were employed to balance the use of experts, these are not explored in the experiments section. It would have been nice to see the effects of these more, along with the effects of increasing effective batch sizes, e.g. measurements of the losses over the course of training, compared to the counts/histogram distributions of per-expert batch sizes. Overall I think this is a well-described system that achieves good results, using a nifty placement for the MoE that can overcome what otherwise might be a disadvantage for sparse computation. Small comment: I like Fig 3, but it*s not entirely clear whether datapoints coincide between left and right plots. The H-H line has 3 points on left but 5 on the right? Also would be nice if the colors matched between corresponding lines.","['Rating:###7: Good paper, accept', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",7,4,"Here is the structured peer review:

**Strengths:**

1. The paper proposes a novel approach to increasing model capacity in recurrent neural networks by using a Mixture of Experts (MoE) layer, which is a clever way to selectively utilize a large number of parameters while keeping computation efficient.
2. The authors provide a clear and well-organized presentation of the MoE layer and its application to language modeling and machine translation tasks, making it easy to follow their approach.
3. The experimental results show significant gains in model capacity and performance, which is a testament to the effectiveness of the proposed method.
4. The authors provide a good discussion on the benefits of using MoE, including the ability to increase effective batch size and reduce computational cost.

**Weaknesses:**

1. The paper could benefit from a more detailed explanation of the gating network and its objectives, as well as a clearer motivation for using MoE in the context of conditional computation.
2. The paper lacks a discussion on the limitations of the proposed method, such as the potential for inefficient branching in GPUs, which could impact the actual training time.
3. The presentation is a bit lengthy, and some sections could be condensed or moved to the appendix for curious readers.
4. The paper could benefit from more references to the existing literature on conditional computation, which could provide a more comprehensive understanding of the topic.
5. The analysis of the computational load and system behavior could be more detailed, including plots or statistics to support the"
Support Regularized Sparse Coding and Its Fast Encoder | OpenReview,"Sparse coding represents a signal by a linear combination of only a few atoms of a learned over-complete dictionary. While sparse coding exhibits compelling performance for various machine learning tasks, the process of obtaining sparse code with fixed dictionary is independent for each data point without considering the geometric information and manifold structure of the entire data. We propose Support Regularized Sparse Coding (SRSC) which produces sparse codes that account for the manifold structure of the data by encouraging nearby data in the manifold to choose similar dictionary atoms. In this way, the obtained support regularized sparse codes capture the locally linear structure of the data manifold and enjoy robustness to data noise. We present the optimization algorithm of SRSC with theoretical guarantee for the optimization over the sparse codes. We also propose a feed-forward neural network termed Deep Support Regularized Sparse Coding (Deep-SRSC) as a fast encoder to approximate the sparse codes generated by SRSC. Extensive experimental results demonstrate the effectiveness of SRSC and Deep-SRSC.","The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding. Namely, two data samples that are similar or neighbours, should have a sparse code that is similar (in terms of support). The general idea is not unique, but it is an interesting one (if one admits that the adjacency matrix A is known a priori), and the novelty mostly lies on the definition of the regularisation term that is an l1-norm (while other techniques would mostly use l2 regularisation). Based on this idea, the authors develop a new SRSC algorithm, which is analysed in detail and shown to perform better than its competitors based on l2 sparse coding regularisation and other schemes in terms of clustering performance. Inspired by LISTA, the authors then propose an approximate solution to the SRSC problem, called Deep-SRSC, that acts as a sort of fast encoder. Here too, the idea is interesting and seems to be quite efficient from experiments on USPS data, even if the framework seems to be strongly inspired from LISTA. That scheme should however be better motivated, by the limitations of SRSC that should be presented more clearly. Overall, the paper is well written, and pretty complete. It is not extremely original in its main ideas though, but the actual algorithm and implementation seem new and effective.  In this paper the authors propose a method to explicitly regularize sparse coding to encode neighbouring datapoints with similar sets of atoms from the dictionary by clustering training examples with KNN in input space. The resulting algorithm is relatively complex and computationally relatively expensive, but the authors provide detailed derivations and use arguments from proximal gradient descent methods to prove convergence (I did not follow all the derivations, only some). In general the paper is well written and the authors explain the motivation behind the algorithms design in detail. In the abstract the authors mention “extensive experimental results …”, but I find the experiments not very convincing: With experiments on the USPS handwritten digits dataset (why not MNIST?), COIL-20 and COIL-100 and UCI, the datasets are all relatively small and the algorithm is run with dictionary sizes between p=100 to p=500. This seems surprising because the authors state that they implemented SRSC in “CUDA C++ with extreme efficiency” (page 10). But more importantly, I find it hard to interpret and compare the results: The paper reports accuracy and and normalized mutual information for a image retrieval / clustering task where the proposed SRSC is used as a feature extractor. The improvements relative to standard Sparse Coding seem very small (often < 1% in terms of NMI; it looks more promising in terms of accuracy) and if I understand the description on page 11 correctly, than the test set was used to select some hyperparameters (the best similarity measure for clustering step)? There is no comparisons to other baselines / state of the art image clustering methods. Besides of providing features for a small scale image clustering system, are there maybe ways to more directly evaluate the properties and qualities of a sparse coding approach? E.g. reconstruction error / sparsity; maybe even denoising performance? In summary, I think in it current form the paper lacks the evaluation and experimental results for an ICLR publication. Intuitively, I agree with the authors that the proposed regularization is an interesting direction, but I don’t see experiments that directly show that the regularization has the desired effect; and the improvements in the clustering task where SRSC is used as a feature extractor are very modest.  I*d like to thank the authors for their detailed response to my questions. The paper proposes a support regularized version of sparse coding that takes into account the underlying manifold structure of the data. For this purpose, the authors augment the classic sparse coding loss with a term that encourages near by points to have similar active set. Convergence guarantees for the optimization procedure are presented. Experimental evaluation on clustering and semi-supervised learning shows the benefits of the proposed approach. The paper is well written and a nice read. The most relevant contribution of this work is to including (and optimizing) the regularization function, and not an approximation or surrogate. The authors derive a a PGD-styple iterative method and present convergence analysis for it. Thanks for the clarifications regarding the assumptions used in Section 3. It would be nice to include some of that in the manuscript. The authors also propose a fast encoding scheme for their proposed method. The authors included a new experiment in semi-supervised consists of a very interesting use (of the method and the fast approximation). While this is an interesting addition, I think that using fast encoders is not particularly novel or the main part of the work. *Converting* iterative optimization algorithms into feed-forward nets for accelerating the inference process has been done in the past (several times with quite similar problems). Is natural that this can be done, and not very surprising. Maybe would be interesting to evaluate how important is to have an architecture matching the optimization algorithm, compared to a generic network (though some of this analysis has also been performed in the past).","['Rating:###7: Good paper, accept', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",7,4,"Here is the structured peer review:

**Strengths:**

1. The paper proposes a novel approach to sparse coding that incorporates the manifold structure of the data, which is an interesting direction.
2. The authors provide a detailed analysis of the optimization algorithm and convergence guarantees, which is impressive.
3. The paper is well-written and easy to follow, with clear explanations of the concepts and methods.
4. The authors provide a fast encoding scheme, which is a useful contribution.
5. The experimental results show that the proposed approach can improve clustering performance, which is a significant achievement.

**Weaknesses:**

1. The paper lacks a clear motivation for why the manifold structure of the data is important, and how it is incorporated into the sparse coding framework.
2. The experimental results are limited to small datasets (USPS, COIL-20, and COIL-100) and do not provide a comprehensive evaluation of the approach.
3. The paper does not provide a clear comparison to other state-of-the-art methods in sparse coding and clustering.
4. The regularization term used in the paper is not clearly justified, and its effectiveness is not thoroughly evaluated.
5. The fast encoding scheme is not particularly novel, as it is based on converting iterative optimization algorithms into feed-forward nets, which has been done before.

**Rating:** 7/10
**Confidence:** 3/5

Note: The rating is based on the overall quality of the paper, and the confidence score reflects"
Learning to Navigate in Complex Environments | OpenReview,"Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks to bootstrap learning. In particular we consider jointly learning the goal-driven reinforcement learning problem with an unsupervised depth prediction task and a self-supervised loop closure classification task. Using this approach we can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour, its ability to localise, and its network activity dynamics, that show that the agent implicitly learns key navigation abilities, with only sparse rewards and without direct supervision.","This paper shows that a deep RL approach augmented with auxiliary tasks improves performance on navigation in complex environments. Specifically, A3C is used for the RL problem, and the agent is simultaneously trained on an unsupervised depth prediction task and a self-supervised loop closure classification task. While the use of auxiliary tasks to improve training of models including RL agents is not new, the main contribution here is the use of tasks that encourage learning an intrinsic representation of space and movement that enables significant improvements on maze navigation tasks. The paper is well written, experiments are convincing, and the value of the auxiliary tasks for the problem are clear. However, the contribution is relatively incremental given previous work on RL for navigation and on auxiliary tasks. The work could become of greater interest provided broader analysis and insights on either optimal combinations of tasks for visual navigation (e.g. the value of other visual / geometry-based tasks), or on auxiliary tasks with RL in general. As it is, it is a useful demonstration of the benefit of geometry-based auxiliary tasks for navigation, but of relatively narrow interest.  I do like the demonstration that including learning of auxiliary tasks does not interfere with the RL tasks but even helps. This is also not so surprising with deep networks. The deep structure of the model allows the model to learn first a good representation of the world on which it can base its solutions for specific goals. While even early representations do of course depend on the task performance itself, it is clear that there are common first stages in sensory representations like the need for edge detection etc. Thus, training by additional tasks will at least increase the effective training size. It is of course unclear how to adjust for this to make a fair comparison, but the paper could have included some more insights such as the change in representation with and without auxiliary training. I still strongly disagree with the implied definition of supervised or even self-supervised learning. The definition of unsupervised is learning without external labels. It does not matter if this comes from a human or for example from an expensive machine that is used to train a network so that a task can be solved later without this expensive machine. I would call EM a self-supervised method where labels are predicted from the model itself and used to bootstrap parameter learning. In this case you are using externally supplied labels, which is clearly a supervised learning task!  This relatively novel work proposes to augment current RL models by adding self-supervised tasks encouraging better internal representations. The proposed tasks are depth prediction and loop closure detection. While these tasks assume a 3D environment as well some position information, such priors are well suited to a large variety of tasks pertaining to navigation and robotics. Extensive experiments suggest to incorporating such auxiliary tasks increase performance and to a large extent learning speed. Additional analysis of value functions and internal representations suggest that some structure is being discovered by the model, which would not be without the auxiliary tasks. While specific to 3D-environment tasks, this work provides additional proof that using input data in addition to sparse external reward signals helps to boost learning speed as well as learning better internal representation. It is original, clearly presented, and strongly supported by empirical evidence. One small downside of the experimental method (or maybe just the results shown) is that by picking top-5 runs, it is hard to judge whether such a model is better suited to the particular hyperparameter range that was chosen, or is simply more robust to these hyperparameter settings. Maybe an analysis of performance as a function of hyperparameters would help confirm the superiority of the approach to the baselines. My own suspicion is that adding auxiliary tasks would make the model robust to bad hyperparameters. Another downside is that the authors dismiss navigation literature as *not RL*. I sympathize with the limit on the number of things that can fit in a paper, but some experimental comparison with such literature may have proven insightful, if just in measuring the quality of the learned representations.","['Rating:###7: Good paper, accept', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###7: Good paper, accept']","['Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",7,5,"Here is the structured peer review:

**Strengths:**

1. The paper presents a novel approach to improving the performance of reinforcement learning (RL) agents in complex environments by incorporating auxiliary tasks. The use of depth prediction and loop closure detection tasks is particularly interesting and demonstrates the potential of geometry-based auxiliary tasks for navigation.
2. The experimental results show significant improvements in performance and learning speed, which is a key contribution of the paper.
3. The authors provide a clear and concise presentation of their approach and results, making it easy to follow and understand.
4. The paper highlights the importance of learning internal representations in RL agents, which is a crucial aspect of navigation tasks.

**Weaknesses:**

1. The paper's contribution may be relatively incremental, as previous work on RL for navigation and auxiliary tasks has already demonstrated the benefits of incorporating additional tasks. A more detailed comparison with existing literature would strengthen the paper's impact.
2. The authors' definition of supervised and self-supervised learning is not entirely accurate, as they imply that unsupervised learning involves external labels. This definition may cause confusion among readers.
3. The experimental method may not be robust to hyperparameter settings, as the authors only show results for a specific hyperparameter range. An analysis of performance as a function of hyperparameters would provide more insight into the robustness of the approach.
4. The paper dismisses the navigation literature as not being RL, which may be an oversimplification. A comparison with existing literature"
A Simple but Tough-to-Beat Baseline for Sentence Embeddings | OpenReview,"The success of neural network methods for computing word embeddings has motivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs. Surprisingly, Wieting et al (ICLR*16) showed that such complicated methods are outperformed, especially in out-of-domain (transfer learning) settings, by simpler methods involving mild retraining of word embeddings and basic linear regression. The method of Wieting et al. requires retraining with a substantial labeled dataset such as Paraphrase Database (Ganitkevitch et al., 2013). The current paper goes further, showing that the following completely unsupervised sentence embedding is a formidable baseline: Use word embeddings computed using one of the popular methods on unlabeled corpus like Wikipedia, represent the sentence by a weighted average of the word vectors, and then modify them a bit using PCA/SVD. This weighting improves performance by about 10% to 30% in textual similarity tasks, and beats sophisticated supervised methods including RNN*s and LSTM*s. It even improves Wieting et al.*s embeddings. This simple method should be used as the baseline to beat in future, especially when labeled training data is scarce or nonexistent. The paper also gives a theoretical explanation of the success of the above unsupervised method using a latent variable generative model for sentences, which is a simple extension of the model in Arora et al. (TACL*16) with new *smoothing* terms that allow for words occurring out of context, as well as high probabilities for words like and, not in all contexts.","This is a good paper with an interesting probabilistic motivation for weighted bag of words models. The (hopefully soon) added comparison to Wang and Manning will make it stronger. Though it is sad that for sufficiently large datasets, NB-SVM still works better. In the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f. Minor comments: *The capturing the similarities* -- typo in line 2 of intro. *Recently, (Wieting et al.,2016) learned* -- use citet instead of parenthesized citation  This paper proposes a simple way to reweight the word embedding in the simple composition function for sentence representation. This paper also shows the connection between this new weighting scheme and some previous work. Here are some comments on technical details: - The word *discourse* is confusing. I am not sure whether the words *discourse* in *discourse vector c_s* and the one in *most frequent discourse* have the same meaning. - Is there any justification about related to syntac? - Not sure what thie line means: *In fact the new model was discovered by our detecting the common component c0 in existing embeddings.* in section *Computing the sentence embedding* - Is there any explanation about the results on sentiment in Table 2?  This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too. Overall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?","['Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept', 'Rating:###8: Top 50% of accepted papers, clear accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",7,4,"Here is the structured peer review:

**Strengths:**

1. The paper presents a simple yet effective method for generating sentence embeddings, which is a significant contribution to the field of natural language processing.
2. The theoretical explanation of the method's success using a latent variable generative model is interesting and provides a principled understanding of the weighting scheme.
3. The experiments demonstrate that the method outperforms sophisticated supervised methods, including RNNs and LSTMs, on textual similarity and entailment tasks.
4. The paper highlights the importance of using a suitable baseline for comparison in NLP tasks, which is a valuable takeaway for the field.

**Weaknesses:**

1. The paper relies on a substantial amount of unlabeled data (Wikipedia) to train the word embeddings, which may not be feasible for all research settings.
2. The method's performance on sentiment analysis is not thoroughly explained, and the results in Table 2 are not clearly justified.
3. The paper could benefit from more discussion on why the simple weighted average method outperforms more complex models like LSTMs, which may be due to artifacts of the specific benchmarks used.
4. The explanation of the ""discourse"" vector in Section 3 is unclear, and more clarification is needed to ensure consistency with existing work.

**Rating:** 8/10
**Confidence:** 4/5

Note: The rating is based on the paper's technical quality, relevance, and overall impact,"
Inverse Problems in Computer Vision using Adversarial Imagination Priors | OpenReview,"Given an image, humans effortlessly run the image formation process backwards in their minds: they can tell albedo from shading, foreground from background, and imagine the occluded parts of the scene behind foreground objects. In this work, we propose a weakly supervised inversion machine trained to generate similar imaginations that when rendered using differentiable, graphics-like decoders, produce the original visual input. We constrain the imagination spaces by providing exemplar memory repositories in the form of foreground segmented objects, albedo, shading, background scenes and imposing adversarial losses on the imagination spaces. Our model learns to perform such inversion with weak supervision, without ever having seen paired annotated data, that is, without having seen the image paired with the corresponding ground-truth imaginations. We demonstrate our method by applying it to three Computer Vision tasks: image in-painting, intrinsic decomposition and object segmentation, each task having its own differentiable renderer. Data driven adversarial imagination priors effectively guide inversion, minimize the need for hand designed priors of smoothness or good continuation, or the need for paired annotated data.","In this work, the authors propose to use a (perhaps deterministic) retrieval function to replace uniform sampling over the train data in training the discriminator of a GAN. Although I like the basic idea, the experiments are very weak. There are essentially no quantitative results, no real baselines, and only a small amount of not especially convincing qualititative results. It is honestly hard to review the paper- there isn*t any semblance of normal experimental validation. Note: what is happening with the curves in fig. 6?  This paper proposes a model that generates a latent representation of input image(s) and optimizes a reconstruction loss with an adversarial loss (Eq (1)) over nearest neighbors from a bank of images (“memory”). The framework is adapted to three tasks: (i) image in-painting, (ii) intrinsic image decomposition, (iii) figure-ground layer extraction. Qualitative results are shown for all three tasks. I think the proposed model has potential merits. I particularly like the fact that it seems to be reasoning over image composites via matching against a bank of images (somewhat similar to “Segmenting Scenes by Matching Image Composites” work in NIPS 2009). However, I won’t champion the paper as the overall clarity and evaluation could be improved. More detailed comments: I believe the fatal flaw of the paper is there is no quantitative evaluation of the approach. At the very least, there should be a comparison against prior work on intrinsic image decomposition (e.g., SIRFS, maybe benchmark on *intrinsic images in the wild” dataset). I found the writing vague and confusing throughout. For instance, “memory database” could mean a number of things, and in the end it seems that it’s simply a set of images. “Imagination” is also vague. On page 4, R(M,x) has the database and input image as arguments, but Fig 2 doesn’t show the input image as an input to R. The contributions listed on page 3 should be tightened (e.g., it’s not clear what “Relevant memory retrieval for informative adversarial priors” means). Fig 3 seems inconsistent with Fig 2 as the module for “memory database” is not present. The fully-convolutional discriminator could use more details; one possibility is to provide a cost function.  The paper describes a network architecture for inverse problems in computer vision. Example inverse problems considered are image inpainting, computing intrinsic image decomposition and foreground/background separation. The architecture is composed of (i) a generator that produces target (latent) output (such as foreground / background regions), (ii) renderer that composes that latent output back to the image that can be compared with the input to measure reconstruction error, and (iii) adversarial prior that ensures the target output (latent) image respects a certain image statistics. Strong points. - The proposed architecture with memory database is interesting and appears to be novel. Weak points: - Experimental results are only proof-of-concept in toy set-ups and do not clearly demonstrate benefits of the proposed architecture. - It is unclear whether the memory retrieval engine that retrieves images based on L2 distance on pixel values is going generalize to other more realistic scenarios. - Clarity. The clarity of explanation can be also improved (see below). Detailed evaluation. Originality: - The novelty of this work lies in the (iii) adversarial prior that places an adversarial loss between the generated latent output and a single image retrieved from a large unlabelled database of target output examples (called memory). The adversarial prior has a convolutional form matching local image statistics, rather than the entire image. The particular form of network architecture with the memory-based fully convolutional adversarial loss appears to be novel and potentially interesting. - Motivation for the Architecture. The weakest point of the proposed architecture is the *Memory retrieval engine* R (section 2.4), where images are retrieved from the memory by measuring L2 distance on pixel intensities. While this maybe ok for simple problems considered in this work, it is unclear how this can generalize to other more complicated datasets and problems. This should be better discussed, better justified and ideally results in some more realistic set-up shown (see below). Quality: - Experiments. Results are shown for inpainting of MNIST digits, intrinsic image decomposition on the MIT intrinsic image database, and figure/ground layer extraction on the synthesized dataset of 3D chairs rendered onto background from real photographs. The experimental validation of the model is not very strong and proof-of-concept only. All the experiments are performed in simplified toy set-ups. The MNIST digit inpainting is far from current state-of-the-art on image inpainting in real photographs (see e.g. Pathak et al., 2016). The foreground background separation is done on only synthetically generated test data. Even for intrinsic image demposition problem there is now relatively large-scale dataset of (Bell et al., 2014), see the citation below. While this is probably ok for the ICLR paper, it diminishes the significance of the work. Is this model going to be useful in a real settings? One possibility to address this would be to focus on one of the problems and show results on a challenging state-of-the-art data. It would be great to see the benefits of the memory database. S. Bell, K. Bala, and N. Snavely. Intrinsic images in the wild. ACM Transactions on Graphics, 33(4):159, 2014. Clarity: - The clarity of the writing can be improved. I found some of the terminology of the paper, specially the “imagination” and “memory” confusing. From figure 2, it is not clear how the “memories” for the given input image are obtained, which also took me some time to understand. - To help understand the proposed architecture, it would be useful to draw an illustration of what is happening in the *feature space”, similar in spirit e.g. to figure 2 in https://arxiv.org/pdf/1612.02136.pdf. Specially, it would be interesting to understand the role of the memory database in this way. Significance: - The paper describes potentially interesting architecture. Given the only proof-of-concept results in toy set-ups, the significance, in the current version, appears only limited. Rather than addressing many different problems, it would be interesting to see benefits of the proposed architecture on realistic challenging data for one of the problems. Overall: - The proposed architecture seems novel and potentially interesting, but experiments are only proof-of-concept and clarity can be improved. It is unclear whether the memory matching engine will generalize to other more complicated datasets and problems. Overall, I am on the edge with this paper, giving the authors the benefit of doubt with a score slightly above the threshold.","['Rating:###3: Clear rejection', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",3,4,"Here is the structured peer review:

**Strengths:**

1. The proposed architecture with memory database is interesting and appears to be novel.
2. The idea of using an adversarial prior to ensure the target output respects a certain image statistics is novel and potentially interesting.
3. The motivation for the architecture is clear, and the authors provide a good explanation of the proposed method.

**Weaknesses:**

1. Experimental results are only proof-of-concept and do not clearly demonstrate the benefits of the proposed architecture.
2. The clarity of the writing can be improved, with some terminology (e.g., ""imagination"" and ""memory"") being confusing.
3. The memory retrieval engine is unclear, and it is unclear whether it will generalize to other more complicated datasets and problems.
4. The experiments are performed in simplified toy set-ups, which diminishes the significance of the work.
5. The paper does not provide a clear comparison to prior work on intrinsic image decomposition and other related problems.

**Rating:** 6
**Confidence:** 3

Note: The rating is slightly above the threshold, but the confidence score is only 3, indicating that the reviewer is not entirely convinced by the paper. The reviewer's concerns about the experimental results, clarity, and generalizability of the proposed architecture are significant, and the paper could benefit from further work to address these issues."
Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context | OpenReview,"Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense word embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields com- parable performance to a state of the art monolingual model trained on five times more training data.","This paper discusses multi-sense embedddings and proposes learning those by using aligned text across languages. Further, the paper suggests that adding more languages helps improve word sense disambiguation (as some ambiguities can be carried across language pairs). While this idea in itself isn*t new, the authors propose a particular setup for learning multi-sense embeddings by exploiting multilingual data. Broadly this is fine, but unfortunately the paper then falls short in a number of ways. For one, the model section is unnecessarily convoluted for what is a nice idea that could be described in a far more concise fashion. Next (and more importantly), comparison with other work is lacking to such an extent that it is impossible to evaluate the merits of the proposed model in an objective fashion. This paper could be a lot stronger if the learned embeddings were evaluated in downstream tasks and evaluated against other published methods. In the current version there is too little of this, leaving us with mostly relative results between model variants and t-SNE plots that don*t really add anything to the story.  this work aims to address representation of multi-sense words by exploiting multilingual context. Experiments on word sense induction and word similarity in context show that the proposed solution improves over the baseline. From a computational linguistics perspective, the fact that languages less similar to English help more is intriguing. I see following problem with this work: - the paper is hard to follow and hard to see what*s new compared to the baseline model [1]. A paragraph of discussion should clearly compare and contrast with that work. - the proposed model is a slight variation of the previous work [1] thus the experimental setup should be designed in a way so that we compare which part helps improvement and how much. thus MONO has not been exposed the same training data and we can*t be sure that the proposed model is better because MONO does not observe the data or lacks the computational power. I suggest following baseline: turning multilingual data to monolingual one using the alignment, then train the baseline model[1] on this pseudo monolingual data. - the paper provides good benchmarks for intrinsic evaluation but the message could be conveyed more strongly if we see improvement in a downstream task. [1] https://arxiv.org/pdf/1502.07257v2.pdf  In this paper, the authors propose a Bayesian variant of the skipgram model to learn word embeddings. There are two important variant compared to the original model. First, aligned sentences from multiple languages are used to train the model. Therefore, the context words of a given target word can be either from the same sentence, or from an aligned sentence in a different language. This allows to learn multilingual embedding. The second difference is that each word is represented by multiple vectors, one for each of its different senses. A latent variable z models which sense should be used, given the context. Overall, I believe that the idea of using a probabilistic model to capture polysemy is an interesting idea. The model introduced in this paper is a nice generalization of the skipgram model in that direction. However, I found the paper a bit hard to follow. The formulation might probably be simplified (e.g. why not consider a target word w and a context c, where c is either in the source or target language. Since all factors are independent, this should not change the model much, and would make the presentation easier). The performance of all models reported in Table 2 & 3 seem pretty low. Overall, I like the main idea of the paper, which is to represent word senses by latent variables in a probabilistic model. I feel that the method could be presented more clearly, which would make the paper much stronger. I also have some concerns regarding the experimental results. Pros: Interesting extension of skipgram to capture polysemy. Cons: The paper is not clearly written. Results reported in the paper seems pretty low.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###4: Ok but not good enough - rejection', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",4,3,"Here is the structured peer review:

**Strengths:**

1. The paper presents an interesting idea of using multilingual context to learn multi-sense word embeddings, which is a significant contribution to the field of natural language processing.
2. The authors propose a Bayesian variant of the skipgram model, which is a nice generalization of the original model.
3. The idea of using a probabilistic model to capture polysemy is an interesting direction, and the authors demonstrate its potential in learning multi-sense word embeddings.

**Weaknesses:**

1. The paper is difficult to follow, with a convoluted model section that could be simplified and clarified.
2. The comparison with other work is lacking, making it impossible to evaluate the merits of the proposed model in an objective fashion.
3. The experimental results are not clearly presented, with low performance metrics reported in Tables 2 and 3.
4. The paper could benefit from more detailed discussion of the limitations of the proposed model and its potential applications.
5. The experimental setup is not well-designed, with the baseline model not being exposed to the same training data as the proposed model, making it difficult to compare the two.

**Rating:** 6
**Confidence:** 3

Note: The rating is based on the overall quality of the paper, with 10 being the highest. The confidence score reflects the reviewer's confidence in the paper's conclusions, with 5 being the highest."
Deep Convolutional Neural Network Design Patterns | OpenReview,"Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications. Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet). Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files are available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work.","The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. Essentially, it reads like a review paper about modern CNN architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100, but seem to achieve relatively poor performance on these datasets (Table 1), so their merit is unclear to me. I*m not sure if such a collection of rules extracted from prior work warrants publication as a research paper. It is not a bad idea to try and summarise some of these observations now that CNNs have been the model of choice for computer vision tasks for a few years, and such a summary could be useful for newcomers. However, a lot of it seems to boil down to common sense (e.g. #1, #3, #7, #11). The rest of it might be more suited for an *introduction to training CNNs* course / blog post. It also seems to be a bit skewed towards recent work that was fairly incremental (e.g. a lot of attention is given to the flurry of ResNet variants). The paper states that *it is universal in all convolutional neural networks that the activations are downsampled and the number of channels increased from the input to the final layer*, which is wrong. We already discussed this previously re: my question about design pattern 5, but I think the answer that was given (*the nature of design patterns is that they only apply some of the time*) does not excuse making such sweeping claims. This should probably be removed. *We feel that normalization puts all the layer*s input samples on more equal footing, which allows backprop to train more effectively* (section 3.2, 2nd paragraph) is very vague language that has many possible interpretations and should probably be clarified. It also seems odd to start this sentence with *we feel*, as this doesn*t seem like the kind of thing one should have an opinion about. Such claims should be corroborated by experiments and measurements. There are several other instances of this issue across the paper. The connection between Taylor series and the proposed Taylor Series Networks seems very tenuous and I don*t think the name is appropriate. The resulting function is not even a polynomial as all the terms represent different functions -- f(x) + g(x)**2 + h(x)**3 + ... is not a particularly interesting object, it is just a nonlinear function of x. Overall, the paper reads like a collection of thoughts and ideas that are not very well delineated, and the experimental results are unconvincing.  The authors have grouped recent work in convolutional neural network design (specifically with respect to image classification) to identify core design principles guiding the field at large. The 14 principles they produce (along with associated references) include a number of useful and correct observations that would be an asset to anyone unfamiliar with the field. The authors explore a number of architectures on CIFAR-10 and CIFAR-100 guided by these principles. The authors have collected a quality set of references on the subject and grouped them well which is valuable for young researchers. Clearly the authors explored a many of architectural changes as part of their experiments and publicly available code base is always nice. Overall the writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion. For example, *Design Pattern 4: Increase Symmetry argues for architectural symmetry as a sign of beauty and quality* is presented as one of 14 core design principles without any further justification. Similarly *Design Pattern 6: Over-train includes any training method where the network is trained on a harder problem than necessary to improve generalization performance of inference* is presented in the middle of a paragraph with no supporting references or further explanation. The experimental portion of this paper feels scattered with many different approaches being presented based on subsets of the design principles. In general, these approaches either are minor modifications of existing networks (different FractalNet pooling strategies) or are novel architectures that do not perform well. The exception being the Fractal-of-Fractal network which achieves slightly improved accuracy but also introduces many more network parameters (increased capacity) over the original FractalNet. Preliminary rating: It is a useful and perhaps noble task to collect and distill research from many sources to find patterns (and perhaps gaps) in the state of a field; however, some of the patterns presented do not seem well developed and include principles that are poorly explained. Furthermore, the innovative architectures motivated by the design principles either fall short or achieve slightly better accuracy by introducing many more parameters (Fractal-Of-Fractal networks). For a paper addressing the topic of higher level design trends, I would appreciate additional rigorous experimentation around each principle rather than novel architectures being presented.  The authors take on the task of figuring out a set of design patterns for current deep architectures - namely themes that are recurring in the literature. If one may say so, a distributed representation of deep architectures. There are two aspects of the paper that I particularly valued: firstly, the excellent review of recent works, which made me realize how many things I have been missing myself. Secondly, the *community service* aspect of helping someone who starts figure out the *coordinate system* for deep architectures - this could potentially be more important than introducing yet-another trick of the trade, as most other submissions may do. However I think this work is still half-done, and even though working on this project is a great idea, the authors do not yet do it properly. Firstly, I am not too sure how the choice of these 14 patterns was made. Maxout for instance (pattern 14) is one of the many nonlinearities (PreLU, ReLU, ...) and I do not see how it stands on the same grounds as something as general as *3 Strive for simplicity*. Similarly some of the patterns are as vague as *Increase symmetry* and are backed up by statements such as *we noted a special degree of elegance in the FractalNet*. I do not see how this leads to a design pattern that can be applied to a new architecture - or if it applies to anything other than the FractalNet. Some other patterns are phrased with weird names *7 Cover the problem space* - which I guess stands for dataset augmentation; or *6 over-train* which is not backed up by a single reference. Unless the authors relate it to regularization (text preceding *overtrain*), which then has no connection to the description of *over-train* provided by the authors (*training a network on a harder problem to improve generalization*). If *harder problem* means one where one adds an additional term (i.e. the regularizer), the authors are doing harm to the unexperienced reader, confusing *regularization* with something that sounds like *overfitting* (i.e. the exact opposite). Furthermore, the extensions proposed in Section 4 seem a bit off tune - in particular I could not figure out -how the Taylor Series networks stem from any of the design patterns proposed in the rest of the paper. -whether the text between 4.1 and 4.1.1 is another of the architecture innovations (and if yes, why it is not in the 4.1.2, or 4.1.0) -and, most importantly, how these design patterns would be deployed in practice to think of a new network. To be more concrete, the authors mention that they propose the *freeze-drop-path* variant from *symmetry considerations* to *drop-path*. Is this an application of the *increase symmetry* pattern? How would *freeze-drop-path* be more symmetric that *drop-path*? Can this be expressed concretely, or is it some intuitive guess? If the second, it is not really part of applying a pattern, in my understanding. If the first, this is missing. What I would have appreciated more (and would like to see in a revised version) would have been a table of *design patterns* on one axis, *Deep network* on another, and a breakdown of which network applies which design pattern. A big part of the previous work is also covered in cryptic language - some minimal explanation of what is taking place in the alternative works would be useful.","['Rating:###3: Clear rejection', 'Rating:###3: Clear rejection', 'Rating:###4: Ok but not good enough - rejection']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",3,4,"Here is the structured peer review:

**Strengths:**

1. The paper provides a useful review of recent works in deep learning, summarizing key principles and design patterns in convolutional neural networks.
2. The authors' attempt to distill research from many sources to find patterns in the state of the field is a noble task, and their efforts to identify core design principles guiding the field are commendable.
3. The paper's focus on providing a set of design patterns that can be applied to new architectures is a valuable contribution to the field.
4. The authors' collection of references on the subject is a valuable resource for young researchers.

**Weaknesses:**

1. The paper's design patterns are not well-developed and include principles that are poorly explained, which may confuse readers.
2. The experimental results are unconvincing, and the proposed architectures often achieve only slightly better accuracy by introducing many more parameters (e.g., Fractal-Of-Fractal networks).
3. The writing is unclear and jumps around, making it difficult to follow the authors' arguments.
4. The paper lacks concrete examples and explanations of how the design patterns can be applied in practice.
5. The connection between the proposed Taylor Series networks and the design patterns is tenuous, and the resulting function is not even a polynomial.
6. The paper's claims about the importance of symmetry and regularization are not well-supported and may be misleading.
7. The paper's organization and structure are unclear,"
Recurrent Hidden Semi-Markov Model | OpenReview,"Segmentation and labeling of high dimensional time series data has wide applications in behavior understanding and medical diagnosis. Due to the difficulty in obtaining the label information for high dimensional data, realizing this objective in an unsupervised way is highly desirable. Hidden Semi-Markov Model (HSMM) is a classical tool for this problem. However, existing HSMM and its variants has simple conditional assumptions of observations, thus the ability to capture the nonlinear and complex dynamics within segments is limited. To tackle this limitation, we propose to incorporate the Recurrent Neural Network (RNN) to model the generative process in HSMM, resulting the Recurrent HSMM (R-HSMM). To accelerate the inference while preserving accuracy, we designed a structure encoding function to mimic the exact inference. By generalizing the penalty method to distribution space, we are able to train the model and the encoding function simultaneously. Empirical results show that the proposed R-HSMM achieves the state-of-the-art performances on both synthetic and real-world datasets.","This paper proposes a novel and interesting way to tackle the difficulties of performing inference atop HSMM. The idea of using an embedded bi-RNN to approximate the posterior is a reasonable and clever idea. That being said, I think two aspects may need further improvement: (1) An explanation as to why a bi-RNN can provide more accurate approximations than other modeling choices (e.g. structured mean field that uses a sequential model to formulate the variational distribution) is needed. I think it would make the paper stronger if the authors can explain in an intuitive way why this modeling choice is better than some other natural choices (in addition to empirical verification). (2) The real world datasets seem to be quite small (e.g. less than 100 sequences). Experimental results reported on larger datasets may also strengthen the paper.  Putting the score for now, will post the full review tomorrow.  This paper presents a novel model for unsupervised segmentation and classification of time series data. A recurrent hidden semi-markov model is proposed. This extends regular hidden semi-markov models to include a recurrent neural network (RNN) for observations. Each latent class has its own RNN for modeling observations for that category. Further, an efficient training procedure based on a variational approximation. Experiments demonstrate the effectiveness of the approach for modeling synthetic and real time series data. This is an interesting and novel paper. The proposed method is a well-motivated combination of duration modeling HMMs with state of the art observation models based on RNNs. The combination alleviates shortcomings of standard HSMM variants in terms of the simplicity of the emission probability. The method is technically sound and demonstrated to be effective. It would be interesting to see how this method compares quantitatively against CRF-based methods (e.g. Ammar, Dyer, and Smith NIPS 2014). CRFs can model more complex data likelihoods, though as noted in the response phase there are still limitations. Regardless, I think the merits of using RNNs for the class-specific generative models are clear.","['Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",7,4,"Here is the structured peer review:

**Strengths:**

1. The paper proposes a novel and interesting approach to addressing the limitations of traditional Hidden Semi-Markov Models (HSMMs) in modeling high-dimensional time series data.
2. The use of Recurrent Neural Networks (RNNs) to model the generative process in HSMM is a clever idea, and the authors provide a clear and well-justified explanation of the benefits of this approach.
3. The proposed method is technically sound and demonstrated to be effective in both synthetic and real-world datasets.
4. The authors provide a clear and concise explanation of the proposed method and its components, making it easy to follow and understand.

**Weaknesses:**

1. The paper could benefit from a more detailed explanation of why a bi-RNN is chosen over other modeling choices, such as structured mean field models. An intuitive explanation of the benefits of this choice would strengthen the paper.
2. The experimental results are based on relatively small datasets (e.g., less than 100 sequences). It would be beneficial to report results on larger datasets to further validate the effectiveness of the proposed method.
3. The paper could benefit from a more detailed comparison with existing methods, such as CRFs, to demonstrate the superiority of the proposed approach.

**Rating:** 8/10

**Confidence:** 4/5

Note: The rating is based on the overall quality of the paper, and the confidence score reflects the reviewer"
A Simple yet Effective Method to Prune Dense Layers of Neural Networks | OpenReview,"Neural networks are usually over-parameterized with significant redundancy in the number of required neurons which results in unnecessary computation and memory usage at inference time. One common approach to address this issue is to prune these big networks by removing extra neurons and parameters while maintaining the accuracy. In this paper, we propose NoiseOut, a fully automated pruning algorithm based on the correlation between activations of neurons in the hidden layers. We prove that adding additional output neurons with entirely random targets results into a higher correlation between neurons which makes pruning by NoiseOut even more efficient. Finally, we test our method on various networks and datasets. These experiments exhibit high pruning rates while maintaining the accuracy of the original network.","Summary: In this paper, the authors introduce NoiseOut, a way to reduce parameters by pruning neurons from a network. They do this by identifying pairs of neurons produce the most correlated outputs, and replacing the pair by one neuron, and then appropriately adjusting weights. This technique relies on neurons having high correlations however, so they introduce an additional output neuron -- a noise output, which results in the network trying to predict the mean of the noise distribution. As this is a constant, it increases correlation between neurons. Experiments test this out on MNIST and SVHN Comments: This is an interesting suggestion on how to prune neurons, but more experiments (on larger datasets) are probably need to be convincing that this is an approach that is guaranteed to work well. Equation (5) seems to be very straightforwards? It seems like that for larger datasets, more noise outputs might have to be added to ensure higher correlations? Is there a downside to this in terms of the overall accuracy? The paper is presented clearly, and was definitely interesting to read, so I encourage the authors to continue this line of work.  This paper proposes and tests two ideas. (1) a method of pruning networks by identifying highly correlated neuron pairs, pruning one of the pair, and then modifying downstream weights to compensate for the removal (which works well if the removed neurons were highly correlated). (2) a method, dubbed NoiseOut, for increasing neuron correlation by adding auxiliary noise target outputs to the network during training. The first idea (1) is fairly straightforward, and it is not clear if it has been tried before. It does seem to work. The second idea (2) is of unclear value and seems to this reviewer that it may merely add a regularizing effect. Comments in this direction: - In Fig 4 (right), the constant and Gaussian treatments seem to produce the same effect in both networks, right? And the Binomial effect seems the same as No_Noise. If this is true, can we conclude that the NoiseOut targets are simply serving to regularize the network, that is, to reduce its capacity slightly? - To show whether this effect is true, one would need to compare to other methods of reducing the network capacity, for example: by reducing the number of neurons, by applying L2 regularization of various values, or by applying Dropout of various strengths. Fig 7 makes an attempt at this direction, but critically misses several comparison treatments: “Pruned without any regularization”, “Pruned with only L2”, and “Pruned with only DropOut”. Have these experiments been run? Can their results be included and used to produce plots like Fig 5 and Fig 7? Without these comparisons, it seems impossible to conclude that NoiseOut does anything but provide similar regularization to DropOut or L2. The combined ideas (1) + (2) DO produce a considerable reduction in parameters, but sadly the experiments and exposition are somewhat too lacking to really understand what is going on. With a little more work the paper could be quite interesting, but as is it should probably not be accepted. Additional comments: - Section 4 states: “In all of these experiments, the only stop criteria is the accuracy decay of the model. We set the threshold for this criteria to match the original accuracy; therefore all the compressed network have the same accuracy as the original network.” Is this accuracy the train accuracy or test accuracy? If train, then test accuracy needs to be shown (how much test performance is lost when pruning?). If test, then this would typically be referred to as “cheating” and so the choice needs to be very clearly stated and then defended. - Lowercase rho is used to indicate correlation but this is never actually specified, which is confusing for. Just state once that it indicates correlation. - How do these results compare to other pruning methods? No numerical comparison is attempted.  The paper proposes to prune a neural network by removing neurons whose operation is highly correlated with other neurons. The idea is nice and somewhat novel - most pruning methods concentrate on removal of individual weights, however I haven*t done a through research on this topic. However, the experimental and theoretical justification of this method need to be improved before publication: 1. Experiments. The authors do not report accuracy degradation while pruning in the tables, laconically stating that the networks did not degrade. This is not convincing. The only details are given in Figure 5, however this Figure disagrees with Table 2: in the Table, the number of parameters ranges from 40k-600k, while the Figure pictures the range 12k-24k. Unless more details are provided, simply claiming that a network can remove 50% neurons with no number on the degradation of accuracy is not convincing. 2. Theory. The proofs do not match the experimental conditions and make unreasonable assumptions. The proofs show that in the absence of biases a network with a constant output will have two correlated neurons that generate the output offset. However, this is exactly why networks have biases and doesn*t explain why noise injection helps (the proof suggests that all should be fine with deterministic auxiliary neuron). My interpretation is that the noisy output injects gradient noise (see e.g. the concurrent ICLR submission https://openreview.net/forum?id=rkjZ2Pcxe). As such the proof muddies the picture more than it helps in understanding what is happening. Verdict: Reject and resubmit. The pruning idea has potential, however its efficiency must be more soundly demonstrated (please provide network accuracies at various pruning levels, the method removes one neuron at a time, this allows making of nice plots) rather than laconically stating that a degradation on mnist from 97% accuracy to 92% is not significant (Figure 5.). Please provide Figures and Tables that agree with the text in terms of numbers provided.","['Rating:###5: Marginally below acceptance threshold', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###3: Clear rejection']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",5,4,"Here is the structured peer review:

**Strengths:**

1. The paper proposes a novel pruning method that targets highly correlated neurons, which is an interesting approach to reducing the number of parameters in neural networks.
2. The authors provide a clear and concise explanation of the method and its underlying theory, making it easy to follow and understand.
3. The experiments are well-designed and provide a good starting point for evaluating the effectiveness of the proposed method.

**Weaknesses:**

1. The experimental results are not convincing, particularly the claim that the pruning method can remove up to 50% of the network's parameters without degrading accuracy. The discrepancy between the number of parameters reported in the tables and the plot in Figure 5 is significant, and the lack of explicit accuracy degradation metrics makes it difficult to evaluate the effectiveness of the method.
2. The theoretical justification of the method is weak and relies on unrealistic assumptions. The proof provided does not account for the presence of biases in the network, which is a common issue in neural networks.
3. The method of injecting noise into the network to increase correlation between neurons is unclear and may not be a significant contributor to the pruning process. The proof provided does not adequately address this aspect, and the experimental results do not provide clear evidence for its effectiveness.
4. The paper lacks a clear comparison to other pruning methods, which makes it difficult to evaluate the relative effectiveness of the proposed method.
5. The writing could be more precise and clear, particularly"
TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency | OpenReview,"In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence – both semantic and syntactic – but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of 6.28%. This is comparable to the state-of-the-art 5.91% resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.","This paper introduces a model that blends ideas from generative topic models with those from recurrent neural network language models. The authors evaluate the proposed approach on a document level classification benchmark as well as a language modeling benchmark and it seems to work well. There is also some analysis as to topics learned by the model and its ability to generate text. Overall the paper is clearly written and with the code promised by the authors others should be able to re-implement the approach. I have 2 potentially major questions I would ask the authors to address: 1 - LDA topic models make an exchangability (bag of words) assumption. The discussion of the generative story for TopicRNN should explicitly discuss whether this assumption is also made. On the surface it appears it is since y_t is sampled using only the document topic vector and h_t but we know that in practice h_t comes from a recurrent model that observes y_t-1. Not clear how this clean exposition of the generative model relates to what is actually done. In the Generating sequential text section it’s clear the topic model can’t generate words without using y_1 - t-1 but this seems inconsistent with the generative model specification. This needs to be shown in the paper and made clear to have a complete paper. 2 - The topic model only allows for linear interactions of the topic vector theta. It seems like this might be required to keep the generative model tractable but seems like a very poor assumption. We would expect the topic representation to have rich interactions with a language model to create nonlinear adjustments to word probabilities for a document. Please add discussion as to why this modeling choice exists and if possible how future work could modify that assumption (or explain why it’s not such a bad assumption as one might imagine) Figure 2 colors very difficult to distinguish.  This paper presents TopicRNN, a combination of LDA and RNN that augments traditional RNN with latent topics by having a switching variable that includes/excludes additive effects from latent topics when generating a word. Experiments on two tasks are performed: language modeling on PTB, and sentiment analysis on IMBD. The authors show that TopicRNN outperforms vanilla RNN on PTB and achieves SOTA result on IMDB. Some questions and comments: - In Table 2, how do you use LDA features for RNN (RNN LDA features)? - I would like to see results from LSTM included here, even though it is lower perplexity than TopicRNN. I think it*s still useful to see how much adding latent topics close the gap between RNN and LSTM. - The generated text in Table 3 are not meaningful to me. What is this supposed to highlight? Is this generated text for topic *trading*? What about the IMDB one? - How scalable is the proposed method for large vocabulary size (>10K)? - What is the accuracy on IMDB if the extracted features is used directly to perform classification? (instead of being passed to a neural network with one hidden state). I think this is a fairer comparison to BoW, LDA, and SVM methods presented as baselines.  This work combines a LDA-type topic model with a RNN and models this by having an additive effect on the predictive distribution via the topic parameters. A variational auto-encoder is used to infer the topic distribution for a given piece of text and the RNN is trained as a RNNLM. The last hidden state of the RNNLM and the topic parameters are then concatenated to use as a feature representation. The paper is well written and easy to understand. Using the topic as an additive effect on the vocabulary allows for easy inference but intuitively I would expect the topic to affect the dynamics too, e.g. the state of the RNN. The results on using this model as a feature extractor for IMDB are quite strong. Is the RNN fine-tuned on the labelled IMDB data? However, the results for PTB are weaker. From the original paper, an ensemble of 2 LSTMs is able to match the topicRNN score. This method of jointly modelling topics and a language model seems effective and relatively easy to implement. Finally, the IMDB result is no longer state of the art since this result appeared in May (Miyato et al., Adversarial Training Methods for Semi-Supervised Text Classification). Some questions: How important is the stop word modelling? What do the results look like if l_t = 0.5 for all t? It seems surprising that the RNN was more effective than the LSTM. Was gradient clipping tried in the topicLSTM case? Do GRUs also fail to work? It is also unfortunate that the model requires a stop-word list. Is the link in footnote 4 the one that is used in the experiments? Does factoring out the topics in this way allow the RNN to scale better with more neurons? How reasonable does the topic distribution look for individual documents? How peaked do they tend to be? Can you show some examples of the inferred distribution? The topics look odd for IMDB with the top word of two of the topics being the same: *campbell*. It would be interesting to compare these topics with those inferred by LDA on the same datasets. Minor comments: Below figure 2: GHz -> GB Gamma is not defined.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept', 'Rating:###8: Top 50% of accepted papers, clear accept']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",6,3,"Here is the structured peer review:

**Strengths:**

1. The paper presents a novel approach that combines the strengths of LDA and RNNs, and the authors provide a clear and concise explanation of the proposed model.
2. The experimental results show that the proposed model outperforms existing RNN-based language models on the PTB dataset and achieves state-of-the-art results on the IMDB dataset.
3. The paper provides a clear and easy-to-follow explanation of the model's architecture and training procedure.
4. The authors provide a good discussion on the potential applications of the model, including sentiment analysis and text generation.

**Weaknesses:**

1. The paper assumes that the topic model is exchangeable, which may not be the case in practice. The authors should provide a more detailed discussion on how the topic model is used in conjunction with the RNN, and how the generative model specification relates to the actual implementation.
2. The paper assumes a linear interaction between the topic vector and the RNN's output, which may not be realistic. The authors should provide more discussion on why this assumption is made, and whether it is a limitation of the model.
3. The paper does not provide a clear explanation of how the topic distribution is used in conjunction with the RNN's output, and how the model is scalable for large vocabulary sizes.
4. The paper assumes that the topic model is used as a feature extractor, but it is not clear how this is done"
HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving | OpenReview,"Large computer-understandable proofs consist of millions of intermediate logical steps. The vast majority of such steps originate from manually selected and manually guided heuristics applied to intermediate goals. So far, machine learning has generally not been used to filter or generate these steps. In this paper, we introduce a new dataset based on Higher-Order Logic (HOL) proofs, for the purpose of developing new machine learning-based theorem-proving strategies. We make this dataset publicly available under the BSD license. We propose various machine learning tasks that can be performed on this dataset, and discuss their significance for theorem proving. We also benchmark a set of simple baseline machine learning models suited for the tasks (including logistic regression convolutional neural networks and recurrent neural networks). The results of our baseline models show the promise of applying machine learning to HOL theorem proving.","Use of ML in ITP is an interesting direction of research. Authors consider the problem of predicting whether a given statement would be useful in a proof of a conjecture or not. This is posed as a binary classification task and authors propose a dataset and some deep learning based baselines. I am not an expert on ITP or theorem proving, so I will present a review from more of a ML perspective. I feel one of the goals of the paper should be to present the problem to a ML audience in a way that is easy for them to grasp. While most of the paper is well written, there are some sections that are not clear (especially section 2): - Terms such as LCF, OCaml-top level, deBruijn indices have been used without explaining or any references. These terms might be trivial in ITP literature, but were hard for me to follow. - Section 2 describes how the data was splits into train and test set. One thing which is unclear is – can the examples in the train and test set be statements about the same conjecture or are they always statements about different conjectures? It also unclear how the deep learning models are applied. Let’s consider the leftmost architecture in Figure 1. Each character is embedded into 256-D vector – and processed until the global max-pooling layer. Does this layer take a max along each feature and across all characters in the input? My another concern is only deep learning methods are presented as baselines. It would be great to compare with standard NLP techniques such as Bag of Words followed by SVM. I am sure these would be outperformed by neural networks, but the numbers would give a sense of how easy/hard the current problem setup is. Did the authors look at the success and failure cases of the algorithm? Are there any insights that can be drawn from such analysis that can inform design of future models? Overall I think the research direction of using ML for theorem proving is an interesting one. However, I also feel the paper is quite opaque. Many parts of how the data is constructed is unclear (atleast to someone with little knowledge in ITPs). If authors can revise the text to make it clearer – it would be great. The baseline models seem to perform quite well, however there are no insights into what kind of ability the models are lacking. Authors mention that they are unable to perform logical reasoning – but that’s a very vague statement. Some examples of mistakes might help make the message clearer. Further, since I am not well versed with the ITP literature it’s not possible for me to judge how valuable is this dataset. From the references, it seems like it’s drawn from a set of benchmark conjectures/proofs used in the ITP community – so its possibly a good dataset. My current rating is a weak reject, but if the authors address my concerns I would change to an accept.  The authors present a dataset extraction method, dataset and first interesting results for machine-learning supported higher order logic theorem proving. The experimental results are impressively good for a first baseline and with an accuracy higher than 0.83 in relevance classification a lot better than chance, and encourage future research in this direction. The paper is well-written in terms of presentation and argumentation and leaves little room for criticism. The related work seems to be well-covered, though I have to note that I am not an expert for automated theorem proving.  The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems. The success of methods like AlphaGo suggests that for hard combinatorial style problems, having a curated set of expert data (in this case the sequence of subproofs) is a good launching point for possibly super-human performance. Super-human ATPs are clearly extremely valuable. Although relatively smaller than the original Go datasets, this dataset seems to be a great first step. Unfortunately, the ATP and HOL aspect of this work is not my area of expertise. I can*t comment on the quality of this aspect. It would be great to see future work scale up the baselines and integrate the networks into state of the art ATPs. The capacity of deep learning methods to scale and take advantage of larger datasets means there*s a possibility of an iterative approach to improving ATPs: as the ATPs get stronger they may generate more data in the form of new theorems. This may be a long way off, but the possibility is exciting.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###7: Good paper, accept']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",6,3,"Here is the structured peer review:

**Strengths:**

1. The authors present a novel approach to using machine learning for Higher-Order Logic (HOL) theorem proving, which is an interesting direction of research.
2. The dataset is well-constructed and seems to be a good starting point for future research in this direction.
3. The experimental results are impressive, with an accuracy of 0.83 in relevance classification, which is higher than chance.
4. The paper is well-written and easy to follow, making it accessible to a non-expert audience.
5. The authors provide a clear and concise presentation of their approach and results.

**Weaknesses:**

1. The paper lacks clarity in its explanation of technical terms and concepts, such as LCF, OCaml-top level, and deBruijn indices, which may be unfamiliar to non-experts in ITP.
2. The dataset construction process is not fully explained, making it difficult to understand how the data was curated and what the criteria were for selecting the conjectures and their corresponding proof steps.
3. The baseline models presented are limited to deep learning methods, and there is no comparison with standard NLP techniques, such as Bag of Words followed by SVM.
4. The paper does not provide insights into the success and failure cases of the algorithm, which would be valuable for understanding its limitations and potential areas for improvement.
5. The authors' statement that the models are unable to perform logical reasoning is vague"
Regularizing Neural Networks by Penalizing Confident Output Distributions | OpenReview,"We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT*14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.","The paper experimentally investigates a slightly modified version of label smoothing technique for neural network training, and reports results on various tasks. Such smoothing idea is not new, but was not investigated previously in wide range of machine learning tasks. Comments: The paper should report the state-of-the-art results for speech recognition tasks (TIMIT, WSJ), even if models are not directly comparable. The error back-propagation of label smoothing through softmax is straightforward and efficient. Is there an efficient solution for BP of the entropy smoothing through softmax? Although the classification accuracy could remain the same, the model will not estimate the true posterior distribution with this kind of smoothing. This might be an issue in complex machine learning problems where the decision is made on higher level and based on the posterior estimations, e.g. language models in speech recognition. More motivation is necessary for the proposed smoothing.  Specifically, this paper suggests regularizing the estimator of a probability distribution to prefer high-entropy distributions. This avoids overfitting. I generally like this idea. Regularizing the behavior of the model often makes more sense than regularizing its parameters. After all, the behavior is interpretable, whereas the parameters are uninterpretable and work together in mysterious ways to produce the behavior. So one might be able to choose a more sensible prior over the behavior. In other words, prefer parameters not because they are individually close to 0 but because they jointly lead to a distribution that is plausible or low-risk a priori. Pro: I believe that the idea is natural and sound (that is, I do not share the doubts of AnonReviewer5). Pro: It*s possible that this hasn*t been well-explored yet in neural networks (not sure). Pro: The experimental results look good. So maybe everyone should use this kind of regularizer. Con: It is a kind of pollution of the scientific literature to introduce this idea to the community as if it were unconnected to (almost) anything else in machine learning. There are many, many papers that include a scaled entropy term in the optimization objective! It*s not just for reinforcement learning. Please see the long list of connections in my pre-review questions / comments. Con: Experimental results should always be accompanied by significance tests and error analysis. Is your trained model actually doing better on the distribution of test data, or was your test set too small to tell? Are the improvements robust across many different training sets? What errors does your model fix, and what errors does it introduce? Summary recommendation: Revise and resubmit. ICLR has lots of submissions. I would prefer to reward authors who not only tried something, but who properly contextualized it and carefully evaluated it. Otherwise, there*s a race to the bottom where everyone wants to be the first to try something, so that readers are confronted with a confusing sea of slapdash papers with unclear relationships.  The authors propose a simple idea. They penalize confident predictions by using the entropy of the predictive distribution as a regularizer. The authors consider two variations on this idea. In one, they penalize the divergence from the uniform distribution. In the other variation, they penalize distance from the base rates. They term this variation *unigram* but I find the name odd as I*ve never seen multi-class labels described as unigrams before. What would a bigram be? The idea is simple, and while it*s been used in the context of reinforcement learning, it hasn*t been popularized as a regularizer for improving generalization in supervised learning. The justifications for the idea still lacks analysis. And the author responses comparing it to L2 regularization have some holes. A simple number line example with polynomial regression makes clear how L2 regularization could prevent a model from badly overfitting to accommodate every data point. In contrast, it seems trivial to fit every data point and satisfy arbitrarily high entropy. Of course, the un-regularized optimization is to maximize log likelihood, not simply to maximize accuracy. And perhaps something interesting may be happening at the interplay between the log likelihood objective and the regularization objective. But the paper doesn*t indicate precisely what. I could imagine the following scenario: when the network outputs probabilities near 0, it can get high loss (if the label is 1). The entropy regularization could be stabilizing the gradient, preventing sharp loss on outlier examples. The regularization then might owe mainly to faster convergence. Could the authors analyze the effect empirically, on the distribution of the gradient norms? The strength of this paper is its empirical rigor. The authors take their idea and put it through its paces on a host of popular and classic benchmarks spanning CNNs and RNNs. It appears that on some datasets, especially language modeling, the confidence penalty outperforms label smoothing. At present, I rate this paper as a borderline contribution but I*m open to revising my review pending further modifications. Typo: In related work: *Penalizing entropy* - you mean penalizing low entropy","['Rating:###6: Marginally above acceptance threshold', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",6,4,"Here is the structured peer review:

**Strengths:**

1. The paper proposes a novel idea for regularizing neural networks by penalizing low entropy output distributions, which is a fresh perspective on the topic of regularization.
2. The authors provide a clear and concise explanation of the concept and its connection to label smoothing.
3. The experimental results on various benchmarks, including image classification, language modeling, and speech recognition, demonstrate the effectiveness of the proposed method.
4. The authors' use of a simple and intuitive regularization term, which penalizes low entropy output distributions, makes the approach accessible to a wide range of readers.

**Weaknesses:**

1. The paper lacks a clear motivation for the proposed regularization term, and the justification for its effectiveness is limited to a brief discussion of its connection to label smoothing.
2. The authors' analysis of the interplay between the log likelihood objective and the regularization objective is incomplete, and the potential benefits of the regularization term are not fully explored.
3. The experimental results do not provide a clear understanding of the strengths and limitations of the proposed method, and the comparison to L2 regularization is superficial.
4. The paper could benefit from a more rigorous analysis of the empirical results, including significance tests and error analysis to ensure that the improvements are statistically significant.

**Rating:** 7
**Confidence:** 3

Note: I rated the paper as 7 out of 10, indicating that it is a solid contribution to the field, but"
Learning a Natural Language Interface with Neural Programmer | OpenReview,"Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.7% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser.","The paper presents an end-to-end neural network model for the problem of designing natural language interfaces for database queries. The proposed approach uses only weak supervision signals to learn the parameters of the model. Unlike in traditional approaches, where the problem is solved by semantically parsing a natural language query into logical forms and executing those logical forms over the given data base, the proposed approach trains a neural network in an end-to-end manner which goes directly from the natural language query to the final answer obtained by processing the data base. This is achieved by formulating a collection of operations to be performed over the data base as continuous operations, the distributions over which is learnt using the now-standard soft attention mechanisms. The model is validated on the smallish WikiTableQuestions dataset, where the authors show that a single model performs worse than the approach which uses the traditional Semantic Parsing technique. However an ensemble of 15 models (trained in a variety of ways) results in comparable performance to the state of the art. I feel that the paper proposes an interesting solution to the hard problem of learning natural language interfaces for data bases. The model is an extension of the previously proposed models of Neelakantan 2016. The experimental section is rather weak though. The authors only show their model work on a single smallish dataset. Would love to see more ablation studies of their model and comparison against fancier version of memnns (i do not buy their initial response to not testing against memory networks). I do have a few objections though. -- The details of the model are rather convoluted and the Section 2.1 is not very clearly written. In particular with the absence of the accompanying code the model will be super hard to replicate. I wish the authors do a better job in explaining the details as to how exactly the discrete operations are modeled, what is the role of the *row selector*, the *scalar answer* and the *lookup answer* etc. -- The authors do a full attention over the entire database. Do they think this approach would scale when the data bases are huge (millions of rows)? Wish they experimented with larger datasets as well.  This paper proposes a weakly supervised, end-to-end neural network model for solving a challenging natural language understanding task. As an extension of the Neural Programmer, this work aims at overcoming the ambiguities imposed by natural language. By predefining a set of operations, the model is able to learn the interface between the language reasoning and answer composition using backpropagation. On the WikiTableQuestions dataset, it is able to achieve a slightly better performance than the traditional semantic parser methods. Overall, this is a very interesting and promising work as it involves a lot of real-world challenges about natural language understanding. The intuitions and design of the model are very clear, but the complication makes the paper a bit difficult to read, which means the model is also difficult to be reimplemented. I would expect to see more details about model ablation and it would help us figure out the prominent parts of the model design.  This paper proposes a weakly supervised, end-to-end neural network model to learn a natural language interface for tables. The neural programmer is applied to the WikiTableQuestions, a natural language QA dataset and achieves reasonable accuracy. An ensemble further boosts the performance by combining components built with different configurations, and achieves comparable performance as the traditional natural language semantic parser baseline. Dropout and weight decay seem to play a significant role. It*ll be interesting to see more error analysis and the major reason for the still low accuracy compared to many other NLP tasks. What*s the headroom and oracle number with the current approach?","['Rating:###7: Good paper, accept', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",7,3,"Here is the structured peer review:

**Strengths:**

1. The paper proposes a novel approach to learning natural language interfaces for database queries, which is a challenging task that involves deep language understanding and multi-step reasoning.
2. The authors present a clear and concise intuition behind the model's design, which is easy to follow and understand.
3. The experimental results show that the model achieves reasonable accuracy on the WikiTableQuestions dataset, which is a significant achievement.
4. The use of weak supervision and end-to-end training is an interesting approach that avoids the need for domain-specific grammars, rules, or annotations.

**Weaknesses:**

1. The experimental section is relatively weak, with only a single smallish dataset and limited comparison to other models.
2. The model's architecture is convoluted, making it difficult to replicate and understand, particularly the role of the discrete operations, row selector, scalar answer, and lookup answer.
3. The paper does not provide sufficient details about the model's performance on larger datasets, which raises concerns about its scalability.
4. The authors do not provide enough error analysis or insights into the major reasons for the low accuracy compared to other NLP tasks.
5. The comparison to traditional semantic parsing methods is limited, and the authors do not provide a clear explanation of why their approach is more effective.

**Rating:** 7
**Confidence:** 3

Note: The rating is 7 because the paper proposes a novel approach to a"
Representation Stability as a Regularizer for Improved Text Analytics Transfer Learning | OpenReview,"Although neural networks are well suited for sequential transfer learning tasks, the catastrophic forgetting problem hinders proper integration of prior knowledge. In this work, we propose a solution to this problem by using a multi-task objective based on the idea of distillation and a mechanism that directly penalizes forgetting at the shared representation layer during the knowledge integration phase of training. We demonstrate our approach on a Twitter domain sentiment analysis task with sequential knowledge transfer from four related tasks. We show that our technique outperforms networks fine-tuned to the target task. Additionally, we show both through empirical evidence and examples that it does not forget useful knowledge from the source task that is forgotten during standard fine-tuning. Surprisingly, we find that first distilling a human made rule based sentiment engine into a recurrent neural network and then integrating the knowledge with the target task data leads to a substantial gain in generalization performance. Our experiments demonstrate the power of multi-source transfer techniques in practical text analytics problems when paired with distillation. In particular, for the SemEval 2016 Task 4 Subtask A (Nakov et al., 2016) dataset we surpass the state of the art established during the competition with a comparatively simple model architecture that is not even competitive when trained on only the labeled task specific data.","This paper proposes a method for transfer learning, i.e. leveraging a network trained on some original task A in learning a new task B, which not only improves performance on the new task B, but also tries to avoid degradation in performance on A. The general idea is based on encouraging a model trained on A, while training on the new task B, to match fake targets produced by the model itself but when it is trained only on the original task A. Experiments show that this method can help in improving the result on task B, and is better than other baselines, including standard fine-tuning. General comments/questions: - As far as I can tell, there is no experimental result supporting the claim that your model still performs well on the original task. All experiments show that you can improve on the new task only. - The introduction makes a strong statements about the distilling logical rule engine into a neural network, which I find a bit misleading. The approach in the paper is not specific to transferring from logical rules (as stated in the Sec 2) and is simply relying on the rule engine to provide labels for unlabelled data. - One of the obvious baselines to compare with your approach is standard multi-task learning on both tasks A and B together. That is, you train the model from scratch on both tasks simultaneously (which sharing parameters). It is not clear this is the same as what is referred to in Sec. 8 as *joint training*. Can you please explain more clearly what you refer to as joint training? - Why can*t we find the same baselines in both Table 2 and Table 3? For example Table 2 is missing *joint training*, and Table 3 is missing GRU trained on the target task. - While the idea is presented as a general method for transfer learning, experiments are focused on one domain (sentiment analysis on SemEval task). I think that either experiments should include applying the idea on at least one other different domain, or the writing of the paper should be modified to make the focus more specific to this domain/task. Writing comments - The writing of the paper in general needs some improvement, but more specifically in the experiment section, where experiment setting and baselines should be explained more concisely. - Ensemble methodology paragraph does not fit the flow of the paper. I would rather explain it in the experiments section, rather than including it as part of your approach. - Table 1 seems like reporting cross-validation results, and I do not think is very informative to general reader.  This paper proposes a regularization technique for neural network training that relies on having multiple related tasks or datasets in a transfer learning setting. The proposed technique is straightforward to describe and can also leverage external labeling systems perhaps based on logical rules. The paper is clearly written and the experiments seem relatively thorough. Overall this is a nice paper but does not fully address how robust the proposed technique is. For each experiment there seems to be a slightly different application of the proposed technique, or a lot of ensembling and cross validation. I can’t figure out if this is because the proposed technique does not work well in general and thus required a lot of fiddling to get right in experiments, or if this is simply an artifact of ad-hoc experiments to try and get the best performance overall. If more datasets or addressing this issue directly in discussion was able to show this the strengths and limitations of the proposed technique more clearly, this could be a great paper. Overall the proposed method seems nice and possibly useful for other problems. However in the details of logical rule distillation and various experiment settings it seems like there is a lot of running the model many times or selecting a particular way of reusing the models and data that makes me wonder how robust the technique is or whether it requires a lot of trying various approaches, ensembling, or picking the best model from cross validation to show real gains. The authors could help by discussing this explicitly for all experiments in one place rather than listing the various choices / approaches in each experiment. As an example, these sorts of phrases make me very unsure how reliable the method is in practice versus how much the authors had to engineer this regularizer to perform well: “We noticed that equation 8 is actually prone to overfitting away from a good solution on the test set although it often finds a pretty good one early in training. “ The introduction section should first review the definitions of transfer learning vs multi-task learning to make the discussion more clear. It also deems justification why “catastrophic forgetting” is actually a problem. If the final target task is the only thing of interest then forgetting the source task is not an issue and the authors should motivate why forgetting matters in their setting. This paper explores sequential transfer so it’s not obvious why forgetting the source task matters. Section 7 introduces the logical rules engine in a fairly specific context. Rather it would be good state more generally what this system entails to help people figure out how this method would apply to other problems.  This paper introduces a new method for transfer learning that avoids the catastrophic forgetting problem. It also describes an ensembling strategy for combining models that were learned using transfer learning from different sources. It puts all of this together in the context of recurrent neural networks for text analytics problems, to achieve new state-of-the-art results for a subtask of the SemEval 2016 competition. As the paper acknowledges, 1.5% improvement over the state-of-the-art is somewhat disappointing considering that it uses an ensemble of 5 quite different networks. These are interesting contributions, but due to the many pieces, unfortunately, the paper does not seem to have a clear focus. From the title and abstract/conclusion I would*ve expected a focus on the transfer learning problem. However, the description of the authors* approach is merely a page, and its evaluation is only another page. In order to show that this idea is a new methodological advance, it would*ve been good to show that it also works in at least one other application (e.g., just some multi-task supervised learning problem). Rather, the paper takes a quite domain-specific approach and discusses the pieces the authors used to obtain state-of-the-art performance for one problem. That is OK, but I would*ve rather expected that from a paper called something like *Improved knowledge transfer and distillation for text analytics*. If accepted, I encourage the authors to change the title to something along those lines. The many pieces also made it hard for me to follow the authors* train of thought. I*m sure the authors had a good reason for their section ordering, but I didn*t see the red thread in it. How about re-organizing the sections as follows to discuss one contribution at a time? 1,2,4,3,8 including 6, put 9 into an appendix and point to it from here, 7, 5, 10. That would first discuss the transfer learning piece (4, and experiments potentially in a subsection with previous sections 3,8,6), then discuss the distillation of logical rules (7), and then discuss ensembling and experiments for it (5 and 10). One clue that the current structure is suboptimal is that there are 11 sections... I like the authors* idea for transfer learning without catastropic forgetting, and I must admit I would*ve rather liked to read a paper solely about that (studying where it works, and where it fails) than about the many other topics of the paper. I weakly vote for acceptance since I like the ideas, but if the paper does not make it in, I would suggest that the authors consider splitting it into two papers, each of which could hopefully be more focused.","['Rating:###5: Marginally below acceptance threshold', 'Rating:###7: Good paper, accept', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",5,4,"Here is the structured peer review:

**Strengths:**

1. The paper proposes a novel approach to transfer learning that addresses the catastrophic forgetting problem, which is a significant challenge in machine learning.
2. The authors provide a clear and concise explanation of the approach, making it easy to understand for a broad audience.
3. The experiments demonstrate the effectiveness of the proposed method, and the results are impressive, especially considering the simplicity of the model architecture.
4. The paper highlights the potential of multi-source transfer techniques in practical text analytics problems, which is an important area of research.

**Weaknesses:**

1. The paper does not provide a clear focus on the transfer learning problem, and the description of the approach is scattered throughout the paper, making it difficult to follow.
2. The experimental results are not entirely clear, and the paper could benefit from more detailed explanations of the experiment settings and baselines.
3. The paper does not adequately address the limitations of the proposed method, such as its robustness to different experimental settings and its applicability to other problems.
4. The writing could be improved, with more concise and clear explanations of the concepts and results.
5. The paper could benefit from more explicit discussion of the theoretical underpinnings of the approach, such as the definition of transfer learning and the role of catastrophic forgetting.
6. The evaluation of the proposed method is not comprehensive, and the paper could benefit from more thorough analysis of its strengths and weaknesses.

**Rating:**"
