title,abstract,review,rate,confidence,rating_num,confidence_num,zero_shot_review
Training deep neural-networks using a noise adaptation layer | OpenReview,"The availability of large datsets has enabled neural networks to achieve impressive recognition results. However, the presence of inaccurate class labels is known to deteriorate the performance of even the best classifiers in a broad range of classification problems. Noisy labels also tend to be more harmful than noisy attributes. When the observed label is noisy, we can view the correct label as a latent random variable and model the noise processes by a communication channel with unknown parameters. Thus we can apply the EM algorithm to find the parameters of both the network and the noise and to estimate the correct label. In this study we present a neural-network approach that optimizes the same likelihood function as optimized by the EM algorithm. The noise is explicitly modeled by an additional softmax layer that connects the correct labels to the noisy ones. This scheme is then extended to the case where the noisy labels are dependent on the features in addition to the correct labels. Experimental results demonstrate that this approach outperforms previous methods.","This paper looks at how to train if there are significant label noise present. This is a good paper where two main methods are proposed, the first one is a latent variable model and training would require the EM algorithm, alternating between estimating the true label and maximizing the parameters given a true label. The second directly integrates out the true label and simply optimizes the p(z|x). Pros: the paper examines a training scenario which is a real concern for big dataset which are not carefully annotated. Cons: the results on mnist is all synthetic and it*s hard to tell if this would translate to a win on real datasets. - comments: Equation 11 should be expensive, what happens if you are training on imagenet with 1000 classes? It would be nice to see how well you can recover the corrupting distribution parameter using either the EM or the integration method. Overall, this is an OK paper. However, the ideas are not novel as previous cited papers have tried to handle noise in the labels. I think the authors can make the paper better by either demonstrating state-of-the-art results on a dataset known to have label noise, or demonstrate that a method can reliably estimate the true label corrupting probabilities.  The paper addressed the erroneous label problem for supervised training. The problem is well formulated and the presented solution is novel. The experimental justification is limited. The effectiveness of the proposed method is hard to gauge, especially how to scale the proposed method to large number of classification targets and whether it is still effective. For example, it would be interesting to see whether the proposed method is better than training with only less but high quality data. From Figure 2, it seems with more data, the proposed method tends to behave very well when the noise fraction is below a threshold and dramatically degrades once passing that threshold. Analysis and justification of this behavior whether it is just by chance or an expected one of the method would be very useful.  This work address the problem of supervised learning from strongly labeled data with label noise. This is a very practical and relevant problem in applied machine learning. The authors note that using sampling approaches such as EM isn*t effective, too slow and cannot be integrated into end-to-end training. Thus, they propose to simulate the effects of EM by a noisy adaptation layer, effectively a softmax, that is added to the architecture during training, and is omitted at inference time. The proposed algorithm is evaluated on MNIST and shows improvements over existing approaches that deal with noisy labeled data. A few comments. 1. There is no discussion in the work about the increased complexity of training for the model with two softmaxes. 2. What is the rationale for having consecutive (serialized) softmaxes, instead of having a compound objective with two losses, or a network with parallel losses and two sets of gradients? 3. The proposed architecture with only two hidden layers isn*t not representative of larger and deeper models that are practically used, and it is not clear that shown results will scale to bigger networks. 4. Why is the approach only evaluated on MNIST, a dataset that is unrealistically simple.","['Rating:###5: Marginally below acceptance threshold', 'Rating:###7: Good paper, accept', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",5,4,"Strengths: 1. The paper addresses a very important problem in machine learning, namely the issue of noisy labels in supervised learning. 2. The authors propose a novel approach to deal with noisy labels, which is to add a noisy adaptation layer to the network. 3. The paper is well written and easy to follow. Weaknesses: 1. The paper is very short and lacks a clear motivation for the proposed approach. 2. The experimental results are very limited. The authors only evaluate their approach on MNIST, which is a very simple dataset. 3. The authors do not discuss the complexity of the proposed approach, which is a potential issue for larger networks. 4. The paper lacks a clear discussion of the theoretical properties of the proposed approach. Rating: 6 Confidence: 3"
Deep Character-Level Neural Machine Translation By Learning Morphology | OpenReview,"Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.","* Summary: This paper proposes a neural machine translation model that translates the source and the target texts in an end to end manner from characters to characters. The model can learn morphology in the encoder and in the decoder the authors use a hierarchical decoder. Authors provide very compelling results on various bilingual corpora for different language pairs. The paper is well-written, the results are competitive compared to other baselines in the literature. * Review: - I think the paper is very well written, I like the analysis presented in this paper. It is clean and precise. - The idea of using hierarchical decoders have been explored before, e.g. [1]. Can you cite those papers? - This paper is mainly an application paper and it is mainly the application of several existing components on the character-level NMT tasks. In this sense, it is good that authors made their codes available online. However, the contributions from the general ML point of view is still limited. * Some Requests: -Can you add the size of the models to the Table 1? - Can you add some of the failure cases of your model, where the model failed to translate correctly? * An Overview of the Review: Pros: - The paper is well written - Extensive analysis of the model on various language pairs - Convincing experimental results. Cons: - The model is complicated. - Mainly an architecture engineering/application paper(bringing together various well-known techniques), not much novelty. - The proposed model is potentially slower than the regular models since it needs to operate over the characters instead of the words and uses several RNNs. [1] Serban IV, Sordoni A, Bengio Y, Courville A, Pineau J. Hierarchical neural network generative models for movie dialogues. arXiv preprint arXiv:1507.04808. 2015 Jul 17.  Update after reading the authors* responses & the paper revision dated Dec 21: I have removed the comment *insufficient comparison to past work* in the title & update the score from 3 -> 5. The main reason for the score is on novelty. The proposal of HGRU & the use of the R matrix are basically just to achieve the effect of *whether to continue from character-level states or using word-level states*. It seems that these solutions are specific to symbolic frameworks like Theano (which the authors used) and TensorFlow. This, however, is not a problem for languages like Matlab (which Luong & Manning used) or Torch. ----- This is a well-written paper with good analysis in which I especially like Figure 5. However I think there is little novelty in this work. The title is about learning morphology but there is nothing specifically enforced in the model to learn morphemes or subword units. For example, maybe some constraints can be put on the weights in w_i in Figure 1 to detect morpheme boundaries or some additional objective like MDL can be used (though it*s not clear how these constraints can be incorporated cleanly). Moreover, I*m very surprised that litte comparison (only a brief mention) was given to the work of (Luong & Manning, 2016) [1], which trains deep 8-layer word-character models and achieves much better results on English-Czech, e.g., 19.6 BLEU compared to 17.0 BLEU achieved in the paper. I think the HGRU thing is over-complicated in terms of presentation. If I read correctly, what HGRU does is basically either continue the character decoder or reset using word-level states at boundaries, which is what was done in [1]. Luong & Manning (2016) even make it more efficient by not having to decode all target words at the morpheme level & it would be good to know the speed of the model proposed in this ICLR submission. What end up new in this paper are perhaps different analyses on what a character-based model learns & adding an additional RNN layer in the encoder. One minor comment: annotate h_t in Figure 1. [1] Minh-Thang Luong and Christopher D. Manning. 2016. Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models. ACL. https://arxiv.org/pdf/1604.00788v2.pdf  The paper presents one of the first neural translation systems that operates purely at the character-level, another one being https://arxiv.org/abs/1610.03017 , which can be considered a concurrent work. The system is rather complicated and consists of a lot of recurrent networks. The quantitative results are quite good and the qualitative results are quite encouraging. First, a few words about the quality of presentation. Despite being an expert in the area, it is hard for me to be sure that I exactly understood what is being done. The Subsections 3.1 and 3.2 sketch two main features of the architecture at a rather high-level. For example, does the RNN sentence encoder receive one vector per word as input or more? Figure 2 suggests that it’s just one. The notation h_t is overloaded, used in both Subsection 3.1 and 3.2 with clearly different meaning. An Appendix that explains unambiguously how the model works would be in order. Also, the approach appears to be limited by its reliance on the availability of blanks between words, a trait which not all languages possess. Second, the results seem to be quite good. However, no significant improvement over bpe2char systems is reported. Also, I would be curious to know how long it takes to train such a model, because from the description it seems like the model would be very slow to train (400 steps of BiNNN). On a related note, normally an ablation test is a must for such papers, to show that the architectural enhancements applied were actually necessary. I can imagine that this would take a lot of GPU time for such a complex model. On the bright side, Figure 3 presents some really interesting properties that of the embeddings that the model learnt. Likewise interesting is Figure 5. To conclude, I think that this an interesting application paper, but the execution quality could be improved. I am ready to increase my score if an ablation test confirms that the considered encoder is better than a trivial baseline, that e.g. takes the last hidden state for each RNN.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###7: Good paper, accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",6,4,"Strengths: 1. The paper is well-written and the results are very convincing. 2. The paper is well-structured and easy to follow. 3. The paper is very well-motivated. Weaknesses: 1. The paper is mainly an application paper and it is mainly the application of several existing components on the character-level NMT tasks. In this sense, it is good that authors made their codes available online. However, the contributions from the general ML point of view is still limited. 2. The model is complicated. 3. Mainly an architecture engineering/application paper(bringing together various well-known techniques), not much novelty. Rating: 8 Confidence: 4"
Third Person Imitation Learning | OpenReview,"Reinforcement learning (RL) makes it possible to train agents capable of achieving sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstrations are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should have taken. While powerful, this kind of imitation learning is limited by the relatively hard problem of collecting first-person demonstrations. Humans address this problem by learning from third-person demonstrations: they observe other humans perform tasks, infer the task, and accomplish the same task themselves. In this paper, we present a method for unsupervised third-person imitation learning. Here third-person refers to training an agent to correctly achieve a simple goal in a simple environment when it is provided a demonstration of a teacher achieving the same goal but from a different viewpoint; and unsupervised refers to the fact that the agent receives only these third-person demonstrations, and is not provided a correspondence between teacher states and student states. Our methods primary insight is that recent advances from domain confusion can be utilized to yield domain agnostic features which are crucial during the training process. To validate our approach, we report successful experiments on learning from third-person demonstrations in a pointmass domain, a reacher domain, and inverted pendulum.","This paper proposed a novel adversarial framework to train a model from demonstrations in a third-person perspective, to perform the task in the first-person view. Here the adversarial training is used to extract a novice-expert (or third-person/first-person) independent feature so that the agent can use to perform the same policy in a different view point. While the idea is quite elegant and novel (I enjoy reading it), more experiments are needed to justify the approach. Probably the most important issue is that there is no baseline, e.g., what if we train the model with the image from the same viewpoint? It should be better than the proposed approach but how close are they? How the performance changes when we gradually change the viewpoint from third-person to first-person? Another important question is that maybe the network just blindly remembers the policy, in this case, the extracted feature could be artifacts of the input image that implicitly counts the time tick in some way (and thus domain-agonistic), but can still perform reasonable policy. Since the experiments are conduct in a synthetic environment, this might happen. An easy check is to run the algorithm on multiple viewpoint and/or with blurred/differently rendered images, and/or with random initial conditions. Other ablation analysis is also needed. For example, I am not fully convinced by the gradient flipping trick used in Eqn. 5, and in the experiments there is no ablation analysis for that (GAN/EM style training versus gradient flipping trick). For the experiments, Fig. 4,5,6 does not have error bars and is not very convincing.  The paper presents an interesting new problem setup for imitation learning: an agent tries to imitate a trajectory demonstrated by an expert but said trajectory is demonstrated in a different state or observation space than the one accessible by the agent (although the dynamics of the underlying MDP are shared). The paper proposes a solution strategy that combines recent work on domain confusion losses with a recent IRL method based on generative adversarial networks. I believe the general problem to be relevant and agree with the authors that it results in a more natural formulation for imitation learning that might be more widely applicable. There are however a few issues with the paper in its current state that make the paper fall short of being a great exploration of a novel idea. I will list these concerns in the following (in arbitrary order) - The paper feels at times to be a bit hurriedly written (this also mainly manifests itself in the experiments, see comment below) and makes a few fairly strong claims in the introduction that in my opinion are not backed up by their approach. For example: *Advancements in this class of algorithms would significantly improve the state of robotics, because it will enable anyone to easily teach robots new skills*; given that the current method to my understanding has the same issues that come with standard GAN training (e.g. instability etc.) and requires a very accurate simulator to work well (since TRPO will require a large number of simulated trajectories in each step) this seems like an overstatement. There are some sentences that are ungrammatical or switch tense in the middle of the sentence making the paper harder to read than necessary, e.g. Page 2: *we find that this simple approach has been able to solve the problems* - The general idea of third person imitation learning is nice, clear and (at least to my understanding) also novel. However, instead of exploring how to generally adapt current IRL algorithms to this setting the authors pick a specific approach that they find promising (using GANs for IRL) and extend it. A significant amount of time is then spent on explaining why current IRL algorithms will fail in the third-person setting. I fail to see why the situation for the GAN based approach is any different than that of any other existing IRL algorithm. To be more clear: I see no reason why e.g. behavioral cloning could not be extended with a domain confusion loss in exactly the same way as the approach presented. To this end it would have been nice to rather discuss which algorithms can be adapted in the same way (and also test them) and which ones cannot. One straightforward approach to apply any IRL algorithm would for example be to train two autoencoders for both domains that share higher layers + a domain confusion loss on the highest layer, should that not result in features that are directly usable? If not, why? - While the general argument that existing IRL algorithms will fail in the proposed setting seems reasonable it is still unfortunate that no attempts have been made to validate this empirically. No comparison is made regarding what happens when one e.g. performs supervised learning (behavioral cloning) using the expert observations and then transfers to the changed domain. How well would this work in practice ? Also, how fast can different IRL algorithms solve the target task in general (assuming a first person perspective) ? - Although I like the idea of presenting the experiments as being directed towards answering a specific set of questions I feel like the posed questions somewhat distract from the main theme of the paper. Question 2 suddenly makes the use of additional velocity information to be a main point of importance and the experiments regarding Question 3 in the end only contain evaluations regarding two hyperparameters (ignoring all other parameters such as the parameters for TRPO, the number of rollouts per iteration, the number of presented expert episodes and the design choices for the GAN). I understand that not all of these can be evaluated thoroughly in a conference paper but I feel like some more experiments or at least some discussion would have helped here. - The presented experimental evaluation somewhat hides the cost of TRPO training with the obtained reward function. How many roll-outs are necessary in each step? - The experiments lack some details: How are the expert trajectories obtained? The domains for the pendulum experiment seem identical except for coloring of the pole, is that correct (I am surprised this small change seems to have such a detrimental effect)? Figure 3 shows average performance over 5 trials, what about Figure 5 (if this is also average performance, what is the variance here)? Given that GANs are not easy to train, how often does the training fail/were you able to re-use the hyperparameters across all experiments? UPDATE: I updated the score. Please see my response to the rebuttal below.  The paper extends the imitation learning paradigm to the case where the demonstrator and learner have different points of view. This is an important contribution, with several good applications. The main insight is to use adversarial training to learn a policy that is robust to this difference in perspective. This problem formulation is quite novel compared to the standard imitation learning literature (usually first-order perspective), though has close links to the literature on transfer learning (as explained in Sec.2). The basic approach is clearly explained, and follows quite readily from recent literature on imitation learning and adversarial training. I would have expected to see comparison to the following methods added to Figure 3: 1) Standard 1st person imitation learning using agent A data, and apply the policy on agent A. This is an upper-bound on how well you can expect to do, since you have the correct perspective. 2) Standard 1st person imitation learning using agent A data, then apply the policy on agent B. Here, I expect it might do less well than 3rd person learning, but worth checking to be sure, and showing what is the gap in performance. 3) Reinforcement learning using agent A data, and apply the policy on agent A. I expect this might do better than 3rd person imitation learning but it might depend on the scenario (e.g. difficulty of imitation vs exploration; how different are the points of view between the agents). I understand this is how the expert data is collected for the demonstrator, but I don’t see the performance results from just using this procedure on the learner (to compare to Fig.3 results). Including these results would in my view significantly enhance the impact of the paper.","['Rating:###5: Marginally below acceptance threshold', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",5,3,Strengths: 1. The paper presents a novel approach to third-person imitation learning. 2. The authors provide a clear and well-structured introduction to the problem and the proposed approach. 3. The paper is well-written and easy to follow. Weaknesses: 1. The paper does not provide a clear baseline for comparison. 2. The experiments lack some details. 3. The paper does not provide a clear discussion of the limitations of the proposed approach. Rating: 7 Confidence: 4
Unsupervised Learning of State Representations for Multiple Tasks | OpenReview,"We present an approach for learning state representations in multi-task reinforcement learning. Our method learns multiple low-dimensional state representations from raw observations in an unsupervised fashion, without any knowledge of which task is executed, nor of the number of tasks involved. The method is based on a gated neural network architecture, trained with an extension of the learning with robotic priors objective. In simulated experiments, we show that our method is able to learn better state representations for reinforcement learning, and we analyze why and when it manages to do so.","This paper is about learning unsupervised state representations using multi-task reinforcement learning. The authors propose a novel approach combining gated neural networks with multitask learning with robotics priors. They evaluated their approach on two simulated datasets and showed promising results. The paper is clearly written and is theoretically sound. Positives: + Gating to enable learning a joint representation + Multi-task learning extended from a single task in prior work + Combining multiple types of losses to learn a strong representation (Coherence, Proportionality, Causality, Repeatability, Consistency and Separation) Negatives: - Parameters choice is arbitrary (w parameters) - Limiting the multi-task learning to be different to individual tasks rather than sharing and transferring knowledge between tasks - The experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare. I would recommend that the authors consider a more standardized way of picking the model parameters and evaluate on a more standard and high-dimensional datasets.  This paper builds upon the method of Jonschkowski & Brock to learn state representations for multiple tasks, rather than a single task. The research direction of learning representations for multiple tasks is an interesting one, and largely unexplored. The approach in the paper is to learn a different representation for each task, and a different policy for each task, where the task is detected automatically and built into the neural network. The authors state that the proposed method is orthogonal to multi-task learning, though the end goal of learning to solve multiple tasks is the same. It would be interesting and helpful to see more discussion on this point in the paper, as discussed in the pre-review question phase. References to other multi-task learning works, e.g. policy distillation and actor-mimic (both ICLR ’16), may be appropriate as well. The method proposes to jointly learn a task classifier with a state representation learner, by using a differentiable gating mechanism to control the flow of information. The paper proposes a task coherence prior for this gating mechanism to ensure that the learned task classifier is temporally coherent. Introducing this structure is what enables the method to improve performance over the standard, non-multitask approach. The evaluation involves two toy experimental scenarios. The first involves controlling one of two cars to drive around a track. In this task, detecting the “task” is very easy, and the learned state representation is linear in the observation. The paper evaluates the performance of the policies learned with the proposed approach, and shows sufficient comparisons to demonstrate the usefulness of the approach over a standard non-multitask set-up. In the second navigation scenario, only the state representation is qualitatively shown, not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better, the paper should also evaluate on control, with the same comparisons as in the first experiment. Without this evaluation, the experiment is incomplete. Lastly, to be on par with publications at a venue like ICLR, the method should be evaluated more thoroughly, on a wider range of set-ups, to demonstrate the generality of the approach and show that the method applies to more complex tasks. While in theory, the method should scale, the experiments do not demonstrate that it can handle more realistic scenarios, such as scaling beyond MNIST-level images, to 3D or real images, or higher-dimensional control tasks. Evaluating the method in this more complex scenario is important, because unexpected issues can come up when trying to scale. If scaling-up is straight-forward, then running this experiment (and including it in the paper) should be straight-forward. In summary, here are the pros and cons of this paper: Cons - The approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task - Only one experimental set-up that evaluates learned policy with multi-task state representation - No experiments on more realistic scenarios, such 3D environments or high-dimensional control problems Pros: - This approach enables using the same network for multiple tasks, which is often not true for transfer and multi-task learning approaches - Novel way to learn a single policy for multiple tasks, including a task coherence prior which ensures that the task classification is meaningful - Experimentally validated on two toy tasks. One task shows improvement over baseline approaches Thus, my rating would be higher if the paper included an evaluation of the control policy for navigation and included another more challenging and compelling scenario. Lastly, here are some minor comments/questions on how I think the paper could be improved, but are not as important as the above: Approach: Could this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder. Experiments: One additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car), as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the “known car position” baseline (which is also useful in its own right). Does the “observations” baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance. If there are aliasing issues with the images, why not just use higher resolution images?  The paper presents a method to learn a low-dimensional state representations from raw obervation for multi-task setting. In contrast to classic multi-task learning setting where a joint representation is usually learned by exploring the transferable information among different tasks, the method aims to identify individual task and solve them separately. To this end, the authors extend the learning with robotic priors approach by extending the loss function with additional term for task coherence, i.e., a task only changes representation between training episodes. The method has been evaluated on two tasks, multi-task slot-car racing and mobile navigation to prove its efficacy. there were several unclear issues: 1. The first question is that if the method is only appealing on the scenario like the slot-car racing, otherwise it should be benchmarked with mutli-task learning. While the author made the argument in the related work, the proposed method is orthogonal to multi-task learning they did admit both explore shared knowledge between tasks. What*s the advantage and disadvantage for the proposed method for general mutiple task setting, in particular over the multi-task learning? The reply of the authors was not fully satisfactory. The argument did not support the lack of comparison to multi-task joint-learning. It seems they don*t plan to include any comparison neither. I think it*s important for the fundamental motivation for the work, without such comparison, the method seems to be purely an alternative to multi-task joint-learning without any(or much) practical advantage. 2.Following up to the previous question, please clarify the results on the mobile navigation scenario. It*s not clear how the plot on the right indicates MT-LRP identifies all tasks as the author claimed and and seems very weak to support the method, in particular compared to the multi-slot car-racing driving experiment, there is too little results to make sound argument (almost no comparison to alternative methods, i.e. no baseline method, is that true for the problem). The explanation of the authors did provide more details and more explicit information. 3. The proposed gated neural network architecture seems to be a soft gated structure(correct me if I am wrong), a possible baseline would be a hard gated unit, how would this affect the conclusion. This is particularly interesting as the authors reflect on the constraint that the representation should stay consistent during the training. The author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it*s probably less an issue compared to Question 1. In summary, while there are remaining concerns about lacking comparisons, the is a weak tendency towards accepting the submission.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",6,4,"Strengths:
1. The paper presents a novel approach to learning state representations for multiple tasks in reinforcement learning. The authors propose a novel approach combining gated neural networks with multitask learning with robotics priors. They evaluated their approach on two simulated datasets and showed promising results. The paper is clearly written and is theoretically sound. 
2. The paper is well-structured and easy to follow. The authors provide a good overview of the related work and the proposed method. The paper is well-written and free of errors. 

Weaknesses:
1. The parameters choice is arbitrary (w parameters). The authors should consider a more standardized way of picking the model parameters. 
2. The experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare. 
3. The method proposes to jointly learn a task classifier with a state representation learner, by using a differentiable gating mechanism to control the flow of information. The paper proposes a task coherence prior for this gating mechanism to ensure that the learned task classifier is temporally coherent. Introducing this structure is what enables the method to improve performance over the standard, non-multitask approach. The evaluation involves two toy experimental scenarios. The first involves controlling one of two cars to drive around a track. In this task, detecting the “task” is very easy, and the learned state representation is linear in the observation. The paper evaluates the performance of the policies learned with the proposed approach, and shows sufficient comparisons"
The Neural Noisy Channel | OpenReview,"We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. Unlike direct models which can suffer from explaining-away effects during training, noisy channel models must produce outputs that explain their inputs, and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. Using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol, we obtain a tractable and effective beam search decoder. Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use.","This paper proposes to use an SSNT model of p(x|y) to allow for a noisy channel model of conditional generation that (still) allows for incremental generation of y. The authors also propose an approximate search strategy for decoding, and do an extensive empirical evaluation. PROs: This paper is generally well written, and the SSNT model is quite interesting and its application here well motivated. Furthermore, the empirical evaluation is very well done, and the authors obtain good results. CONs: One might be concerned about whether the additional training and decoding complexity is warranted. For instance, one might plausibly obtain the benefits of the proposed approach by reranking (full) outputs from a standard seq2seq model with a score combining p(y|x), p(x|y), and p(y). (It*s worth noting that Li et al. (NAACL 2016) do something similar for conversation modeling). At the same time, being able to rerank during search may be helpful, and so it might be nice to see some experiments addressing this. Other Comments: - Given that the main thrust of the paper is to provide a model for p(x|y), the paper might be slightly clearer if Section 2 were presented from the perspective of modeling p(x|y) instead of switching back to p(y|x) as in the original Yu et al. paper. - It initially seems strange to suggest a noisy-channel model as a way of addressing the *explaining away* problem, since now you have an explicit, uncalibrated p(y) term. However, since seq2seq models appear to naturally do a lot of target-side language modeling, incorporating an explicit p(x|y) term seems quite clever.  The paper proposes an online variant of segment to segment transducers, which allows to circumvent the necessity of observing whole sentence, before making target predictions. Authors mostly build on their previous work, allowing additionally to leverage independent priors on the target hypotheses, like the language grammar or sentence length. Strong points: - well written, interesting idea of combining various sources of information in a Bayesian framework for seq2seq models Handling something in an online manner typically makes things more difficult, and this is what the authors are trying to do here - which is definitely of interest to the community - strong experimental section, with some strong results (though not complete: see weak points) Weak points: - Authors do not improve on computational complexity (w.r.t Tillmann proposal), hence the algorithms may be found difficult to apply in scenarios where inputs may be long (this already takes into account a rather constrained model of alignment latent variables) - What about the baseline where you only combine direct, LM and bias contributions (no channel)? Was there any (non-obvious) algorithmic constraint why - this has not been included? Some other (minor) comments: - Related to the first weak point: can you elaborate more on how the clue of your work is conceptually different from the work of Tillmann et al. (1997) (except, of course, the fact you use connectionist discriminative models to derive particular conditional probabilities). - How sensitive is the model to different choices of hyper-parameters in eq (3). Do you naively search through the search space of those, or do something more clever? - Some more comments on details of the auxiliary direct model would be definitely of interest. - How crucial is the correct choice of the pruning variables (K1 and K2)? - Sec. 2: makes no Markovian assumptions -> no first-order Markovian assumption? Typos: Table 1: chanel -> channel (one before last row) Apologies for late review.  This paper proposes the neural noisy channel model, P(x|y), where (x, y) is a input-to-out sequence pair, based on the authors* previous work on segment to segment neural transduction (SSNT) model. For the noisy channel model, the key difference from sequence-to-sequence is that the complete sequence y is not observed beforehand. SSNT handles this problem elegantly by performing incremental alignment and prediction. However, this paper does not present anything that is particular novel on top of the SSNT. The SSNT model is still applicable by reverting the input and output sequences. The authors said that an unidirectional LSTM has to be used as an encoder instead of the bidirectional LSTM, but I think the difference is minor. The decoding algorithm presented in the appendix is relatively new. The experimental study is very comprehensive and strong, however, there is one important baseline number that is missing for all the experiments. Can you give the number that uses direct + LM + bias, and if you can give direct + bias number would be even better. Although using a LM for the direct model does not make a lot of sense mathematically, however, it works pretty well in practice, and the LM can rescore and smooth your predictions, see Deep Speech 2: End-to-End Speech Recognition in English and Mandarin from Baidu for example. I think the LM may be also the key to explain why noisy channel is much better than direct model in Table 3. A couple minor questions are 1. it is not very clear to me is your direct model in the experiments SSNT or sequence-to-sequence model? 2. O(|x|^2*|y|) training complexity is OK, but it would be great to further cut down the computational cost, as it is still very expensive for long input sequences, for example, for paragraph or document level modeling, or speech sequences. The paper is well written, and overall, it is still an interesting paper, as the channel model is always of great interest to the general public.","['Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",7,4,"Strengths: 1. The paper proposes a novel model for sequence-to-sequence transduction, which is a very active area of research. 2. The authors provide a clear and well-written description of the model, and the experimental results are very comprehensive. 3. The paper is well-structured and easy to follow. Weaknesses: 1. The paper does not present anything that is particularly novel on top of the previous work on segment-to-sequence neural transduction (SSNT) model. 2. The SSNT model is still applicable by reverting the input and output sequences. 3. The decoding algorithm presented in the appendix is relatively new, but it is not clear how it is used in practice. 4. The paper does not provide any clear experimental results for the baseline model, which is a direct model with LM and bias. 5. The paper does not provide any clear experimental results for the baseline model, which is a direct model with LM and bias. 6. The paper does not provide any clear experimental results for the baseline model, which is a direct model with LM and bias. 7. The paper does not provide any clear experimental results for the baseline model, which is a direct model with LM and bias. 8. The paper does not provide any clear experimental results for the baseline model, which is a direct model with LM and bias. 9. The paper does not provide any clear experimental results for the baseline model"
New Learning Approach By Genetic Algorithm In A Convolutional Neural Network For Pattern Recognition | OpenReview,"Almost all of the presented articles in the CNN are based on the error backpropagation algorithm and calculation of derivations of error, our innovative proposal refers to engaging TICA filters and NSGA-II genetic algorithms to train the LeNet-5 CNN network. Consequently, genetic algorithm updates the weights of LeNet-5 CNN network similar to chromosome update. In our approach the weights of LeNet-5 are obtained in two stages. The first is pre-training and the second is fine-tuning. As a result, our approach impacts in learning task.","The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network (LeNet-5 to be precise). The algorithm is validated on some version of the MNIST dataset. Unfortunately the paper is extremely hard to understand and it is not at all clear what the exact training algorithm is. Neither do the authors ever motivate why do such a training as opposed to the standard back-prop. What are its advantages/dis-advantages? Furthermore the experimental section is equally unclear. The authors seem to have merged the training and validation set of the MNIST dataset and use only a subset of it. It is not clear why is that the case and what subset they use. In addition, to the best of my understanding, the results reported are RMSE as opposed to classification error. Why is that the case? In short, the paper is extremely hard to follow and it is not at all clear what the training algorithm is and how is it better than standard way of training. The experimental section is equally confusing and unconvincing. Other comments: -- The figures still say LeCun-5 -- The legends of the plots are not in english. Hence I*m not sure what is going on there. -- The paper is riddled with typos and hard to understand phrasing.  Unfortunately, this paper is very difficult to understand. The current version of this paper seems improved compared to the initial version, but still far from a finished level. I*d encourage the authors to keep editing over the language and presentation. I also think it would be good to also try answering some of the following questions very clearly in the paper: - What is the advantage, if any, of the proposed algorithm over SGD? What is the motivation and goal of the work beyond MNIST benchmarking? - Why are few training examples used? Is this a scenario in which the system might have an advantage? - Concretely describe the genetic algorithms terminology used in the algorithm descriptions, and what each term means in the context of the convolutional network. - Try to make sure that the method, as described, can be understood by a reader without much prior background on genetic algorithms. - A single experiment on MNIST is too small to adequately describe the algorithm performance. Consider using a second or third dataset and/or experimental application. Much work is still needed on the paper*s writing before it can be understood well enough. I hope that some of this might be useful in helping to improve. I would encourage the authors to try to find outside readers, preferably fluent in English, to work with on a frequent basis before resubmitting to another venue.  The paper is still extremely poorly written and presented despite multiple reviewers asking to address that issue. The frequent spelling mistakes and incoherent sentences and unclear presentation make reading and understanding the paper very difficult and time consuming. Consider getting help from someone with good english and presentation skills.","['Rating:###3: Clear rejection', 'Rating:###3: Clear rejection', 'Rating:###2: Strong rejection']","['Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",3,5,Strengths: 1. The authors propose a novel approach to training a convolutional neural network (CNN) by using genetic algorithms. 2. The authors provide a clear and concise description of the proposed algorithm. Weaknesses: 1. The paper is extremely hard to understand and is riddled with typos and unclear phrasing. 2. The experimental section is unclear and does not provide a clear motivation for the proposed algorithm. 3. The paper does not provide a clear motivation for the proposed algorithm and its advantages over standard backpropagation. 4. The paper does not provide a clear description of the genetic algorithms terminology used in the algorithm descriptions. 5. The paper does not provide a clear description of the training process and the parameters used. 6. The paper does not provide a clear description of the performance metric used. 7. The paper does not provide a clear description of the dataset used. 8. The paper does not provide a clear description of the experimental setup. 9. The paper does not provide a clear description of the results and their significance. 10. The paper does not provide a clear description of the limitations of the proposed algorithm. Rating: 4 Confidence: 2
An Actor-Critic Algorithm for Sequence Prediction | OpenReview,"We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.","This paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing. In particular, experiments are shown in a synthetic denoising task as well as in machine translation. I like the idea of the paper, however, the experimental evaluation is not convincing. Why is the LL numbers in Ranzato et al. 2015 and your paper so different? Is the metric different? is it the scheduler? are the parameters different? If one extrapolates the numbers, it seems that MIXER will be much better than the proposed approach. I*d like to see a head-to-head comparison, either by reproducing the same setting or by running the mixer baseline. The authors should also compare their results to the state-of-the-art. How good is their machine translation system? Only comparing to a single baseline and without reproducing the numbers is not sufficient. While the idea makes sense, the authors needed to use many heuristics to make the model to work, e.g., using a delayed actor, update phi* with interpolation, penalize the variance, reducing the value of rare actions, etc. Furthermore, there is no in depth analysis of how much performance each of these heuristics brings. It seems that the authors need more work to make the model work without so many heuristics. The authors also mentioned several optimization difficulties (some of which are non-intuitive), 1) why does the critic assign very high value to actions with very low probability according to the actor? 2) why is a lower square error on Q resulting in much worst performance? The paper will benefit from a serious re-write. The technical part is not clearly written. The manuscript also assumes that the reader knows algorithms such as REINFORCE. I strongly suggest to include a brief description in the text. This will help the reader understand how to use the critic within this framework. Also the experimental section will benefit from dividing it by experiment. Right now is cumbersome to look at the details of each experiment as things are mixed up in the text. The paper criticizes the REINFORCE algorithm a lot, particularly for its high variance, however the best results in the real setting are achieved with this algorithm (+ the critic). How do you explain this? The text is also not consistent with what the results show. The discussion claims that using the critic on REINFORCE reduces the gap with the actor critic. However, it is better than the proposed approach. I*ll revise my score if the authors address my questions. In summary, an interesting idea, however many heuristics are used and the experimental evaluation is not sufficient.  This paper introduces an actor-critic approach for sequence prediction, and shows experiments on spelling correction and machine translation. While previous works e.g. Ranzato et al. 2015 have used an RL-based approach such as REINFORCE for sequence prediction, the main contribution of this work is the use of actor-critic as a novel approach for how to determine the target of network predictions, given the setting that the network should be trained to generate correctly given outputs already produced by the model and not ground-truth reference outputs. Specifically, the actor is the main prediction network and the critic is trained to output the value of specific tokens. The motivations for the approach are well-presented, and while a somewhat natural extension, it is still novel and justified. There are a number of details that are necessary for successful training, that are discussed well. While the full Actor-Critic model does not show strong improvements over REINFORCE with critic, the critic-based models still outperform other baselines. It would be nice to include more discussion of the bias-variance tradeoff and future advantages of Actor-Critic (from the pre-review question response) in the paper. The paper is solid and deserves acceptance  The paper presents a nice application of actor-critic method for conditional sequence prediction. The critic is trained conditional to target sequence output, while the actor is conditional on input sequence. The paper presents a number of interesting design decisions in order to tackle non-standard RL problem with actor-critic (conditional sequence generation with sequence-level reward function, large action space, reward at final step) and shows encouraging results for applying RL in sequence prediction. The interaction of actor and critic is an interesting aspect of this paper. Each has different pieces of information (input sequence, target output sequence), and effectively the actor gets target label information only through greedy optimization of the critic. Letting the critic having access to information only available at train time is interesting and may be applicable to other applications that tie RL with supervised learning. Pre-review discussion on Q-learning vs actor-critic has been good, and indeed I agree that making the critic having access to structured output label may be quite useful. The pros include reasonable improvement over prior attempts at using RL to fine-tune sequence models. One possible con is that the actor-critic is likely more unstable than simpler prior methods, thus requiring a number of tricks to alleviate, and it would be nice to see discussion on stability and hyper-parameter sensitivity. Another possible con is that this is an application paper, but it explores a non-traditional approach in a widely applicable field.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###8: Top 50% of accepted papers, clear accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",4,4,"Strengths:
1. The paper presents a novel approach to sequence prediction by using actor-critic methods from reinforcement learning (RL) to train a sequence-to-sequence model. The approach is interesting and worth exploring.
2. The paper provides a clear and well-structured presentation of the approach and its application to machine translation.
3. The authors provide a good discussion of the pros and cons of the approach and its potential applications.

Weaknesses:
1. The paper does not provide a clear and rigorous mathematical formulation of the problem and the approach. The discussion of the actor-critic algorithm is limited and lacks a clear connection to the RL literature.
2. The paper does not provide a clear and rigorous analysis of the stability and convergence of the approach. The discussion of the importance of the critic having access to structured output labels is interesting, but lacks a clear mathematical justification.
3. The paper does not provide a clear and rigorous evaluation of the approach. The comparison to prior works is limited and lacks a clear discussion of the pros and cons of the approach.
4. The paper does not provide a clear and rigorous discussion of the potential applications of the approach. The discussion of the potential applications is limited and lacks a clear connection to the RL literature.
5. The paper does not provide a clear and rigorous discussion of the hyper-parameter sensitivity of the approach. The discussion of the importance of the critic having access to structured output labels is interesting, but lacks a clear mathematical justification.

Rating: 6"
Joint Training of Ratings and Reviews with Recurrent Recommender Networks | OpenReview,"Accurate modeling of ratings and text reviews is at the core of successful recommender systems. While neural networks have been remarkably successful in modeling images and natural language, they have been largely unexplored in recommender system research. In this paper, we provide a neural network model that combines ratings, reviews, and temporal patterns to learn highly accurate recommendations. We co-train for prediction on both numerical ratings and natural language reviews, as well as using a recurrent architecture to capture the dynamic components of users* and items* states. We demonstrate that incorporating text reviews and temporal dynamic gives state-of-the-art results over the IMDb dataset.","This paper proposes an RNN-based model for recommendation which takes into account temporal dynamics in user ratings and reviews. Interestingly, the model infers time-dependant user/item vectors by applying an RNN to previous histories. Those vectors are used to predict ratings, in a similar fashion to standard matrix factorization methods, and also bias a conditional RNN language model for reviews. The paper is well written and the architectural choices make sense. The main shortcomings of the paper are in the experiments: 1) The full model (rating+text) is only applied to one and relatively small dataset. Applying the model on multiple datasets with more data, e.g. Amazon reviews dataset (https://snap.stanford.edu/data/web-Amazon.html) would be more convincing. 2) While modelling order in review text seems like the right choice, previous papers (e.g. Almahairi et al. 2015) have shown that for rating prediction, modelling order in reviews might not be useful. A comparison with a similar model, but with bag-of-words reviews model would be nice in order to show the importance of the RNN-based review model, especially given previous literature. Finally, this paper is an application paper applying well-established deep learning techniques, and I do not feel the paper offers new insights which the ICLR community in general would benefit from. This is not to undermine the importance of this paper, but I would like the authors to comment on why they think ICLR is a good avenue for their work.  This paper proposed a joint model for rate prediction and text generation. The author compared the methods on a more realistic time based split setting, which requires “predict into the future.” One major flaw of the paper is that it does not address the impact of BOW vs the RNN based text model, specifically RRN(rating+text) already uses RNN for text modeling, so it is unclear whether the improvement comes from RNN(as opposed to BOW) or application of text information. A more clear study on impact of each component could make it more clear and benefit the readers. Another potential improvement direction of the paper is to support ranking objectives, as opposed to rate prediction, which is more realistic for recommendation settings. The overall technique is intuitive and novel, but can be improved to give more insights to the reader,.  The paper seeks to jointly model ratings and reviews in addition to temporal patterns. Existing papers have captured these aspects previously, but what*s novel here is the particular combination of parts and choice of techniques. In particular, the use of LSTMs as opposed to *bag-of-words* models that have previously been used when combining the same components. A criticism is made of existing models that use bag-of-words features as being too *coarse* to capture the real dynamics of reviews. This seems a valid criticsm, though it*s not clear exactly what features those existing models miss. Something missing from this paper is any interpretation of what the model *learns* that may explain its better performance. I also don*t know about the significance of predicting *future* ratings as opposed to random splitting of the data. Lots of work on temporal recommender systems uses a variety of hold-out strategies besides random sampling, e.g. holding out the final ratings. Is there a methodological contribution associated with this change? The experiments evaluate the extent to which the model achieves good performance due to its ability to capture evolving temporal patterns at the level of items, and the ability of the model to capture additional dynamics from text. Text makes a small but significant contribution; I was surprised by how large an improvement is achieved given how little text was included. Overall this is a reasonably strong experimental comparison, though could be improved in two dimensions: (a) There*s no comparison to models that use ratings + text (e.g. CoBaFi, JMARS, etc.). These methods have reported substantial improvements over rating-only models in the past, perhaps even larger improvements than what is reported here. Such an experiment is important given the claim that existing models (in particular bag-of-words models) are too coarse to capture the dynamics of text. (b) The restriction to movie datasets is okay, but these datasets are somewhat outliers when it comes to recommendation. In particular, they*re extremely dense datasets that support very parameter-rich models. I*d question whether the results would hold on sparser data (though in fact I*d suspect they would hold, since on a sparser dataset the contribution due to using reviews ought to be larger). Otherwise the experiments are fine. Perplexity results are nice but essentially what we*d expect. It is a shame that unlike other papers that use text to inform recommender systems there*s no high-level analysis here of what the model has uncovered.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",6,4,Strengths: 1. The paper proposes a novel approach to jointly modeling ratings and reviews with temporal patterns. 2. The authors provide a clear and well-written description of the model and its components. 3. The paper is well-structured and easy to follow. 4. The authors provide a clear and well-written description of the model and its components. Weaknesses: 1. The paper does not provide a clear comparison to existing methods. 2. The paper does not provide a clear analysis of the model*s performance. 3. The paper does not provide a clear explanation of the model*s strengths and weaknesses. 4. The paper does not provide a clear direction for future work. Rating: 7 Confidence: 4
What Is the Best Practice for CNNs Applied to Visual Instance Retrieval? | OpenReview,"Previous work has shown that feature maps of deep convolutional neural networks (CNNs) can be interpreted as feature representation of a particular image region. Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years. The key to the success of such methods is the feature representation. However, the different factors that impact the effectiveness of features are still not explored thoroughly. There are much less discussion about the best combination of them. The main contribution of our paper is the thorough evaluations of the various factors that affect the discriminative ability of the features extracted from CNNs. Based on the evaluation results, we also identify the best choices for different factors and propose a new multi-scale image feature representation method to encode the image effectively. Finally, we show that the proposed method generalises well and outperforms the state-of-the-art methods on four typical datasets used for visual instance retrieval.","The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks. Technically, the contribution is clear, particularly with the promised clarifications on how multiple scales are handled in the representation. However, I am still not entirely clear whether there would be a difference in the multi-scale settting for full and cropped queries. While the paper focuses on comparing different baseline architectures for CNN-based image retrieval, several recent papers have proposed to learn end-to-end representations specific for this task, with very good result (see for instance the recent work by Gordo et al. *End-to-end Learning of Deep Visual Representations for Image Retrieval*). The authors clarify that their work is orthogonal to papers such as Gordo et al. as they assess instead the performance of networks pre-trained from image classification. In fact, they also indicate that image retrieval is more difficult than image classification -- this is because it is performed by using features originally trained for classification. I can partially accept this argument. However, given the results in recent papers, it is clear than end-to-end training is far superior in practice and it is not clear the analysis developed by the authors in this work would transfer or be useful for that case as well.  Authors investigate how to use pretrained CNNs for retrieval and perform an extensive evaluation of the influence of various parameters. For detailed comments on everything see the questions I posted earlier. The summary is here: I don*t think we learn much from this paper: we already knew that we should use the last conv layer, we knew we should use PCA with whitening, we knew we should use original size images (authors say Tolias didn*t do this as they resized the images, but they did this exactly for the same reason as authors didn*t evaluate on Holidays - the images are too big. So they basically used *as large as possible* image sizes, which is what this paper effectively suggests as well), etc. This paper essentially concatenates methods that people have already used, and performs some more parameter tweaking to achieve the state-of-the-art (while the tweaking is actually performed on the test set of some of the tests). The setting of the state-of-the-art results is quite misleading as it doesn*t really come from the good choice of parameters, but mainly due to the usage of the deeper VGG-19 network. Furthermore, I don*t think it*s sufficient to just try one network and claim these are the best practices for using CNNs for instance retrieval - what about ResNet, what about Inception, I don*t know how to apply any of these conclusions for those networks, and would these conclusions even hold for them. Furthermore the parameter tweaking was done on Oxford, I really can*t tell what conclusions would we get if we tuned on UKB for example. So a more appropriate paper title would be *What are the best parameter values for VGG-19 on Oxford/Paris benchmarks?* - I don*t think this is sufficiently novel nor interesting for the community.  This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification (e.g. VGG), and post-process them for image retrieval. In other words, the network is off-the-shelf and solely acts as a feature extractor. The post-processing strategies are borrowed from traditional retrieval pipelines relying on hand-crafted features (e.g. SIFT + Fisher Vectors), denoted by the authors as *traditional wisdom*. Specifically, the authors examine where to extract features in the network (i.e. features are neurons activations of a convolution layer), which type of feature aggregation and normalization performs best, whether resizing images helps, whether combining multiple scales helps, and so on. While this type of experimental study is reasonable and well motivated, it suffers from a huge problem. Namely it *ignores* 2 major recent works that are in direct contradictions with many claims of the paper ([a] *End-to-end Learning of Deep Visual Representations for Image Retrieval* by Gordo et al. and [b] *CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples* by Radenovi? et al., both ECCV*16 papers). These works have shown that training for retrieval can be achieved with a siamese architectures and have demonstrated outstanding performance. As a result, many claims and findings of the paper are either outdated, questionable or just wrong. Here are some of the misleading claims: - *Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years.* Until [a] (not cited), the state-of-the-art was still largely dominated by sparse invariant features based methods (see last Table in [a]). - *the proposed method [...] outperforms the state-of-the-art methods on four typical datasets* That is not true, for the same reasons than above, and also because the state-of-the-art is now dominated by [a] and [b]. - *Also in situations where a large numbers of training samples are not available, instance retrieval using unsupervised method is still preferable and may be the only option.*. This is a questionable opinion. The method exposed in *End-to-end Learning of Deep Visual Representations for Image Retrieval* by Gordo et al. outperforms the state-of-the-art on the UKB dataset (3.84 without QE or DBA) whereas it was trained for landmarks retrieval and not objects, i.e. in a different retrieval context. This demonstrates that in spite of insufficient training data, training is still possible and beneficial. - Finally, most findings are not even new or surprising (e.g. aggregate several regions in a multi-scale manner was already achieved by Tolias at al, etc.). So the interest of the paper is limited overall. In addition, there are some problems in the experiments. For instance, the tuning experiments are only conducted on the Oxford dataset and using a single network (VGG-19), whereas it is not clear whether these conditions are well representative of all datasets and all networks (it is well known that the Oxford dataset behaves very differently than the Holidays dataset, for instance). In addition, tuning is performed very aggressively, making it look like the authors are tuning on the test set (e.g. see Table 3). To conclude, the paper is one year too late with respect to recent developments in the state of the art.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###3: Clear rejection', 'Rating:###3: Clear rejection']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",6,4,"Strengths:
1. The paper is well written and easy to follow.
2. The authors provide a clear motivation for their work.
Weaknesses:
1. The paper is based on a very old state-of-the-art, and the authors do not even mention it in the paper. This is a problem because the paper is claiming to be state-of-the-art, which is not true. The authors should mention the state-of-the-art and explain why their results are better.
2. The paper is based on a very small dataset (Oxford) and a very large dataset (Holidays). This is a problem because the authors are claiming that their results are generalizable to other datasets, which is not true. The authors should mention the results on the Holidays dataset and explain why they are not generalizable.
3. The paper is based on a very simple and naive approach. The authors are using a pre-trained network and then tuning some parameters. This is a problem because the authors are not exploring any new ideas or approaches. The authors should mention the limitations of their approach and explain why it is not better than other approaches.
4. The paper is not well-structured. The authors are jumping between different ideas and approaches without a clear thread. This is a problem because the authors are not providing a clear and coherent argument. The authors should reorganize their paper to make it more clear and concise.
Rating: 4
Confidence: 2"
Learning Efficient Algorithms with Hierarchical Attentive Memory | OpenReview,"In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM). It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in O(log n) complexity, which is a significant improvement over the standard attention mechanism that requires O(n) operations, where n is the size of the memory. We show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples. In particular, it learns to sort n numbers in time O(n log n) and generalizes well to input sequences much longer than the ones seen during the training. We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue.","This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells. This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks such as sorting from pure input-output examples and dealing with longer sequences. The idea of the paper is novel and well-presented, and the memory structure seems reasonable to have advantages in practice. However, the main weakness of the paper is the experiments. There is no experimental comparison with other external memory-based approaches (e.g. those discussed in Related Work), or experimental analysis of computational efficiency given overhead costs (beyond just computational complexity) despite that being one of the main advantages. Furthermore, the experimental setups are relatively weak, all on artificial tasks with moderate increases in sequence length. Improving on these would greatly strengthen the paper, as the core idea is interesting.  This paper proposes to use a hierarchical softmax to speed up attention based memory addressing in memory augmented network (e.g. NTM, memNN…). The model build a hierarchical softmax on top of the input sequence then at each time step SEARCH for the most relevant input to predict the next output (this search is discrete), and use its corresponding embedding to update the state of an LSTM that will then produce the output. Finally the embedding of the used input is update by a WRITE function (an LSTM working that takes hidden state of the other LSTM as an input). The model has a discrete component (the SEARCH) and is thus trained with REINFORCE. In the experimental section they test their approach on several algorithmic tasks such as search, sort... The main advantage of replacing the full softmax by a hierarchical softmax is that during inference, the complexity goes from O(N) to O(log(N)). It would be great to see if the gain in complexity allows to tackle problem which are a few orders of magnitude bigger than the one addressed with full softmax. However the authors only test on toy sequences up to 32 tokens, which is quite small. The model requires a relatively complex search mechanism that can only be trained with REINFORCE. While this seems to work on problems with relatively small and simple sequences, it would be great to see how performance changes with the size of the problem. Overall, while the idea of replacing the softmax in the attention mechanism by a hierachical softmax is appealing, this work is not quite convincing yet. Their approach is not very natural, may be hard to train and may not be that simple to scale. The experiment section is very weak.  The authors introduce a new memory model which allows memory access in O(log n) time. Pros: * The paper is well written and everything is clear. * It*s a new model and I*m not aware of a similar model. * It*s clear that memory access time is an issue for longer sequences and it is clear how this model solves this problem. Cons: * The motivation for O(log n) access time is to be able to use the model on very long sequences. While it is clear from the definition that the computation time is low because of its design, it is not clear that the model will really generalize well to very long sequences. * The model was also not tested on any real-world task. I think such experiments should be added to show whether the model really works on long sequences and real-world tasks, otherwise it is not clear if this is a useful model.","['Rating:###5: Marginally below acceptance threshold', 'Rating:###3: Clear rejection', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",5,5,"Strengths: 1. The authors introduce a novel memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells. This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks such as sorting from pure input-output examples and dealing with longer sequences. 2. The idea of the paper is novel and well-presented, and the memory structure seems reasonable to have advantages in practice. Weaknesses: 1. The main weakness of the paper is the experiments. There is no experimental comparison with other external memory-based approaches (e.g. those discussed in Related Work), or experimental analysis of computational efficiency given overhead costs (beyond just computational complexity) despite that being one of the main advantages. Furthermore, the experimental setups are relatively weak, all on artificial tasks with moderate increases in sequence length. Improving on these would greatly strengthen the paper, as the core idea is interesting. 2. The experimental results are not very convincing. The authors only test their approach on toy sequences up to 32 tokens, which is quite small. It would be great to see how performance changes with the size of the problem. Overall, the paper is well written and the idea of the memory architecture is interesting. However, the experimental results are not very convincing. The authors only test their approach on toy sequences up to 32 tokens, which is quite small. It would be great to see how performance changes with the size of the problem."
A Learned Representation For Artistic Style | OpenReview,"The diversity of painting styles represents a rich visual vocabulary for the construction of an image. The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style.","This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles (as is done, e.g., in Johnson et al.), this paper extends the instance normalization work of Ulyanov et al. to train a single network and learn a smaller set “conditional instance normalization” parameters dependent on the desired output style. The conditional instance normalization applies a learnt affine transformation on normalized feature maps at each layer in the network. Qualitative results are shown. I have not worked in this area, but I’m generally aware of the main issues in transferring artistic style. The paper addresses a known challenge of incorporating different styles into the same net, which have a number of practical benefits. As far as I can tell the results look compelling. As I’m less confident in my expertise in this area, I’m happy to support another reviewer who is willing to champion this paper. My main comments are on the paper writing. As far as I understand, the main novelty of the approach starts in Section 2.2, and before that is review of prior art. If this is indeed the case, one suggestion is to remove the subsection heading for 2.1 so it’s grouped with the first part of Section 2, and to cite related work for the feedforward network (e.g., Johnson et al.) in the text and in Fig 2 so it’s clear. In fact, I’m wondering if Figs 2 and 3 can be combined somehow so that the contribution is clearer in the figures. I was at first confused by Eq (5) as x and z are not defined anywhere. Also, it may be helpful to write out everything explicitly as is done in the instance normalization paper. In Eq (4), perhaps you could write T_s to emphasize that there are separate networks for different styles. Fig 5 left: I’m assuming the different colors correspond to the different styles. If so, perhaps mention this in the caption. Also, this figure is hard to read. Maybe instead show single curves with error bars that are averages over the loss curves for N-styles and individual styles. Typos: Page 1: Shouldn’t it be “VGG-16” network (not *VGG-19”)? Page 2: “newtork” => “network”. Paragraph after Eq. (5): “much less” => “fewer”.  The paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles. This is achieved by a global, style-dependent scale and shift parameter for each feature in the network. Thus image style is encoded in a very condensed subset of the network parameters, with only two parameters per feature map. This enables to easily incorporate new styles into an existing network by fine-tuning. At the same time, the quality of the generated stylisations is comparable to existing feed-forward single-style transfer networks. While this also means that the stylisation results in the paper are limited by the quality of current feed-forward methods, the proposed method seems general enough to be combined with future improvements in feed-forward style transfer. Finally, the paper shows that having multiple styles encoded in one feature space allows to gradually interpolate between different styles to generate new mixtures of styles. This is comparable to interpolating between the Gram Matrices of different style images in the iterative style transfer algorithm by Gatys et al. and comes with similar limitations: Right now the parameters of the style feature space are hard to interpret and therefore there is little control over the stylisation outcome when moving in that feature space. Here I see the most potential for improvement of the paper: The parameterisation of style in terms of scale and shift parameters of individual features seems like a promising basis to achieve interpretable style features. It would be a great addition to explore to what extend statements such as “The parameters of neuron N in layer L encodes e.g. the colour or brush-strokes of the styles” can be made. I agree that this is a potentially laborious endeavour, but even just qualitative statements of this kind that are demonstrated with the respective manipulations in the stylisation would be very interesting. In conclusion, this is a good paper presenting an elegant and valuable contribution that will have considerable impact on the design of feed-forward stylisation networks.  CONTRIBUTIONS The authors propose a simple architectural modification (conditional instance normalization) for the task of feedforward neural style transfer that allows a single network to apply many different styles to input images. Experiments show that the proposed multi-style networks produce qualitatively similar images as single-style networks, train as fast as single-style networks, and achieve comparable losses as single-style networks. In addition, the authors shows that new styles can be incrementally added to multi-style networks with minimal finetuning, and that convex combinations of per-style parameters can be used for feedforward style blending. The authors have released open-source code and pretrained models allowing others to replicate the experimental results. NOVELTY The problem setup is very similar to prior work on feedforward neural style transfer, but the paper is the first to my knowledge that uses a single network to apply different styles to input images; the proposed conditional instance normalization layer is also novel. This paper is also the first that demonstrates feedforward neural style blending; though not described in published literature, optimization-based neural style blending had previously been demonstrated in https://github.com/jcjohnson/neural-style. MISSING CITATION The following paper was concurrent with Ulyanov et al (2016a) and Johnson et al in demonstrating feedforward neural style transfer, though it did not use the Gram-based formulation of Gatys et al: Li and Wand, *Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks*, ECCV 2016 CLARITY The paper is very well written and easy to follow. SIGNIFICANCE Though simple, the proposed method is a significant addition to the growing field of neural style transfer. Its benefits are especially clear for mobile applications, which are often constrained in both disk space and bandwidth. Using the proposed method, only a single trained network needs to be transmitted and stored on the mobile device; in addition the ability of the proposed method to incrementally learn new styles means that new styles can be added by transmitting only a small number of new style-specific parameters to a mobile device. EVALUATION Like many other papers on neural style transfer, the results are mostly qualitative. Following existing literature, the authors use style and content loss as a quantitative measure of quality, but these metrics are unfortunately not always well-correlated with the perceptual quality of the results. I find the results of this paper convincing, but I wish that this and other papers on this topic could find a way to evaluate their results more quantitatively. SUMMARY The problem and method are slightly incremental, but the several improvements over prior work make this paper a significant addition to the growing literature on neural style transfer. The paper is well-written and its experiments convincingly validate the benefits of the method. Overall I believe the paper would be a valuable addition to the conference. Pros - Simple modification to feedforward neural style transfer with several improvements over prior work - Strong qualitative results - Well-written - Open-source code has already been released Cons - Slightly incremental - Somewhat lacking in quantitative evaluation, but not any more so than prior work on this topic","['Rating:###7: Good paper, accept', 'Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###8: Top 50% of accepted papers, clear accept']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",7,3,"Strengths:
1. The paper addresses a known challenge of incorporating different styles into the same net, which have a number of practical benefits. As far as I can tell the results look compelling. 
2. The paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles. This is achieved by a global, style-dependent scale and shift parameter for each feature in the network. Thus image style is encoded in a very condensed subset of the network parameters, with only two parameters per feature map. This enables to easily incorporate new styles into an existing network by fine-tuning. At the same time, the quality of the generated stylisations is comparable to existing feed-forward single-style transfer networks. While this also means that the stylisation results in the paper are limited by the quality of current feed-forward methods, the proposed method seems general enough to be combined with future improvements in feed-forward style transfer. Finally, the paper shows that having multiple styles encoded in one feature space allows to gradually interpolate between different styles to generate new mixtures of styles. This is comparable to interpolating between the Gram Matrices of different style images in the iterative style transfer algorithm by Gatys et al. and comes with similar limitations: Right now the parameters of the style feature space are hard to interpret and therefore there is little control over the stylisation outcome when moving in that feature space. Here I see the most potential for improvement of the paper: The parameterisation of style in terms of"
Dynamic Steerable Frame Networks | OpenReview,"Filters in a convolutional network are typically parametrized in a pixel basis. As an orthonormal basis, pixels may represent any arbitrary vector in Rn. In this paper, we relax this orthonormality requirement and extend the set of viable bases to the generalized notion of frames. When applying suitable frame bases to ResNets on Cifar-10+ we demonstrate improved error rates by substitution only. By exploiting the transformation properties of such generalized bases, we arrive at steerable frames, that allow to continuously transform CNN filters under arbitrary Lie-groups. Further allowing us to locally separate pose from canonical appearance. We implement this in the Dynamic Steerable Frame Network, that dynamically estimates the transformations of filters, conditioned on its input. The derived method presents a hybrid of Dynamic Filter Networks and Spatial Transformer Networks that can be implemented in any convolutional architecture, as we illustrate in two examples. First, we illustrate estimation properties of steerable frames with a Dynamic Steerable Frame Network, compared to a Dynamic Filter Network on the task of edge detection, where we show clear advantages of the derived steerable frames. Lastly, we insert the Dynamic Steerable Frame Network as a module in a convolutional LSTM on the task of limited-data hand-gesture recognition from video and illustrate effective dynamic regularization and show clear advantages over Spatial Transformer Networks. In this paper, we have laid out the foundations of Frame-based convolutional networks and Dynamic Steerable Frame Networks while illustrating their advantages for continuously transforming features and data-efficient learning.","This paper presents an improved formulation of CNN, aiming to separate geometric transformation from inherent features. The network can estimate the transformation of filters given the input images. This work is based on a solid technical foundation and is motivated by a plausible rationale. Yet, the value of this work in practice is subject to questions: (1) It relies on the assumption that the input image is subject to a transformation on a certain Lie group (locally). Do such transformations constitute real challenges in practice? State-of-the-art CNNs, e.g. ResNet, are already quite resilient to such local deformations. What such components would add to the state of the art? Limited experiments on Cifar-10 does not seem to provide a very strong argument. (2) The computational cost is not discussed.  I sincerely apologize for the late review! The first part has a strong emphasis on the technical part. It could benefit from some high level arguments on what the method aims to achieve, what limitation is there to overcome. I may have misunderstood the contribution (in which case please correct me) that the main novel part of the paper is the suggestion to learn the group parameterizations instead of pre-fixing them. So instead of applying it to common spatial filters as in De Brabandere et al., it is applied to Steerable Frames? The first contribution suggests that *general frame bases are better suited to represent sensory input data than the commonly used pixel basis.*. The experiments on Cifar10+ indicate that this is not true in general. Considering the basis as a hyper-parameter, expensive search has to be conducted to find that the Gauss-Frame gives better results. I assume this does not suggest that the Gauss-Frame is always better, at least there is weak evidence on a single network presented. Maybe the first contribution has to be re-stated. Further is the *Pixel* network representation corrected for the larger number of parameters. As someone who is interested in using this, what are the runtime considerations? I would strongly suggest to improve Fig.3. The Figure uses *w* several times in different notations and depictions. It mixes boxes, single symbols and illustrative figures. It took some time to decipher the Figure and its flow. Summary: The paper is sufficiently clear, technical at many places and readability can be improved. E.g., the introduction of frames in the beginning lacks motivation and is rather unclear to someone new to this concept. The work falls in the general category of methods that impose knowledge about filter transformations into the network architecture. For me that has always two sides, the algorithmic and technical part (there are several ways to do this) and the practical side (should I do it)? This is a possible approach to this problem but after the paper I was a bit wondering what I have learned, I am certainly not inspired based on the content of the paper to integrate or build on this work. I am lacking insights into transformational parameters that are relevant for a problem. While the spatial transformer network paper was weaker on the technical elegance side, it provided exactly this: an insight into the feature transformation learned by the algorithm. I am missing this here, e.g., from Table 2 I learn that among four choices one works empirically better. What is destroyed by the x^py^p and Hermite frames that the ResNet is *not* able to recover from? You can construct network architectures that are the superset of both, so that inferior performance could be avoided. The algorithm is clear but it is similar to the Dynamic Filter Networks paper. And I am unfortunately not convinced about the usefulness of this particular formulation. I*d expect a stronger paper with more insights into transformations and comparisons to standard techniques, a clear delineation of when this is advised.  This works applies steerable frames for various tasks where convolutional neural networks with location invariant operators are traditionally applied. Authors provide a detailed overview of steerable frames followed with an experimental section which applies dynamic steerable network to small machine learning problems where the steerability is conceptually useful. Even though the evaluation is performed only on few small tasks, the reason why more tasks were not evaluated is that piece-wise pose invariance is needed only for a subset of tasks. The fact, that simply using overcomplete bases as a sort of *feature pre-processing* improves the results for already highly optimized ResNet and DenseNet architectures is quite interesting achievement. For the edge detection, a relatively hard baseline is selected - the Dynamic Filter Networks, which already attempts to achieve position invariant filters. The fact that DSFN improves the performance on this task verifies that regressing the parametrization of the steerable filters yields better results than regressing the filters directly. In the last experiment authors apply the network to video classification using LSTMs and they show that the improved performance is not due to increased capacity of the network. In general, it is quite interesting work. Even though it does not offer ground-breaking results (mainly in a sense of not performing experiments on larger tasks), it is theoretically interesting and shows promising results. There are few minor issues and suggestions related to the paper: * For the LSTM experiment, in order to be more exact, it would be useful to include information about total number of parameters, as the network which estimates the pose also increases the number of parameters. * Would it be possible to provide more details about how the back-propagation is done through the steerable filters? * For the Edge Detection experiment, it would be useful to provide results for some standard baseline - e.g. CNN with a similar number of parameters. Simply to see how useful it is to have location-variant filters for this task. * The last sentence in second paragraph on page 1 is missing a verb. Also it is maybe unnecessary. * The hyphenation for ConvNet is incorrect on multiple places (probably `hyphenation{Conv-Net}` would fix it).","['Rating:###5: Marginally below acceptance threshold', 'Rating:###4: Ok but not good enough - rejection', 'Rating:###7: Good paper, accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",5,4,Strengths: 1. The paper presents a novel approach to CNNs by introducing steerable frames. 2. The paper is well-written and clear. 3. The authors provide a good overview of the steerable frame concept. Weaknesses: 1. The paper is not very well motivated. The authors do not explain why steerable frames are better than the standard pixel basis. 2. The paper is not very well evaluated. The authors only evaluate their method on a small dataset (Cifar-10+) and do not compare it to other methods. 3. The paper is not very well explained. The authors do not explain how the back-propagation is done through the steerable filters. Rating: 6 Confidence: 3
Tartan: Accelerating Fully-Connected and Convolutional Layers in Deep Learning Networks by Exploiting Numerical Precision Variability | OpenReview,"Tartan {TRT} a hardware accelerator for inference with Deep Neural Networks (DNNs) is presented and evaluated on Convolutional Neural Networks. TRT exploits the variable per layer precision requirements of DNNs to deliver execution time that is proportional to the precision p in bits used per layer for convolutional and fully-connected layers. Prior art has demonstrated an accelerator with the same execution performance only for convolutional layers. Experiments on image classification CNNs show that on average across all networks studied, TRT outperforms a state-of-the-art bit-parallel accelerator by 1.90x without any loss in accuracy while it is 1.17x more energy efficient. TRT requires no network retraining while it enables trading off accuracy for additional improvements in execution performance and energy efficiency. For example, if a 1% relative loss in accuracy is acceptable, TRT is on average 2.04x faster and 1.25x more energy efficient than the bit-parallel accelerator. This revision includes post-layout results and a better configuration that processes 2bits at time resulting in better efficiency and lower area overhead.","The authors present TARTAN, a derivative of the previously published DNN accelerator architecture: “DaDianNao”. The key difference is that TARTAN’s compute units are bit-serial and unroll MAC operation over several cycles. This enables the units to better exploit any reduction in precision of the input activations for improvement in performance and energy efficiency. Comments: 1. I second the earlier review requesting the authors to be present more details on the methodology used for estimating energy numbers for TARTAN. It is claimed that TARTAN gives only a 17% improvement in energy efficiency. However, I suspect that this small improvement is clearly within the margin of error ij energy estimation. 2. TARTAN is a derivative of DaDianNao, and it heavily relies the overall architecture of DaDianNao. The only novel aspect of this contribution is the introduction of the bit-serial compute unit, which (unfortunately) turns out to incur a severe area overhead (of nearly 3x over DaDianNao*s compute units). 3. Nonetheless, the idea of bit-serial computation is certainly quite interesting. I am of the opinion that it would be better appreciated (and perhaps be even more relevant) in a circuit design / architecture focused venue.  Summary: The paper describes how the DaDianNao (DaDN) DNN accelerator can be improved by employing bit serial arithmetic. They replace the bit-parallel multipliers in DaDN with multipliers that accept the weights in parallel but the activations serially (serial x parallel multipliers). They increase the number of units keeping the total number of adders constant. This enables them to tailor the time and energy consumed to the number of bits used to represent activations. They show how their configuration can be used to process both fully-connected and convolutional layers of DNNs. Strengths: Using variable precision for each layer of the network is useful - but was previously reported in Judd (2015) Good evaluation including synthesis - but not place and route - of the units. Also this evaluation is identical to that in Judd (2016b) Weaknesses: The idea of combining bit-serial arithmetic with the DaDN architecture is a small one. The authors have already published almost everything that is in this paper at Micro 2016 in Judd (2016b). The increment here is the analysis of the architecture on fully-connected layers. Everything else is in the previous publication. The energy gains are small - because the additional flip-flop energy of shifting the activations in almost offsets the energy saved on reducing the precision of the arithmetic. The authors don’t compare to more conventional approaches to variable precision - using bit-parallel arithmetic units but data gating the LSBs so that only the relevant portion of the arithmetic units toggle. This would not provide any speedup, but would likely provide better energy gains than the bit-serial x bit-parallel approach. Overall: The Tartan and Stripes architectures are interesting but the incremental contribution of this paper (adding support for fully-connected layers) over the three previous publications on this topic, and in particular Judd (2016b) is very small. This idea is worth one good paper, not four.  This seems like a reasonable study, though it*s not my area of expertise. I found no fault with the work or presentation, but did not follow the details or know the comparable literature. There seem to be real gains to be had through this technique, though they are only in terms of efficiency in hardware, not changing accuracy on a task. The tasks chosen (Alexnet / VGG) seem reasonable. The results are in simulation rather than in actual hardware. The topic seems a little specialized for ICLR, since it does not describe any new advances in learning or representations, albeit that the CFP includes *hardware*. I think the appeal among attendees will be rather limited. Please learn to use parenthetical references correctly. As is your references make reading harder.  I do not feel very qualified to review this paper. I studied digital logic back in university, that was it. I think the work deserves a reviewer with far more sophisticated background in this area. It certainly seems useful. My advice is also to submit it another venue.  This paper proposed a hardware accelerator for DNN. It utilized the fact that DNN are very tolerant to low precision inference and outperforms a state-of-the-art bit-parallel accelerator by 1.90x without any loss in accuracy while it is 1.17x more energy efficient. TRT requires no network retraining. It achieved super linear scales of performance with area. The first concern is that this paper doesn*t seem very well-suited to ICLR. The circuit diagrams makes it more interesting for the hardware or circuit design community. The second concern is the *take-away for machine learning community*, seeing from the response, the take-away is using low-precision to make inference cheaper. This is not novel enough. In last year*s ICLR, there were at least 4 papers discussing using low precision to make DNN more efficient. These ideas have also been explored in the authors* previous papers.","['Rating:###5: Marginally below acceptance threshold', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###4: Ok but not good enough - rejection', 'Rating:###4: Ok but not good enough - rejection']","['Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'Confidence:###1: The reviewer*s evaluation is an educated guess', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",5,5,"Strengths:
1. The authors present a hardware accelerator for DNNs that exploits the variable precision requirements of DNNs to deliver execution time that is proportional to the precision p in bits used per layer for convolutional and fully-connected layers.
2. The authors present a hardware accelerator for DNNs that exploits the variable precision requirements of DNNs to deliver execution time that is proportional to the precision p in bits used per layer for convolutional and fully-connected layers.
3. The authors present a hardware accelerator for DNNs that exploits the variable precision requirements of DNNs to deliver execution time that is proportional to the precision p in bits used per layer for convolutional and fully-connected layers.
Weaknesses:
1. The idea of combining bit-serial arithmetic with the DaDN architecture is a small one. The authors have already published almost everything that is in this paper at Micro 2016 in Judd (2016b).
2. The increment here is the analysis of the architecture on fully-connected layers. Everything else is in the previous publication.
3. The energy gains are small - because the additional flip-flop energy of shifting the activations in almost offsets the energy saved on reducing the precision of the arithmetic.
4. The authors don’t compare to more conventional approaches to variable precision - using bit-parallel arithmetic units but data gating the LSBs so that only the relevant portion of the arithmetic units toggle. This would not provide any speedup, but would likely provide better energy"
Incorporating long-range consistency in CNN-based texture generation | OpenReview,"Gatys et al. (2015) showed that pair-wise products of features in a convolutional network are a very effective representation of image textures. We propose a simple modification to that representation which makes it possible to incorporate long-range structure into image generation, and to render images that satisfy various symmetry constraints. We show how this can greatly improve rendering of regular textures and of images that contain other kinds of symmetric structure. We also present applications to inpainting and season transfer.","The paper introduces a variation to the CNN-based texture synthesis procedure of Gatys et al. that matches correlations between spatially shifted feature responses in addition to the correlations between feature responses at the same position in the feature maps. The paper claims that this a) improves texture synthesis for textures with long-range regular structures, that are not preserved with the Gatys et al. method b) improves performance on texture inpainting tasks compared to the Gatys et al. method c) improves results in season transfer when combined with the style transfer method by Gatys et al. Furthermore the paper shows that d) by matching correlations between spatially flipped feature maps, symmetry properties around the flipping axis can be preserved. I agree with claim a). However, the generated textures still have some issues such as greyish regions so the problem is not solved. Additionally, the procedure proposed is very costly which makes an already slow texture synthesis method substantially slower. For example, in comparison, the concurrent work by Liu et al. (http://arxiv.org/abs/1605.01141) that tackles the same problem by adding constraints to the Fourier spectrum of the synthesised texture shows comparable or better results while being far more efficient. Also with b) the presented results constitute an improvement over the Gatys et al. method but again the results are not too exciting - one would not prefer this model to other inpainting algorithms. With c) I don’t see a clear advantage of the proposed method to the existing Gatys et al. algorithm. Finally, d) is a neat idea and the initial results look interesting but they don’t go much further than that. All in all I think it is decent work but neither its originality and technical complexity nor the quality of the results are convincing enough for acceptance. That said, I could imagine this to be a nice contribution to the workshop track though.  The paper investigates a simple extension of Gatys et al. CNN-based texture descriptors for image generation. Similar to Gatys et al., the method uses as texture descriptor the empirical intra-channel correlation matrix of the CNN feature response at some layer of a deep network. Differently from Gatys et al., longer range correlations are measured by introducing a shift between the correlated feature responses, which translates in a simple modification of the original architecture. The idea is simple but has interesting effects on the generated textures and can be extended to transformations other than translation. While longer range correlations could be accounted for by considering the response of deeper CNN features in the original method by Gatys et al., the authors show that modelling them explicitly using shallower features is more effective, which is reasonable. An important limitation that this work shares with most of its peers is the lack of a principled quantitative evaluation protocol, such that judging the effectiveness of the approach remains almost entirely a qualitative affair. While this should not be considered a significant drawback of the paper due to the objective difficulty of solving this open issue, nevertheless it is somewhat limiting that no principled evaluation method could be devised and implemented. The authors suggest that, as future work, a possible evaluation method could be based on a classification task -- this is a potentially interesting approach that merits some further investigation.  This paper proposes a modification of the parametric texture synthesis model of Gatys et al. to take into account long-range correlations of textures. To this end the authors add the Gram matrices between spatially shifted feature vectors to the synthesis loss. Some of the synthesised textures are visually superior to the original Gatys et al. method, in particular on textures with very structured long-range correlations (such as bricks). The paper is well written, the method and intuitions are clearly exposed and the authors perform quite a wide range of synthesis experiments on different textures. My only concern, which is true for all methods including Gatys et al., is the variability of the samples. Clearly the global minimum of the proposed objective is the original image itself. This issue is partially circumvented by performing inpainting experiments, by which the synthesised paths needs to stay coherent with the borders (as the authors did). There are no additional insights into this problem in this paper, which would have been a plus. All in all, this work is a simple and nice modification of Gatys at al. which is worth publishing but does not constitute a major breakthrough.","['Rating:###5: Marginally below acceptance threshold', 'Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept']","['Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",5,5,"Strengths: 1. The paper proposes a modification of the parametric texture synthesis model of Gatys et al. to take into account long-range correlations of textures. 2. The authors perform a wide range of synthesis experiments on different textures. 3. The paper is well written, the method and intuitions are clearly exposed. Weaknesses: 1. The proposed method is not very efficient. For example, in comparison, the concurrent work by Liu et al. (http://arxiv.org/abs/1605.01141) that tackles the same problem by adding constraints to the Fourier spectrum of the synthesised texture shows comparable or better results while being far more efficient. 2. The generated textures still have some issues such as greyish regions. 3. The paper lacks a principled quantitative evaluation protocol. Rating: 6 Confidence: 3"
Recurrent Coevolutionary Feature Embedding Processes for Recommendation | OpenReview,"Recommender systems often use latent features to explain the behaviors of users and capture the properties of items. As users interact with different items over time, user and item features can influence each other, evolve and co-evolve over time. To accurately capture the fine grained nonlinear coevolution of these features, we propose a recurrent coevolutionary feature embedding process model, which combines recurrent neural network (RNN) with a multi-dimensional point process model. The RNN learns a nonlinear representation of user and item embeddings which take into account mutual influence between user and item features, and the feature evolution over time. We also develop an efficient stochastic gradient algorithm for learning parameters. Experiments on diverse real-world datasets demonstrate significant improvements in user behavior prediction compared to state-of-the-arts.","The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations. The later are modeled as coupled – autoregressive processes – i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t. This is called coevolution here and the autoregressive process is called recurrent NN. The model may also incorporate heterogeneous inputs. Experiments are performed on several datasets, and the model is compared with different baselines. There are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation. The paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation. The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same. By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization? Both are quite similar. There is no justification on the choice of the specific form of the point process in the two papers. Did the authors tried other forms as well? The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t. a linear model, but there is no evidence of the role of this non linearity in the paper. Note that there are some inconsistencies between the results in the two papers. Concerning the evaluation, the authors introduce two criteria. I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with. Do you mean, the next item the user will interact with after time t? For the time prediction, why is it a relevant metric for recommendation? A comparison of the complexity, or execution time of the different methods would be helpful. The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods. Overall, the paper is quite nice and looks technically sound, albeit many details are missing. On the other hand, I have a mixed feeling because of the similarity with NIPS paper. The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors. I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets.  This paper proposes a method to model time changing dynamics in collaborative filtering. Comments: 1) The main idea of the paper is build upon similar to a previous work by the same group of author (Wang et.al KDD), the major difference appears to be change some of the latent factors to be RNN 2) The author describes a BPTT technique to train the model 3) The author introduced time prediction as a new metric to evaluate the effectiveness of time dependent model. However, this need to be condition on a given user-item pair. 4) It would be interesting to consider other metrics, for example - The switching time where a user changes his/her to another item - Jointly predict the next item and switching time. In summary, this is a paper that improves over an existing work on time dynamics model in recommender system. The time prediction metric is interesting and opens up interesting discussion on how we should evaluate recommender systems when time is involved (see also comments).  The paper seeks to predict user events (interactions with items at a particular point in time). Roughly speaking the contributions are as follows: (a) the paper models the co-evolutionary process of users* preferences toward items (b) the paper is able to incorporate external sources of information, such as user and item features (c) the process proposed is generative, so is able to estimate specific time-points at which events occur (d) the model is able to account for non-linearities in the above Following the pre-review questions, I understand that it is the combination of (a) and (c) that is the most novel aspect of the paper. A fully generative process which can be sampled is certainly nice (though of course, non-generative processes like regular old regression can estimate specific time points and such too, so not sure in practice how relevant this distinction is). Other than that the above parts have all appeared in some combination in previous work, though the combination of parts here certainly passes the novelty bar. I hadn*t quite followed the issue mentioned in the pre-review discussion that the model requires multiple interactions per userXitem pair in order to fit the model (e.g. a user interacts with the same business multiple times). This is a slightly unusual setting compared to most temporal recommender systems work. I question to some extent whether this problem setting isn*t a bit restrictive. That being said I take the point about why the authors had to subsample the Yelp data, but keeping only users with *hundreds* of events means that you*re left with a very biased sample of the user base. Other than the above issues, the paper is technically nice, and the experiments include strong baselines and reports good performance.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",6,4,Strengths: 1. The paper proposes a novel approach to modeling time dependent recommender systems. 2. The authors provide a good discussion on the importance of modeling time dependent recommender systems. 3. The paper is well written and easy to understand. Weaknesses: 1. The paper does not provide a clear motivation for the choice of the point process model. 2. The authors do not provide a clear discussion on the choice of the point process model. 3. The authors do not provide a clear discussion on the choice of the RNN model. 4. The authors do not provide a clear discussion on the choice of the sigmoid function. 5. The authors do not provide a clear discussion on the choice of the learning rate. 6. The authors do not provide a clear discussion on the choice of the optimization algorithm. 7. The authors do not provide a clear discussion on the choice of the evaluation metric. 8. The authors do not provide a clear discussion on the choice of the dataset. 9. The authors do not provide a clear discussion on the choice of the hyperparameters. 10. The authors do not provide a clear discussion on the choice of the model parameters. Rating: 6 Confidence: 3
Communicating Hierarchical Neural Controllers for Learning Zero-shot Task Generalization | OpenReview,"The ability to generalize from past experience to solve previously unseen tasks is a key research challenge in reinforcement learning (RL). In this paper, we consider RL tasks defined as a sequence of high-level instructions described by natural language and study two types of generalization: to unseen and longer sequences of previously seen instructions, and to sequences where the instructions themselves were previously not seen. We present a novel hierarchical deep RL architecture that consists of two interacting neural controllers: a meta controller that reads instructions and repeatedly communicates subtasks to a subtask controller that in turn learns to perform such subtasks. To generalize better to unseen instructions, we propose a regularizer that encourages to learn subtask embeddings that capture correspondences between similar subtasks. We also propose a new differentiable neural network architecture in the meta controller that learns temporal abstractions which makes learning more stable under delayed reward. Our architecture is evaluated on a stochastic 2D grid world and a 3D visual environment where the agent should execute a list of instructions. We demonstrate that the proposed architecture is able to generalize well over unseen instructions as well as longer lists of instructions.","Description: This paper presents a reinforcement learning architecture where, based on *natural-language* input, a meta-controller chooses subtasks and communicates them to a subtask controller that choose primitive actions, based on the communicated subtask. The goal is to scale up reinforcement learning agents to large-scale tasks. The subtask controller embeds the subtask definition (arguments) into vectors by a multi-layer perceptron including an *analogy-making* regularization. The subtask vectors are combined with inputs at each layer of a CNN. CNN outputs (given the observation and the subtask) are then fed to one of two MLPs; one to compute action probabilities in the policy (exponential falloff of MLP outputs) and the other to compute termination probability (sigmoid from MLP outputs). The meta controller takes a list of sentences as instructions embeds them into a sequence of subtask arguments (not necessarily a one-to-one mapping). A context vector is computed by a CNN from the observation, the previous sentence embedding, the previous subtask and its completion state. The subtask arguments are computed from the context vector through further mechanisms involving instruction retrieval from memory pointers, and hard/soft decisions whether to update the subtask or not. Training involves policy distillation+actor-critic training for the subtask controller, and actor-critic training for the meta controller keeping the subtask controller frozen. The system is tested in a grid world where the agent moves and interacts with (picks up/transforms) various item/enemy types. It is compared to a) a flat controller not using a subtask controller, and b) subtask control by mere concatenation of the subtask embedding to the input with/without the analogy-making regularization. Evaluation: The proposed architecture seems reasonable, although it is not clear why the specific way of combining subtask embeddings in the subtask controller would be the *right* way to do it. I do not feel the grid world here really represents a *large-scale task*: in particular the 10x10 size of the grid is very small. This is disappointing since this was a main motivation of the work. Moreover, the method is not compared to any state of the art alternatives. This is especially problematic because the test is not on established benchmarks. It is not really possible, based on the shown results, to put the performance in context of other works.  This paper can be seen as instantiating a famous paper by the founder of AI John McCarthy on learning to take advice (which was studied in depth by other later researchers, such as Jack Mostow in the card game Hearts). The idea is that the agent is given high level instructions on how to solve a problem, and must distill from it a low level policy. This is quite related to how humans learn complex tasks in many domains (e.g., driving, where a driving instructor may provide advice such as *keep a certain distance from the car in front*). A fairly complex neural deep learning controller architecture is used, although the details of this system are somewhat confusing in terms of many details that are presented. A simpler approach might have been easier to follow, at least initially. The experiments unfortunately are on a rather simplistic 2D maze, and it would have been worthwhile to see how the approach scaled to more complex tasks of the sort usually seen in deep RL papers these days (e.g, Atari, physics simulators etc.). Nice overall idea, somewhat confusing description of the solution, and an inadequate set of experiments on a less than satisfactory domain of 2D grid worlds.  The paper presents a hierarchical DRL algorithm that solves sequences of navigate-and-act tasks in a 2D maze domain. During training and evaluation, a list of sub-goals represented by text is given to the agent and its goal is to learn to use pre-learned skills in order to solve a list of sub-goals. The authors demonstrate that their method generalizes well to sequences of varying length as well as to new combinations of sub-goals (i.e., if the agent knows how to pick up a diamond and how to visit an apple, it can also visit the diamond). Overall, the paper is of high technical quality and presents an interesting and non-trivial combination of state-of-the-art advancements in Deep Learning (DL) and Deep Reinforcement Learning (DRL). In particular, the authors presents a DRL agent that is hierarchical in the sense that it can learn skills and plan using them. The skills are learned using a differential temporally extended memory networks with an attention mechanism. The authors also make a novel use of analogy making and parameter prediction. However, I find it difficult to understand from the paper why the presented problem is interesting and why hadn*t it bee solved before. Since the domain being evaluated is a simple 2D maze, using deep networks is not well motivated. Similar problems have been solved using simpler models. In particular, there is a reach literature about planning with skills that had been ignored completely by the authors. Since all of the skills are trained prior to the evaluation of the hierarchical agent, the problem that is being solved is much more similar to supervised learning than reinforcement learning (since when using the pre-trained skills the reward is not particularly delayed). The generalization that is demonstrated seems to be limited to breaking a sentence (describing the subtask) into words (item, location, action). The paper is difficult to read, it is constantly switching between describing the algorithm and giving technical details. In particular, I find it to be overloaded with details that interfere with the general understanding of the paper. I suggest moving many of the implementation details into the appendix. The paper should be self-contained, please do not assume that the reader is familiar with all the methods that you use and introduce all the relevant notations. I believe that the paper will benefit from addressing the problems I described above and will make a better contribution to the community in a future conference.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###3: Clear rejection']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",4,3,Strengths: 1. The paper presents a novel approach to hierarchical reinforcement learning. 2. The authors provide a clear and concise description of the algorithm. 3. The paper is well-written and easy to follow. Weaknesses: 1. The paper is not self-contained. The authors assume that the reader is familiar with many of the methods used in the paper. 2. The paper is difficult to read. The authors switch between describing the algorithm and giving technical details. 3. The experiments are not well motivated. The authors do not compare their results to state-of-the-art methods. Rating: 6 Confidence: 3
The Predictron: End-To-End Learning and Planning | OpenReview,"One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple *imagined* planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function, thereby focusing the model upon the aspects of the environment most relevant to planning. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.","The paper proposes an approach to learning models that are good for planning problems, using deep netowork architectures. The key idea is to ensure that models are self-consistent and accurately predict the future. The problem of learning good planning models (as opposed to simply good predictive models is really crucial and attempts so far have failed. This paper is conceptually interesting and provides a valuable perspective on how to achieve this goal. Its incorporation of key RL concepts (like discounting and eligibility traces) and the flexibility to learn these is very appealing. Hence, I think it should be accepted. This being said, I think the paper does not quite live up to its claims. Here are some aspects that need to be addressed (in order of importance): 1. Relationship to past work: the proposed representation seems essentially a non-linear implementation of the Horde architecture. It is also very similar in spirit to predictive state representations. Yet these connections are almost not discussed at all. The related work paragraph is very brief and needs expansion to situate the work in the context of other predictive modelling attempts that both were designed to be used for planning and (in the case of PSRs) were in fact successsfully used in planning tasks. Some newer work on learning action-conditional models in Atari games are also not discussed. Situating the paper better in the context of existing model learning would also help understand easier both the motivations and the novel contributions of the work (otherwise, the reader is left to try and elucidate this for themselves, and may come to the wrong conclusion). 2. The paper needs to provide some insight about the necessity of the recurrent core of the architecture. The ideas are presented nicely in general fashion, yet the proposed impolementation is quite specific and *bulky* (very high number of parameters). Is this really necessary in all tasks? Can one implement the basic ideas outside of the particular architecture proposed? Can we use feedforward approximations or is the recurrent part somehow necessary? At the very least the paper should expand the discussion on this topic, if not provide some empirical evidence. 3. The experiments are very restricted in their setup: iid data drawn from fixed distributions, correct targets. So, the proposed approach seems like an overkill for these particular tasks. There is an indirect attempt to provide evidence the learned models would be useful for planning, but no direct measurement to support this*d claim (no use of the models in planning). Compared to the original Horde paper, fewer predictions are learned, and these are more similar to each other. While I sympathize with the desire to go in steps, I think the paper stops short of where it should. At the very least, doing prediction in the context of an actual RL prediction task, with non-iid inputs, should be included in the paper. This should only require minor modifications to the experiments (same task, just different data). Ideally, in the case of the mazes, the learned models should be used in some form of simplified planning to learn paths. This would align the experiments much better with the claims in the presentation of the architecture.  I think there may be a nice paper to made from this, but as it is, it should not be accepted. The authors describe a new architecture for regression, inspired by techniques for estimating the value function of an Markov reward process. The connection is interesting, and there is certainly merit in the idea. However, the writing is confusing, and as far as I can tell, the experiments and discussion are inadequate. It is quite possible that I am misunderstanding some things, so I am not putting high confidence. Because of all the discussion of MRP*s and the background that inspired the model, it is difficult to see that the authors are in a pure, i.i.d. regression setting, where they sample inputs i.i.d. (with deterministic outputs given the input) from a distribution, and try to match a parameterized function to the input output pairs. Because they are in this setting, there is a lot lacking from the experiments. For example, they report l2 loss on the maze problem; but not *percent correct*; indeed, it looks like the deep net with skips goes to about .001 average l2 loss on the 0-1 output maze problem. This is an issue because because it suggests that by simply thresholding the outputs, you could get nearly perfect results, which would point to a model specification error of the baseline. Are there sigmoids at the end of the baseline plain deep network? Note that the proposed models do have sigmoids in the outputs in the multiplicative weightings. How do the number of parameters of the proposed network compare to the baselines? Is the better performance (and again, better is really marginal if I am understanding the way loss is measured) simply an issue of modeling power (perhaps because of the multiplicative connections of the proposed model vs. the baseline)? Because the input is taken i.i.d and the test distribution exactly matches the train, this is an important part of the discussion. Moreover, there do not seem to be experiments where the size of the training set is fixed- the axis in the graphs is number of samples seen, which is tied to the number of optimization steps. Thus there is no testing of over-fitting. Why not try the model on more standard regression problems (as at heart, the paper seems to be about a new convnet architecture for regression)? Show imagenet or cifar accuracies, for example. If the proposed model does worse there, try to explain/understand what it is about the reported tasks that favor the proposed model? ********************************************************************************** edited with increased confidence in post review discussions **********************************************************************************  This work proposes a computational structure of function approximator with a strong prior: it is optimized to act as an abstract MRP, capable of learning its own internal state, model, and notion of time-step. Thanks to the incorporation of a lambda-return style return estimation, it can effectively adapt its own *thinking-depth* on the current input, thus performing some sort of soft iterative inference. Such a prior, maintained by strong regularization, helps perform better than similar baselines or some prediction tasks that require some form of sequential reasoning. The proposed idea is novel, and a very interesting take on forcing internal models upon function approximators which begs for future work. The experimental methodology is complete, showcases the potential of the approach, and nicely analyses the iterative/adaptative thinking depth learned by the model. As pointed out by my previous comments, the paper reads well but utilizes language that may confuse a reader unfamiliar with the subject. I think some rewording could be done without having much impact on the depth of the paper. In particular, introducing the method as a regularized model pushed to act like an MRP, rather than an actual MRP performing some abstract reasoning, may help confused readers such as myself.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###4: Ok but not good enough - rejection', 'Rating:###9: Top 15% of accepted papers, strong accept']","['Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']",6,5,"Strengths:
1. The paper proposes a novel approach to learning models that are good for planning problems, using deep neural network architectures. The key idea is to ensure that models are self-consistent and accurately predict the future. This is a crucial aspect of planning problems, and the paper provides a valuable perspective on how to achieve this goal.
2. The paper is well-written and easy to follow, with a clear and concise presentation of the ideas.
3. The experimental methodology is complete and showcases the potential of the approach.
Weaknesses:
1. The paper does not provide a clear and concise explanation of the relationship between the proposed architecture and the Horde architecture. The connection is not explicitly stated, and the reader is left to infer it.
2. The paper does not provide a clear and concise explanation of the necessity of the recurrent core of the architecture. The ideas are presented nicely in general fashion, yet the proposed implementation is quite specific and bulky.
3. The paper does not provide a clear and concise explanation of the experimental methodology. The experiments are very restricted in their setup, and the paper does not provide a clear and concise explanation of the results.
Rating: 8
Confidence: 4"
The Preimage of Rectifier Network Activities | OpenReview,"The preimage of the activity at a certain level of a deep network is the set of inputs that result in the same node activity. For fully connected multi layer rectifier networks we demonstrate how to compute the preimages of activities at arbitrary levels from knowledge of the parameters in a deep rectifying network. If the preimage set of a certain activity in the network contains elements from more than one class it means that these classes are irreversibly mixed. This implies that preimage sets which are piecewise linear manifolds are building blocks for describing the input manifolds specific classes, i.e. all preimages should ideally be from the same class. We believe that the knowledge of how to compute preimages will be valuable in understanding the efficiency displayed by deep learning networks and could potentially be used in designing more efficient training algorithms.","Summary: This paper looks at the structure of the preimage of a particular activity at a hidden layer of a network. It proves that any particular activity has a preimage of a piecewise linear set of subspaces. Pros: Formalizing the geometry of the preimages of a particular activity vector would increase our understanding of networks Cons: Analysis seems quite preliminary, and no novel theoretical results or clear practical conclusions. The main theoretical conclusion seems to be the preimage being this stitch of lower dimensional subspaces? Would a direct inductive approach have worked? (e.g. working backwards from the penultimate layer say?) This is definitely an interesting direction, and it would be great to see more results on it (e.g. how does the depth/width, etc affect the division of space, or what happens during training) but it doesn*t seem ready yet.  I have not read the revised version in detail yet. SUMMARY This paper studies the preimages of outputs of a feedforward neural network with ReLUs. PROS The paper presents a neat idea for changes of coordinates at the individual layers. CONS Quite unpolished / not enough contributions for a finished paper. COMMENTS - In the first version the paper contains many typos and appears to be still quite unpolished. - The paper contains nice ideas but in my opinion it does not contribute sufficiently many results for a Conference paper. I would be happy to recommend for the Workshop track. - Irreversibly mixed and several other notions from the present paper are closely related to the concepts discussed in [Montufar, Pascanu, Cho, Bengio, NIPS 2014]. I feel that that paper should be cited here and the connections should be discussed. In particular, that paper also contains a discussion on the local linear maps of ReLU networks. - I am curious about the practical considerations when computing the pre-images. The definition should be rather straight forward really, but the implementation / computation could be troublesome. DETAILED COMMENTS - On page 1 ``can easily be shown to be many to one** in general. - On page 2 ``For each point x^{l+1}** The parentheses in the superscript are missing. - After eq. 6 ``the mapping is unique** is missing `when w1 and w2 are linearly independent* - Eq. 1 should be a vector. - Above eq. 3. ``collected the weights a_i into the vector w** and bias b. Period is missing. - On page 2 ``... illustrate the preimage for the case of points on the lines ... respectively** Please indicate which is which. - In Figure 1. Is this a sketch, or the actual illustration of a network. In the latter case, please state the specific value of x and the weights that are depicted. Also define and explain the arrows precisely. What are the arrows in the gray part? - On page 3 `` This means that the preimage is just the point x^{(l)}** the points that W maps to x^{(l+1)}. - On page 3 the first display equation. There is an index i on the left but not on the right hand side. The quantifier in the right hand side is not clear. - ``generated by the mapping ... w^i ** subscript - ``get mapped to this hyperplane** to zero - ``remaining** remaining from what? - ``using e.g. Grassmann-Cayley algebra** How about using elementary linear algebra?! - ``gives rise to a linear manifold with dimension one lower at each intersection** This holds if the hyperplanes are in general position. - ``is complete in the input space** forms a basis - ``remaining kernel** remaining from what? - ``kernel** Here kernel is referring to nullspace or to a matrix of orthonormal basis vectors of the nullspace, or to what specifically? - Figure 3. Nullspaces of linear maps should pass through the origin. - `` from pairwise intersections** cap - ``indicated as arrows or the shaded area** this description is far from clear. - typos: peieces, diminsions, netork, me,","['Rating:###4: Ok but not good enough - rejection', 'Rating:###4: Ok but not good enough - rejection']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",4,4,Strengths: 1. The paper is well written and easy to understand. 2. The authors are clearly interested in the topic. Weaknesses: 1. The paper is not well motivated. The authors do not clearly state the main question of the paper. 2. The paper is not well structured. The authors jump between different ideas without a clear connection. 3. The paper is not well supported by experiments. The authors do not provide any empirical results to support their claims. 4. The paper is not well related to the current state of the art. The authors do not clearly state how their work relates to previous work. Rating: 4 Confidence: 2
Recurrent Mixture Density Network for Spatiotemporal Visual Attention | OpenReview,"In many computer vision tasks, the relevant information to solve the problem at hand is mixed to irrelevant, distracting information. This has motivated researchers to design attentional models that can dynamically focus on parts of images or videos that are salient, e.g., by down-weighting irrelevant pixels. In this work, we propose a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. We model visual attention with a mixture of Gaussians at each frame. This distribution is used to express the probability of saliency for each pixel. Time consistency in videos is modeled hierarchically by: 1) deep 3D convolutional features to represent spatial and short-term time relations and 2) a long short-term memory network on top that aggregates the clip-level representation of sequential clips and therefore expands the temporal domain from few frames to seconds. The parameters of the proposed model are optimized via maximum likelihood estimation using human fixations as training data, without knowledge of the action in each video. Our experiments on Hollywood2 show state-of-the-art performance on saliency prediction for video. We also show that our attentional model trained on Hollywood2 generalizes well to UCF101 and it can be leveraged to improve action classification accuracy on both datasets.","The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition. quality: I am missing a more thorough evaluation of the fixation prediction performance. The center bias performance in Table 1 differs significantly from the on in Table 2. All the state-of-the-art models reported in Table 2 have a performance worse than the center bias performance reported in Table 1. Is there really no other model better than the center bias? Additionally I am missing details on how central bias and human performance are modelled. Is human performance cross-validated? You claim that your *results are very close to human performance (the difference is only 3.2%). This difference is actually larger than the difference between Central Bias and your model reported in Table 1. Apart from this, it is dangerous to compare AUC performance differences due to e.g. saturation issues. clarity: the explanation for Table 3 is a bit confusing, also it is not clear why the CONV5 and the FC6 models differ in how the saliency map is used. At least one should also evaluate the CONV5 model when multiplying the input with the saliency map to see how much of the difference comes from the different ways to use the saliency map and how much from the different features. Other issues: You cite Kümmerer et. al 2015 as a model which *learns ... indirectly rather than from explicit information of where humans look*, however the their model has been trained on fixation data using maximum-likelihood. Apart from these issues, I think the paper make a very interesting contribution to spatio-temporal fixation prediction. If the evaluation issues given above are sorted out, I will happily improve my rating.  This work proposes to a spatiotemporal saliency network that is able to mimic human fixation patterns, thus helping to prune irrelevant information from the video and improve action recognition. The work is interesting and has shown state-of-the-art results on predicting human attention on action videos. It has also shown promise for helping action clip classification. The paper would benefit from a discussion on the role of context in attention. For instance, if context is important, and people give attention to context, why is it not incorporated automatically in your model? One weak point is the action recognition section, where the comparison between the two (1)(2) and (3) seems unfair. The attention weighted feature maps in fact reduce the classification performance, and only improve performance when doubling the feature and associated model complexity by concatenating the weighted maps with the original features. Is there a way to combine the context and attention without concatenation? The rational for concatenating the features extracted from the original clip, and the features extracted from the saliency weighted clip seems to contradict the initial hypothesis that `eliminating or down-weighting pixels that are not important* will improve performance. The authors should also mention the current state-of-the-art results in Table 4, for comparison. # Other comments: # Abstract - Typo: `mixed with irrelevant ...* ``Time consistency in videos ... expands the temporal domain from few frames to seconds** - These two points are not clear, probably need a re-write. # Contributions - 1) `The model can be trained without having to engineer spatiotemporal features* - you would need to collect training data from humans though.. # Section 3.1 The number of fixation points is controlled to be fixed for each frame - how is this done? In practice we freeze the layers of the C3D network to values pretrained by Tran etal. What happens when you allow gradients to flow back to the C3D layers? Is it not better to allow the features to be best tuned for the final task? The precise way in which the features are concatenated needs to be clarified in section 3.4. Minor typo: `we added them trained central bias*  This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet (in particular, C3D) to extract visual features. The visual features are then passed to LSTM. The hidden state at each time step in LSTM is used to generate the parameters in a Gaussian mixture model. Finally, the visual attention map is generated from the Gaussian mixture model. Overall, the idea in this paper is reasonable and the paper is well written. RNN/LSTM has been used in lots of vision problem where the outputs are discrete sequences, there has not been much work on using RNN/LSTM for problems where the output is continuous like in this paper. The experimental results have demonstrated the effectiveness of the proposed approach. In particular, it outperforms other state-of-the-art on the saliency prediction task on the Hollywood2 datasets. It also shows improvement over baselines (e.g. C3D + SVM) on the action recognition task. My only *gripe* of this paper is that this paper is missing some important baseline comparisons. In particular, it does not seem to show how the *recurrent* part help the overall performance. Although Table 2 shows RMDN outperforms other state-of-the-art, it might be due to the fact that it uses strong C3D features (while other methods in Table 2 use traditional handcrafted features). Since saliency prediction is essentially a dense image labeling problem (similar to semantic segmentation). For dense image labeling, there has been lots of methods proposed in the past two years, e.g. fully convolution neural network (FCN) or deconvnet. A straightforward baseline is to simply take FCN and apply it on each frame. If the proposed method still outperforms this baseline, we can know that the *recurrent* part really helps.","['Rating:###7: Good paper, accept', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",7,4,"Strengths: 1. The authors propose a novel approach to spatiotemporal visual attention. 2. The paper is well written and easy to understand. 3. The authors provide a good discussion on the role of context in attention. Weaknesses: 1. The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition. quality: I am missing a more thorough evaluation of the fixation prediction performance. The center bias performance in Table 1 differs significantly from the on in Table 2. All the state-of-the-art models reported in Table 2 have a performance worse than the center bias performance reported in Table 1. Is there really no other model better than the center bias? Additionally I am missing details on how central bias and human performance are modelled. Is human performance cross-validated? You claim that your *results are very close to human performance (the difference is only 3.2%). This difference is actually larger than the difference between Central Bias and your model reported in Table 1. Apart from this, it is dangerous to compare AUC performance differences due to e.g. saturation issues. clarity: the explanation for Table 3 is a bit confusing, also it is not clear why the"
Intelligible Language Modeling with Input Switched Affine Networks | OpenReview,"The computational mechanisms by which nonlinear recurrent neural networks (RNNs) achieve their goals remains an open question. There exist many problem domains where intelligibility of the network model is crucial for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations, in other words an RNN without any nonlinearity and with one set of weights per input. We show that this architecture achieves near identical performance to traditional architectures on language modeling of Wikipedia text, for the same number of model parameters. It can obtain this performance with the potential for computational speedup compared to existing methods, by precomputing the composed affine transformations corresponding to longer input sequences. As our architecture is affine, we are able to understand the mechanisms by which it functions using linear methods. For example, we show how the network linearly combines contributions from the past to make predictions at the current time step. We show how representations for words can be combined in order to understand how context is transferred across word boundaries. Finally, we demonstrate how the system can be executed and analyzed in arbitrary bases to aid understanding.","Summary: The authors present a simple RNN with linear dynamics for language modeling. The linear dynamics greatly enhance the interpretability of the model, as well as provide the potential to improve performance by caching the dynamics for common sub-sequences. Overall, the quantitative comparison on a benchmark task is underwhelming. It’s unclear why the authors didn’t consider a more common dataset, and they only considered a single dataset. On the other hand, they present a number of well-executed techniques for analyzing the behavior of the model, many of which would be impossible to do for a non-linear RNN. Overall, I recommend that the paper is accepted, despite the results. It provides an interesting read and an important contribution to the research dialogue. Feedback The paper could be improved by shortening the number of analysis experiments and increasing the discussion of related sequence models. Some of the experiments were very compelling, whereas some of them (eg. 4.6) sort of feels like you’re just showing the reader that the model fits the data well, not that the model has any particularly important property. We trust that the model fits the data well, since you get reasonable perplexity results. LSTMS/GRUs are great for for language modeling for data with rigid combinatorial structure, such as nested parenthesis. It would have been nice if you compared your model to non-linear methods on this sort of data. Don’t be scared of negative results! It would be interesting if the non-linear methods were substantially better on these tasks. You should definitely add a discussion of Belanger and Kakade 2015 to the related work. They have different motivations (fast, scalable learning algorithms) rather than you (interpretable latent state dynamics and simple credit assignment for future predictions given past). On the other hand, they also have linear dynamics, and look at the singular vectors of the transition matrix to analyze the model. More broadly, it would be useful for readers if you discussed LDS more directly. A lot of this comparison came up in the openreview discussion, and I recommend folding this into the paper. For example, it would be useful to emphasize that the bias vectors correspond to columns of the Kalman gain matrix. One last thing regarding LDS: your model corresponds to Kalman filtering but in an LDS you can also do Kalman smoothing, where state vectors are inferred using the future in addition to the past observations. Could you do something similar in your model? What if you said that each matrix is a sparse/convex combination of a set of dictionary matrices? This parameter sharing could provide even more interpretability, since the characters are then represented by the low-dimensional weights used to combine the dictionary elements. This could also provide more scalability to word-level problems.  Summary: The authors propose an input switched affine network to do character-level language modeling, a kind of RNN without pointwise nonlinearity, but with switching the transition matrix & bias based on the input character. This is motivated by intelligibility, since it allows decomposition of output contribution into these kappa_s^t terms, and use of basic linear algebra to probe the network. Regarding myself as a reviewer, I am quite sure I understood the main ideas and arguments of this paper, but am not an expert on RNN language models or intelligibility/interpretability in ML. I did not read any papers with a similar premise - closest related work I*m familiar with would be deconvnet for insight into vision-CNNs. PRO: I think this is original and novel work. This work is high quality, well written, and clearly is the result of a lot of work. I found section 4.5 about projecting into readout subspace vs *computational* subspace most interesting and meaningful. CON: + The main hesitation I have is that the results on both parts (ISAN model, and analysis of it) are not entirely convincing: (1) ISAN is only trained on small task (text8), not clear whether it can be a strong char-LM on larger scale tasks, (2) nor do the analysis sections provide all that much real insight in the learned network. (1b) Other caveat towards ISAN architecture: this model in its proposed form is really only fit for small-vocabulary (i.e. character-based) language modeling, not a general RNN with large-vocab discrete input nor continuous input. (2a) For analysis: many cute plots and fun ideas of quantities to look at, but not much concrete insights. (2b) Not very clear which analysis is specific to the ISAN model, and which ideas will generalize to general nonlinear RNNs. (2c) Re sec 4.2 - 4.3: It seems that the quantity kappa_s^t on which analysis rests, isn*t all that meaningful. Elaborating a bit on what I wrote in the question: For example: Fig 2, for input letter *u* in revenue, there*s a red spot where *_* character massively positively impacts the logit of *e*. This seems quite meaningless, what would be the meaning of influence of *_* character? So it looks ot me that the switching matrix W_u (and prior W_n W_e etc) are using previous state in an interesting way to produce that following e. So that metric kappa_s^t just doesn*t seem very meaningful. This remark relates to the last paragraph of Sec4.2. Even though the list of cons here is longer than pro*s, I recommend accept; specifically because the originality of this work will in any case make it more vulnerable to critiques. This work is well-motivated, very well-executed, and can inspire many more interesting investigations along these lines.  The authors present a character language model that gains some interpretability without large losses in predictivity. CONTRIBUTION: I*d characterize the paper as some experimental investigation of a cute insight. Recall that multi-class logistic regression allows you to apportion credit for a prediction to the input features: some features raised the probability of the correct class, while others lowered it. This paper points out that a sufficiently simple RNN model architecture is log-linear in the same way, so you can apportion credit for a prediction among elements of the past history. PROS: The paper is quite well-written and was fun to read. It*s nice to see that a simple architecture still does respectably. It*s easy to imagine using this model for a classroom assignment. It should be easy to implement, and the students could replicate the authors* investigation of what influences the network*s predictions. The authors present some nice visualizations. Section 5.2 also describes some computational benefits. CAVEATS ON PREDICTIVE ACCURACY: * Figure 1 says that the ISAN has *near identical performance to other architectures.* But this appears true only when comparing the largest models. Explanation: It appears that for smaller parameter sizes, a GRU still beats the authors* model by 22% to 39% in the usual metric of perplexity per word (ppw). (That*s how LM people usually report performance, with a 10% reduction in ppw traditionally being considered a good Ph.D. dissertation. I assumed an average of 7 chars/word when converting cross-entropy/char to perplexity/word.) * In addition, it*s not known whether this model family will remain competitive beyond the toy situations tested here. Explanation: The authors tried it only on character-based language modeling, and only on a 10M-char dataset, so their ppw is extremely high: 2135 for the best models in this paper. By contrast, a word-based RNN LM trained on 44M words gets ppw of 133, and trained on 800M words gets ppw of 51. [Numbers copied from the paper I cited before: https://transacl.org/ojs/index.php/tacl/article/view/561 .] Those are language models that are good enough to use for something; maybe the authors* model would continue to fare well in this regime, but we just don*t know. (Has the Text8 benchmark in this paper been seriously used for language modeling before? It was designed for text compression, a rather different setting where smaller datasets are meaningful because compression is done online, without a training/test split as done for language modeling. The baseline results in this paper are drawn from a contemporaneous submission with many of the same authors.) CAVEATS ON INTERPRETABILITY: I liked the visualizations as an educational tool. Maybe they*ll inspire other visualization ideas for other models. On the other hand, I*m not sure whether one gets much actionable information from these visualizations: * Sometimes, visualization is used as a way to understand what a model is doing wrong so that you can fix the model. But that might not work here: this model doesn*t seem to have a lot of room for adjustment before it would stop being interpretable. (Although you could leave the model alone and preprocess the input data, I guess ...) * Sometimes, visualization is used to explain a single machine prediction to a human who will make the final decision about whether to trust that prediction (e.g., Singh et al.*s LIME paper). It*s hard to imagine how that would work in this kind of SEQUENTIAL prediction setting, though. OTHER COMMENTS: Most of my technical reactions are already given in my pre-review questions. Thanks to the authors for their answers, and I appreciate that they are running followup experiments for the next version of the paper!","['Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",6,4,"Strengths: 
1. The paper is well-written and clearly motivated. The authors clearly explain the main ideas and the experiments are well-designed. 
2. The paper is original and novel. The authors present a simple RNN model with linear dynamics for language modeling. This is a good contribution to the research dialogue. 

Weaknesses: 
1. The results on both parts (ISAN model, and analysis of it) are not entirely convincing: (1) ISAN is only trained on small task (text8), not clear whether it can be a strong char-LM on larger scale tasks, (2) nor do the analysis sections provide all that much real insight in the learned network. (1b) Other caveat towards ISAN architecture: this model in its proposed form is really only fit for small-vocabulary (i.e. character-based) language modeling, not a general RNN with large-vocabulary discrete input nor continuous input. (2a) For analysis: many cute plots and fun ideas of quantities to look at, but not much concrete insights. (2b) Not very clear which analysis is specific to the ISAN model, and which ideas will generalize to general nonlinear RNNs. (2c) Re sec 4.2 - 4.3: It seems that the quantity kappa_s^t on which analysis rests, is not all that meaningful. Elaborating a bit on what I wrote in the question: For example: Fig"
DRAGNN: A Transition-Based Framework for Dynamically Connected Neural Networks | OpenReview,"In this work, we present a compact, modular framework for constructing new recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and recursive tree-structured models. A TBRU can also serve as both an {em encoder} for downstream tasks and as a {em decoder} for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.","The paper proposes a new neural architecture, called DRAGNN, for the transition-based framework. A DRAGNN uses TBRUs which are neural units to compute hidden activations for the current state of a transition-based system. The paper proves that DRAGNNs can cover a wide range of transition-based methods in the literature. In addition, one can easily implement multitask learning systems with DRAGNNs. The experimental results shows that using DRAGNNs the authors built (near) state-of-the-art systems for 2 tasks: parsing and summarization. The paper contains two major parts: DRAGNN and demonstrations of its usages. Regarding to the first part, the proposed DRAGNN is a neat tool for building any transition-based systems. However, it is difficult to say whether the DRAGNN is novel. Transition-based framework is already well defined and there*s a huge trend in NLP using neural networks to implement transition-based systems. In my opinion, the difference between the Stack-LSTM (Dyer et al., 2015) and DRAGNN is slight. Of course, the DRAGNN is a powerful architecture but the contribution here should be considered mainly in terms of software engineering. In the second part, the authors used DRAGNN to implement new transition-based systems for different (multi-)tasks. The implementations are neat, confirming that DRAGNN is a powerful architecture, especially for multitask learning. However, we should bear in mind that the solutions employed are already there in the literature, thus making difficult to judge the novelty of this part w.r.t. the theme of the conference.  The authors present a general framework for defining a wide variety of recurrent neural network architectures, including seq2seq models, tree-structured models, attention, and a new family of dynamically connected architectures. The framework defines a new, general-purpose recurrent unit called the TBRU, which takes a transition system, defining and constraining its inputs and outputs, and input function which defines the mapping between raw inputs and fixed-width vector representations, and recurrence function that defines the inputs to each recurrent step as a function of the current state, and an RNN cell that computes the output from the input (fixed and recurrent). Many example instantiations of this framework are provided, including sequential tagging RNNs, Google’s Parsey McParseface parser, encoder/decoder networks, tree LSTMs and less familiar examples that demonstrate the power this framework. The most interesting contribution of this work is the ease by which it can be used to incorporate dynamic recurrent connections through the definition of the transition system. In particular, this paper explores the application of these dynamic connections to syntactic dependency parsing, both as a standalone task, and by multitasking parsing with extractive summarization, using the same compositional phrase representations as features for the parser and summarization (previous work used discrete parse features), which is particularly simple/elegant in this framework. In experimental results, the authors demonstrate that such multitasking leads to more accurate summarization models, and using the framework to incorporate more structure into existing parsing models also leads to increased accuracy with no big-oh efficiency loss (compared with e.g. attention). The “raison d’etre,” in particular the example, perhaps described even more thoroughly/explicitly, should be made as clear as possible as soon as possible. This is the most important contribution, but it gets lost in the description and presentation as a framework — emphasizing that attention, seq2seq, etc can be represented in the framework is distracting and makes it seem less novel than it is. AnonReviewer6 clearly missed this point, as did I in my first pass over the paper. To get this idea across and to emphasize the benefits of this representation, I’d love to see more detailed analysis of these representations and their importance to achieving your experimental results. I think it would also be helpful to emphasize the difference between a stack LSTM and Example 6. Overall I think this paper presents a valuable contribution, though the exposition could be improved and analysis of experimental results expanded.  Overall, this is a nice paper. Developing a unifying framework for these newer neural models is a worthwhile endeavor. However, it*s unclear if the DRAGNN framework (in its current form) is a significant standalone contribution. The main idea is straightforward: use a transition system to unroll a computation graph. When you implement models in this way you can reuse code because modules can be mixed and matched. This is nice, but (in my opinion) is just good software engineering, not machine learning research. Moreover, there appears to be little incentive to use DRAGNN, as there are no *free things* (benefits) that you get by using the framework. For example: - If you write your neuralnet in an automatic differentiation library (e.g., tensorflow or dynet) you get gradients for *free*. - In the VW framework, there are efficiency tricks that *the credit assignment compiler* provides for you, which would be tedious to implement on your own. There is also a variety of algorithms for training the model in a principled way (i.e., without exposure bias). I don*t feel that my question about the limitations of the framework has been satisfactorily addressed. Let me ask it in a different way: Can you give me examples of a few models that I can*t (nicely) express in the DRAGNN framework? What if I wanted to implement https://openreview.net/pdf?id=HkE0Nvqlg or http://www.cs.jhu.edu/~jason/papers/rastogi+al.naacl16.pdf? Can I implement the dynamic programming components as transition units and (importantly) would it be efficient? disagree that the VW framework is orthogonal, it is a *competing* way to implement recurrent models. The main different to me appears to be that VW*s imperative framework is more general, but less modular. The experimental contribution seems useful as does the emphasis on how easy it is to incorporate multi-task learning. Minor: - It would be useful to see actual code snippets (possibly in an appendix). Otherwise, its unclear how modular DRAGNN really are. - The introduction states that (unlike seq2seq+attention) inference remains linear. Is this *necessarily* the case? Users define a transition system that is quadratic, just let attention be over all previous states. I recommend that authors rephrase statement more carefully. - It seems strange to use A() as in *actions*, then use d as *decision* for its elements. - I recommend adding i as an argument to the definition of the recurrence function r(s) to make it clear that it*s the subset of previous states at time i, otherwise it looks like an undefined variable. A nice terse option is to write r(s_i). - Real numbers should be mathbb{R} not mathcal{R}. - It*s more conventional to use t for a time-step instead of i. - Example 2: *52 feature embeddings* -> did you mean *52-DIMENSIONAL feature embeddings*?","['Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",6,3,Strengths: 1. The paper presents a novel framework for building recurrent neural networks. 2. The framework is well-defined and easy to understand. 3. The authors provide a good set of examples to demonstrate the power of the framework. 4. The paper is well-written and easy to follow. Weaknesses: 1. The paper does not provide a clear motivation for the framework. 2. The framework is not a significant contribution to the field. 3. The paper does not provide a clear analysis of the experimental results. 4. The paper does not provide a clear comparison to existing methods. Rating: 6 Confidence: 3
Gradients of Counterfactuals | OpenReview,"Gradients have been used to quantify feature importance in machine learning models. Unfortunately, in nonlinear deep networks, not only individual neurons but also the whole network can saturate, and as a result an important input feature can have a tiny gradient. We study various networks, and observe that this phenomena is indeed widespread, across many inputs. We propose to examine interior gradients, which are gradients of counterfactual inputs constructed by scaling down the original input. We apply our method to the GoogleNet architecture for object recognition in images, as well as a ligand-based virtual screening network with categorical features and an LSTM based language model for the Penn Treebank dataset. We visualize how interior gradients better capture feature importance. Furthermore, interior gradients are applicable to a wide variety of deep networks, and have the attribution property that the feature importance scores sum to the the prediction score. Best of all, interior gradients can be computed just as easily as gradients. In contrast, previous methods are complex to implement, which hinders practical adoption.","This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks. The interior gradient is the gradient measured on a scaled version of the input. The integrated gradient is the integral of interior gradients over all scaling factors. Visualizations comparing integrated gradients with standard gradients on real images input to the Inception CNN show that integrated gradients correspond to an intuitive notion of feature importance. While motivation and qualitative examples are appealing, the paper lacks both qualitative and quantitative comparison to prior work. Only the baseline (simply the standard gradient) is presented as reference for qualitative comparison. Yet, the paper cites numerous other works (DeepLift, layer-wise relevance propagation, guided backpropagation) that all attack the same problem of feature importance. Lack of comparison to any of these methods is a major weakness of the paper. I do not believe it is fit for publication without such comparisons. My pre-review question articulated this same concern and has not been answered.  The authors propose to measure “feature importance”, or specifically, which pixels contribute most to a network’s classification of an image. A simple (albeit not particularly effective) heuristic for measuring feature importance is to measure the gradients of the predicted class wrt each pixel in an input image I. This assigns a score to each pixel in I (that ranks how much the output prediction would change if a given pixel were to change). In this paper, the authors build on this and propose to measure feature importance by computing gradients of the output wrt scaled version of the input image, alpha*I, where alpha is a scalar between 0 and 1, then summing across all values of alpha to obtain their feature importance score. Here the scaling is simply linear scaling of the pixel values (alpha=0 is all black image, alpha=1 is original image). The authors call these scaled images “counterfactuals” which seems like quite an unnecessarily grandiose name for literally, a scaled image. The authors show a number of visualizations that indicate that the proposed feature importance score is more reasonable than just looking at gradients only with respect to the original image. They also show some quantitative evidence that the pixels highlighted by the proposed measure are more likely to fall on the objects rather than spurious parts of the image (in particular, see figure 5). The method is also applied to other types of networks. The quantitative evidence is quite limited and most of the paper is spent on qualitative results. While the goal of understanding deep networks is of key importance, it is not clear whether this paper really help elucidate much. The main interesting observation in this paper is that scaling an image by a small alpha (i.e. creating a faint image) places more “importance” on pixels on the object related to the correct class prediction. Beyond that, the paper builds a bit on this, but no deeper insight is gained. The authors propose some hand-wavy explanation of why using small alpha (faint image) may force the network to focus on the object, but the argument is not convincing. It would have been interesting to try to probe a bit deeper here, but that may not be easy. Ultimately, it is not clear how the proposed scheme for feature importance ranking is useful. First, it is still quite noisy and does not truly help understand what a deep net is doing on a particular image. Performing a single gradient descent step on an image (or on the collection of scaled versions of the image) hardly begins to probe the internal workings of a network. Moreover, as the authors admit, the scheme makes the assumption that each pixel is independent, which is clearly false. Considering the paper presents a very simple idea, it is far too long. The main paper is 14 pages, up to 19 with references and appendix. In general the writing is long-winded and overly verbose. It detracted substantially from the paper. The authors also define unnecessary terminology. “Gradients of Coutnerfactuals” sounds quite fancy, but is not very related to the ideas explored in the writing. I would encourage the authors to tighten up the writing and figures down to a more readable page length, and to more clearly spell out the ideas explored early on.  This work proposes to use visualization of gradients to further understand the importance of features (i.e. pixels) for visual classification. Overall, this presented visualizations are interesting, however, the approach is very ad hoc. The authors do not explain why visualizing regular gradients isn*t correlated with the importance of features relevant to the given visual category and proceed to the interior gradient approach. One particular question with regular gradients at features that form the spatial support of the visual class. Is it the case that the gradients of the features that are confident of the prediction remain low, while those with high uncertainty will have strong gradients? With regards to the interior gradients, it is unclear how the scaling parameter alpha affects the feature importance and how it is related to attention. Finally, does this model use batch normalization?","['Rating:###3: Clear rejection', 'Rating:###3: Clear rejection', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",3,4,Strengths: 1. The paper proposes a novel method for visualizing the importance of features in a deep neural network. 2. The authors provide a clear and concise description of the method. 3. The paper is well-structured and easy to follow. Weaknesses: 1. The paper lacks a clear motivation for the proposed method. 2. The authors do not provide a clear comparison to prior work. 3. The paper is too long. 4. The writing is overly verbose. 5. The paper does not provide a clear explanation of the assumptions made. 6. The paper does not provide a clear explanation of the limitations of the proposed method. Rating: 6 Confidence: 3
Tensorial Mixture Models | OpenReview,"We introduce a generative model, we call Tensorial Mixture Models (TMMs) based on mixtures of basic component distributions over local structures (e.g. patches in an image) where the dependencies between the local-structures are represented by a *priors tensor* holding the prior probabilities of assigning a component distribution to each local-structure. In their general form, TMMs are intractable as the priors tensor is typically of exponential size. However, when the priors tensor is decomposed it gives rise to an arithmetic circuit which in turn transforms the TMM into a Convolutional Arithmetic Circuit (ConvAC). A ConvAC corresponds to a shallow (single hidden layer) network when the priors tensor is decomposed by a CP (sum of rank-1) approach and corresponds to a deep network when the decomposition follows the Hierarchical Tucker (HT) model. The ConvAC representation of a TMM possesses several attractive properties. First, the inference is tractable and is implemented by a forward pass through a deep network. Second, the architectural design of the model follows the deep networks community design, i.e., the structure of TMMs is determined by just two easily understood factors: size of pooling windows and number of channels. Finally, we demonstrate the effectiveness of our model when tackling the problem of classification with missing data, leveraging TMMs unique ability of tractable marginalization which leads to optimal classifiers regardless of the missingness distribution.","This paper proposes a generative model for mixtures of basic local structures where the dependency between local structures is a tensor. They use tensor decomposition and the result of their earlier paper on expressive power of CNNs along with hierarchical Tucker to provide an inference mechanism. However, this is conditioned on the existence of decomposition. The authors do not discuss how applicable their method is for a general case, what is the subspace where this decomposition exists/is efficient/has low approximation error. Their answer to this question is that in deep learning era these theoretical analysis is not needed. While this claim is subjective, I need to emphasize that the paper does not clarify this claim and does not mention the restrictions. Hence, from theoretical perspective, the paper has flaws and the claims are not justified completely. Some claims cannot be justified with the current results in tensor literature as the authors also mentioned in the discussions. Therefore, they should have corrected their claims in the paper and made the clarifications that this approach is restricted to a clear subclass of tensors. If we ignore the theoretical aspect and only consider the paper from empirical perspective, the experiments the appear in the paper are not enough to accept the paper. MNIST and CIFAR-10 are very simple baselines and more extensive experiments are required. Also, the experiments for missing data are not covering real cases and are too synthetic. Also, the paper lacks the extension beyond images. Since the authors repeatedly mention that their approach goes beyond images, and since the theory part is not complete, those experiments are essential for acceptance of this paper.  This paper uses Tensors to build generative models. The main idea is to divide the input into regions represented with mixture models, and represent the joint distribution of the mixture components with a tensor. Then, by restricting themselves to tensors that have an efficient decomposition, they train convolutional arithmetic circuits to generate the probability of the input and class label, providing a generative model of the input and labels. This approach seems quite elegant. It is not completely clear to me how the authors choose the specific architecture for their model, and how these choices relate to the class of joint distributions that they can represent, but even if these choices are somewhat heuristic, the overall framework provides a nice way of controlling the generality of the distributions that are represented. The experiments are on simple, synthetic examples of missing data. This is somewhat of a limitation, and the paper would be more convincing if it could include experiments on a real-world problem that contained missing data. One issue here is that it must be known which elements of the input are missing, which somewhat limits applicability. Could experiments be run on problems relating to the Netflix challenge, which is the classic example of a prediction problem with missing data? In spite of these limitations, the experiments provide appropriate comparisons to prior work, and form a reasonable initial evaluation. I was a little confused about how the input of missing data is handled experimentally. From the introductory discussion my impression was that the generative model was built over region patches in the image. This led me to believe that they would marginalize over missing regions. However, when the missing data consists of IID randomly missing pixels, it seems that every region will be missing some information. Why is it appropriate to marginalize over missing pixels? Specifically, in Equation 6 represents a local region, and the ensuing discussion shows how to marginalize over missing regions. How is this done when only a subset of a region is missing? It also seems like the summation in the equation following Equation 6 could be quite large. What is the run time of this? The paper is also a bit schizophrenic about the extent to which the results are applicable beyond images. The motivation for the probabilistic model is mostly in terms of images. But in the experiments, the authors state that they do not use state-of-the-art inpainting algorithms because their method is not limited to images and they want to compare to methods that are restricted to images. This would be more convincing if there were experiments outside the image domain. It was also not clear to me how, if at all, the proposed network makes use of translation invariance. It is widely assumed that much of the success of CNNs comes from their encoding of translation invariance through weight sharing. Is such invariance built into the authors’ network? If not, why would we expect it to work well in challenging image domains? As a minor point, the paper is not carefully proofread. To just give a few examples from the first page or so: “significantly lesser” -> “significantly less” “the the” “provenly” -> provably  The paper provides an interesting use of generative models to address the classification with missing data problem. The tensorial mixture models proposed take into account the general problem of dependent samples. This is an nice extension of current mixture models where samples are usually considered as independent. Indeed the TMM model is reduced to the conventional latent variable models. As much as I love the ideas behind the paper, I feel pitiful about the sloppiness of the presentation (such as missing notations) and flaws in the technical derivations. Before going into the technical details, my high level concerns are as follows: (1) The joint density over all samples is modeled as a tensorial mixture generative model. The interpretation of the CP decomposition or HT decomposition on the prior density tensor is not clear. The authors have an interpretation of TMM as product of mixture models when samples are independent, however their interpretation seems flawed to me, and I will elaborate on this in the detailed technical comments below. (2) The authors employ convolution operators to compute an inner product. It is realizable by zero padding, but the invariance structure, which is the advantage of CNN compared to feed-forward neural network, will be lost. However, I am not sure how much this would affect the performance in practice. (3) The author could comment in the paper a little bit on the sample complexity of this method given the complexity of the model. Because I liked the ideas of the paper so much, and the ICLR paper submitted didn*t present the technical details well due to sloppiness of notations, so I read the technical details in the arXiv version the authors pointed out. There are a few technical typos that I would like to point out (my reference to equations are to the ones in the arXiv paper). (1) The generative model as in figure (5) is flawed. P(x_i|d_i; heta_{d_i}) are vectors of length s, there the product of vectors is not well defined. It is obvious that the dimensions of the terms between two sides of the equation are not equal. In fact, this should be a tucker decomposition instead of multiplication. It should be P(X) = sum_{d1,ldots,d_N} P(d_1,ldots,d_N) (P(x_1|d_1;theta_{d_1},P(x_2|d_2;theta_{d_2},ldots,P(x_N|d_N;theta_{d_N}), which means a sum of multi-linear operation on tensor P(d_1,ldots,d_N), and each mode is projected onto P(x_i|d_i;theta_{d_i}. (2) I suspect the special case for diagonal Gaussian Mixture Models has some typos as I couldn*t derive the third last equation on page 6. But it might be just I didn*t understand this example. (3) The claim that TMM reduces to product of mixture model is not accurate. The first equation on page 7 is only right when *sum of product* operation is equal to *product of sum* operation. Similarly, in equation (6), the second equality doesn*t hold unless in some special cases. However, this is not true. This might be just a typo, but it is good if the authors could fix this. I also suspect that if the authors correct this typo,the performance on MNIST might be improved. Overall, I like the ideas behind this paper very much. I suggest the authors fix the technical typos if the paper is accepted.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###7: Good paper, accept', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",4,4,Strengths: 1. The paper presents a novel approach to generative models for classification with missing data. 2. The authors provide a clear and simple framework for modeling the joint distribution of the input and class label. 3. The paper is well-written and easy to follow. Weaknesses: 1. The paper is not well-clarified in its theoretical aspects. The authors do not discuss the restrictions on the decomposition of the prior tensor and the implications of these restrictions. 2. The paper lacks experiments on real-world problems. The experiments are mostly on simple synthetic examples of missing data. 3. The paper does not discuss the sample complexity of the method. 4. The paper does not discuss the invariance structure of the model. 5. The paper does not discuss the computational complexity of the method. Rating: 6 Confidence: 3
CONTENT2VEC: SPECIALIZING JOINT REPRESENTATIONS OF PRODUCT IMAGES AND TEXT FOR THE TASK OF PRODUCT RECOMMENDATION | OpenReview,"We propose a unified product embedded representation that is optimized for the task of retrieval-based product recommendation. We generate this representation using Content2Vec, a new deep architecture that merges product content infor- mation such as text and image and we analyze its performance on hard recom- mendation setups such as cold-start and cross-category recommendations. In the case of a normal recommendation regime where collaborative information signal is available we merge the product co-occurence information and propose a sec- ond architecture Content2vec+ and show its lift in performance versus non-hybrid approaches.","The problem of utilizing all available information (across modalities) about a product to learn a meaningful *joint* embedding is an interesting one, and certainly seems like it a promising direction for improving recommender systems, especially in the *cold start* scenario. I*m unaware of approaches combining as many modalities as proposed in this paper, so an effective solution could indeed be significant. However, there are many aspects of the proposed architecture that seem sub-optimal to me: 1. A major benefit of neural-network based systems is that the entire system can be trained end-to-end, jointly. The proposed approach sticks together largely pre-trained modules for different modalities... this can be justifiable when there is very little training data available on which to train jointly. With 10M product pairs, however, this doesn*t seem to be the case for the Amazon dataset (although I haven*t worked with this dataset myself so perhaps I*m missing something... either way it*s not discussed at all in the paper). I consider the lack of a jointly fine-tuned model a major shortcoming of the proposed approach. 2. The discussion of *pairwise residual units* is confusing and not well-motivated. The residual formulation (if I understand it correctly) applies a ReLU layer to the concatenation of the modality specific embeddings, giving a new similarity (after dot products) that can be added to the similarity obtained from the concatenation directly. Why not just have an additional fully-connected layer that mixes the modality specific embeddings to form a final embedding (perhaps of lower dimensionality)? This should at least be presented as a baseline, if the pairwise residual unit is claimed as a contribution... I don*t find the provided explanation convincing (in what way does the residual approach reduce parameter count?). 3. More minor: The choice of TextCNN for the text embedding vectors seems fine (although I wonder how an LSTM-based approach would perform)... However the details surrounding how it is used are obscured in the paper. In response to a question, the authors mention that it runs on the concatenation of the first 10 words of the title and product description. Especially for the description, this seems insufficiently long to contain a lot of information to me. More care could be given to motivating the choices made in the paper. Finally, I*m not familiar with state of the art on this dataset... do the comparisons accurately reflect it? It seems only one competing technique is presented, with none on the more challenging cold-start scenarios. Minor detail: In the second paragraph of page 3, there is a reference that just says (cite Julian).  This paper proposes combining different modalities of product content (e.g. review text, images, co-purchase info ...etc) in order to learn one unified product representation for recommender systems. While the idea of combining multiple sources of information is indeed an effective approach for handling data sparsity in recommender systems, I have some reservations on the approach proposed in this paper: 1) Some modalities are not necessarily relevant for the recommendation task or item similarity. For example, cover images of books or movies (which are product types in the experiments of this paper) do not tell us much about their content. The paper should clearly motivate and show how different modalities contribute to the final task. 2) The connection between the proposed joint product embedding and residual networks is a bit awkward. The original residual layers are composed of adding the original input vector to the output of an MLP, i.e. several affine transformations followed by non-linearities. These layers allow training very deep neural networks (up to 1000 layers) as a result of easier gradient flow. In contrast, the pairwise residual unit of this paper adds the dot product of two item vectors to the dot product of the same vectors but after applying a simple non-linearity. The motivation of this architecture is not very obvious, and is not well motivated in the paper. 3) While it is a minor point, but the choice of the term embedding for the dot product of two items is not usual. Embeddings usually refer to vectors in R^n, and for specific entities. Here it refers to the final output, and renders the output layer in Figure 2 pointless. Finally, I believe the paper can be improved by focusing more on motivating architectural choices, and being more concise in your description. The paper is currently very long (11 pages) and I strongly encourage you to shorten it.  The paper proposes a method to combine arbitrary content into recommender systems, such as images, text, etc. These various features have been previously used to improve recommender systems, though what*s novel here is the contribution of a general-purpose framework to combine arbitrary feature types. Positively, the idea of combining many heterogeneous feature types into RS is ambitious and fairly novel. Previous works have certainly sought to include various feature types to improve RSs, though combining different features types successfully is difficult. Negatively, there are a few aspects of the paper that are a bit ad-hoc. In particular: -- There are a lot of pieces here being *glued together* to build the system. Different parts are trained separately and then combined together using another learning stage. There*s nothing wrong with doing things in this way (and indeed it*s the most straightforward and likely to work approach), but it pushes the contribution more toward the *system building* direction as opposed to the *end-to-end learning* direction which is more the focus of this conference. -- Further to the above, this makes it hard to say how easily the model would generalize to arbitrary feature types, say e.g. if I had audio or video features describing the item. To incorporate such features into the system would require a lot of implementation work, as opposed to being a system where I can just throw more features in and expect it to work. The pre-review comments address some of these issues. Some of the responses aren*t entirely convincing, e.g. it*d be better to have the same baselines across tables, rather than dropping some because *the case had already been made elsewhere*. Other than that, I like the effort to combine several different feature types in real recommender systems datasets. I*m not entirely sure how strong the baselines are, they seem more like ablation-style experiments rather than comparison against any state-of-the-art RS.","['Rating:###3: Clear rejection', 'Rating:###3: Clear rejection', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",3,3,"Strengths: 1. The paper proposes a novel approach to combining different modalities of product content to learn a unified product representation for recommender systems. 2. The authors provide a clear and concise description of the proposed approach. Weaknesses: 1. The paper lacks a clear motivation for the proposed approach. The authors do not provide a clear explanation of why combining different modalities of product content is beneficial for recommender systems. 2. The proposed approach is not well-motivated. The authors do not provide a clear explanation of why the pairwise residual unit is beneficial. 3. The paper lacks a clear comparison to state-of-the-art approaches. The authors only compare their approach to a baseline approach, but do not provide a clear comparison to other state-of-the-art approaches. Rating: 6 Confidence: 3"
Cortical-Inspired Open-Bigram Representation for Handwritten Word Recognition | OpenReview,"Recent research in the cognitive process of reading hypothesized that we do not read words by sequentially recognizing letters, but rather by identifing open-bigrams, i.e. couple of letters that are not necessarily next to each other. In this paper, we evaluate an handwritten word recognition method based on original open-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) to predict open-bigrams rather than characters, and we show that such models are able to learn the long-range, complicated and intertwined dependencies in the input signal, necessary to the prediction. For decoding, we decomposed each word of a large vocabulary into the set of constituent bigrams, and apply a simple cosine similarity measure between this representation and the bagged RNN prediction to retrieve the vocabulary word. We compare this method to standard word recognition techniques based on sequential character recognition. Experiments are carried out on two public databases of handwritten words (Rimes and IAM), an the results with our bigram decoder are comparable to more conventional decoding methods based on sequences of letters.","This paper explores the use of Open Bigrams as a target representation of words, for application to handwriting image recognition. Pros: - The use of OBs is novel and interesting. - Clearly written and explained. Cons: - No comparison to previous state of the art, only with author-generated results. - More ablation studies needed -- i.e. fill in Table3 with rnn0,1 rnn0,1,2 rnn0,1* etc etc. It is not clear where the performance is coming from, as it seems that it is single character modelling (0) and word endings (*) that are actually beneficial. - While the use of Open bigrams is novel, there are works which use bag of bigrams and ngrams as models which are not really compared to or explored. E.g. https://arxiv.org/abs/1406.2227 [1] and https://arxiv.org/abs/1412.5903 [2]. Both use bag of ngrams models and achieve state of the art results, so it would be interesting to see whether open bigrams in the same experimental setup as [1] would yield better results. - Why not use a graph-based decoder like in Fig 2 b? Overall an interesting paper but the lack of comparisons and benchmarks makes it difficult to assess the reality of the contributions.  This paper uses an LSTM model to predict what it calls *open bigrams* (bigrams of characters that may or may not have letters inbetween) from handwriting data. These open bigrams are subsequently used to predict the written word in a decoding step. The experiments indicate that the system does slightly better than a baseline model that uses Viterbi decoding. I have some major concerns about this paper: - I find the *cortical inspired* claim troublesome. If anything, it is psychology/cognitive science inspired, in the sense that open bigrams appear to help for word recognition (Touzet et al. 2014). But the implied cortical characteristics, implicitly referred to e.g. by pointing to analogies between deep neural nets for object recognition and in that case the visual cortex, is unfounded. Is there any direct evidence from neuroscience that open-bigrams constitute a wholly separate layer in the cortex for a handwriting recognition task? Dehaene*s work is a proposal, so you*ll need to describe more *findings in cognitive neurosciences [sic] research on reading* (p. 8) to substantiate those claims. I am further worried by the fact that the authors seem to think that *deep neural networks are based on a series of about five pairs of neurons [sic] layers*. Unless I misunderstand something, you are specifically referring to Krizhevsky*s AlexNet here (which you should probably have cited there)? I hope you don*t mean to imply that all deep neural nets need five layers. It is also not true that ten is *quite close to the number of layers of an efficient deep NN* -- what network? what task? etc. - The model is not clearly explained. There is a short paragraph in Appendix A.3. that roughly describes the setup, but this does not include e.g. the objective function, or answer why the network output is only considered each two consecutive time steps, rather than at each time step (or so it seems?). This is probably because the paper argues that it *is focused on the decoder* (p. 6), rather than on the whole problem. I find this problematic, because in that case we*re effectively measuring how easy it is to reconstruct a word from its open bigrams, which has very little to do with handwriting recognition (it could have been evaluated on any text corpus). In fact, as the example on page 4 shows, handwriting is not necessary to illustrate the open bigram hypothesis. Which leads me to wonder why these particular tasks were chosen, if we are only interested in the decoding mechanism? - The comparison is not really fair. The Viterbi decoder only has access to unigrams, as far as I can tell. The only model that does better than that baseline has access to a lot more information, and does not do that much better. Did the Viterbi model have access to the word boundary information (at one point rather confusingly called *extremities*) that pushed the open bigram model over the edge in terms of performance? Why is there no comparison to e.g. rnn_0,1* (unigram+bigram+boundary markers)? The dataset also appears to be biased in favor of the proposed approach (longer words, only ). I am not convinced that this paper really shows that open bigrams help. I very much like the idea of the paper, but I am simply not convinced by its claims. Minor points: - There are quite a few typos. Just a sample: *independant* (Fig.1), *we evaluate an handwritten*, *, hand written words [..], an the results*, *their approach include*, *the letter bigrams of a word w is*, *for the two considered database* - Wouldn*t it be easy to add how many times a bigram occurs, which would improve the decoding process? You can just normalize over the full counts instead of the binary occurrence counts. - The results in Table 5 are the same (but different precision) as the results in Table 2, except that edit distance and SER are added, this is confusing.  This submission investigates the usability of cortical-inspired distant bigram representations for handwritten word recognition. Instead of generating neural network based posterior features for character (optionally in local context), sets posterior for character bigrams of different length are used to represent words. The aim here is to investigate the viability of this approach and to compare to the standard approach. Overall, the submission is well written, although information is missing w.r.t. to the comparison between the proposed approach and the standard approach, see below. It would be desirable to see the model complexity of all the different models used here, i.e. the number of parameters used. Language models are not used here. Since the different models utilize different levels of context, language models can be expected to have a different effect on the different approaches. Therefore I suggest to include the use of language models into the evaluation. For your comparative experiments you use only 70% of the data by choosing longer words only. On the other hand, it is well known that the shorter words are more prone to result in misrecognitions. The question remains, if this choice is advantageous for one of the tasks, or not - corresponding quantitative results should be provided to be able to better evaluate the effect of using this constrained corpus. Without clarification of this I would not readily agree that the error rates are competitive or better than the standard approach, as stated at the end of Sec. 5. I do see the motivation for introducing open-bigrams in an unordered way due to the corresponding evidence from cognitive research. However, decision theoretically I wonder, why the order should be given up, if the underlying sequential classification problem clearly is of a monotonous nature. It would be interesting to see an experiment, where only the use of the order is varied, to differentiate the effect of the order from the effect of other aspects of the approach. End of page 1: *whole language method* - please explain what is meant by this. Page 6: define your notation for rnn_d(x,t). The number of target for the RNNs modeling order 0 (unigrams effectively) and the RNNs modeling order 1 and larger are very much different. Therefore the precision and recall numbers in Table 2 do not seem to be readily comparable between order 0 and orders >=1. At least, the column for order 0 should be visually separated to highlight this. Minor comments: a spell check is recommended p. 2: state-of-art -> state-of-the-art p. 2: predict character sequence -> predict a character sequence p. 3, top: Their approach include -> Their approach includes p. 3, top: an handwritten -> a handwritten p. 3, bottom: consituent -> constituent p. 4, top: in classical approach -> in the classical approach p. 4, top: transformed in a vector -> transformed into a vector p. 5: were build -> were built References: first authors name written wrongly: Thodore Bluche -> Theodore Bluche","['Rating:###5: Marginally below acceptance threshold', 'Rating:###4: Ok but not good enough - rejection', 'Rating:###7: Good paper, accept']","['Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",5,5,Strengths: 1. The paper is well-written and easy to follow. 2. The idea of using open bigrams as a representation for handwritten words is interesting and novel. Weaknesses: 1. The paper lacks a clear motivation for the use of open bigrams. What is the theoretical basis for this approach? 2. The paper lacks a clear comparison to the standard approach. What are the standard approaches? How do they compare to the proposed approach? 3. The paper lacks a clear explanation of the model. What is the objective function? How are the model parameters learned? 4. The paper lacks a clear explanation of the experimental setup. What is the dataset used? How was it split? What is the baseline model? 5. The paper lacks a clear explanation of the results. What are the results? How were they obtained? What are the implications of the results? Rating: 6 Confidence: 3
Counterpoint by Convolution | OpenReview,"Machine learning models of music typically break down the task of composition into a chronological process, composing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, often revisiting choices previously made. We explore the use of blocked Gibbs sampling as an analogue to the human approach, and introduce Coconet, a convolutional neural network in the NADE family of generative models. Despite ostensibly sampling from the same distribution as the NADE ancestral sampling procedure, we find that a blocked Gibbs approach significantly improves sample quality. We provide evidence that this is due to some conditional distributions being poorly modeled. Moreover, we show that even the cheap approximate blocked Gibbs procedure from Yao et al. (2014) yields better samples than ancestral sampling. We demonstrate the versatility of our method on unconditioned polyphonic music generation.","The paper tackles the task of music generation. They use an orderless NADE model for the task of *fill in the notes*. Given a roll of T timesteps of pitches, they randomly mask out some pitches, and the model is trained to predict the missing notes. This follows how the orderless NADE model can be trained. During sampling, one normally follows an ancestral sampling procedure. For this, an ordering is defined over outputs, and one runs the model on the current input, samples one of the outputs according to the order, adds this output to the next input, and continues this procedure until all outputs have been sampled. The key point of the paper is that this is a bad sampling strategy. Instead, they suggest the strategy of Yao et al. 2014, which uses a blocked Gibbs sampling approach. The blocked Gibbs strategy instead masks N inputs randomly and independently, samples them, and repeats this procedure. The point of this strategy is the make sure the sampling chain mixes well, which will happen for large N. However, since the samples are independent, having a large N gives incoherent samples. Thus, the authors follow an annealed schedule for N, making it smaller over time, which will eventually reduce to ancestral sampling (giving global structure to the sample). They conduct a variety of experiments involving both normal metrics and human evaluations, and find that this blocked Gibbs sampling outperforms other sampling procedures. This is a well written paper - great job. My main problem with the paper is that having read Uria and Yao, I don*t know how much I have learned from this work in the context of this being an ICLR submission. If this was submitted to some computational music / art conference, this paper would be a clear accept. However, for ICLR, I don*t see enough novelty compared with previous works this builds upon. Orderless NADE is an established model. The blocked Gibbs sampling and annealing scheme are basically the exact same one used in Yao. Thus, the main novelty of this paper is its application to the music domain, and finding that Yao*s method works better for sampling music. This is a good contribution, but more tailored to those working in the music domain. If the authors found that these results also hold for other domains like images (e.g. on CIFAR / tiny Imagenet) and text (e.g. document generation), then I would change my mind and accept this paper for ICLR. Even just trying musical domains other than Bach chorales would be useful. However, as it stands, the experiments are not convincing enough.  The paper presents a way to model the distribution of four-part Bach chorales using Convolutional Neural Networks. Furthermore it addresses the task of artificial music generation by sampling from the model using blocked Gibbs sampling and shows The CNN model for the distribution seems very appropriate for the data at hand. Also the analysis of the proposed sampling schemes with the analogy between Gibbs sampling and human music composition are very interesting. I am not too sure about the evaluation though. Since the reported likelihoods are not directly comparable to previous work, I have difficulties judging the quality of the quantitative results. For the human evaluation I would like to see the data for the direct comparisons between the models. E.g. How did NADE vs. Bach perform. Also I find the question: ‘what piece of music do you prefer’ a stronger test than the question ‘what piece is more musical to you’ because I don’t really know what ‘musical’ means to the AMT workers. Finally, while I think the Bach Chorales are interesting musical pieces that deserve to be subject of the analysis but I find it hard to judge how well this modelling approach will transfer to other types of music which might have a very different data distribution. Nevertheless, in conclusion, I believe this is an exciting model for an interesting task that produces non-trivial musical data.  This paper proposed COCONET, which is a neural autoregressive model with convolution, to do music composition task. This paper also proposed to use blocked Gibbs sampling instead of the ancestral sampling of the original NADE model to generate better pieces of music. The experimental results showed that the NLL of COCONET is better than the other baselines and the human evaluation task by Amazon’s Mechanical Turk illustrated that the model can generate compelling music. In general, I think the paper is good. Using NADE based model with convolution operations on music generation tasks and using blocked Gibbs sampling contains some kind of novelty. However, the novelty of the paper is incremental, since the blocked Gibbs sampling for NADE model is already proposed by Yao et al., (2014) and the using NADE based model for music modeling has also been proposed by Boulanger-Lewandowski et al., (2012).","['Rating:###5: Marginally below acceptance threshold', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",5,4,Strengths: 1. The paper presents a novel approach to music generation by using a convolutional neural network to model the distribution of a piece of music. 2. The authors provide a good analysis of the sampling strategy used in the model. 3. The paper is well written and easy to understand. Weaknesses: 1. The paper does not provide a clear motivation for why the authors chose to use a convolutional neural network to model the distribution of a piece of music. 2. The authors do not provide a clear comparison to previous work in the field. 3. The paper does not provide a clear evaluation of the model's performance. Rating: 6 Confidence: 3
FILTER SHAPING FOR CONVOLUTIONAL NEURAL NETWORKS | OpenReview,"Convolutional neural networks (CNNs) are powerful tools for classification of visual inputs. An important property of CNN is its restriction to local connections and sharing of local weights among different locations. In this paper, we consider the definition of appropriate local neighborhoods in CNN. We provide a theoretical analysis that justifies the traditional square filter used in CNN for analyzing natural images. The analysis also provides a principle for designing customized filter shapes for application domains that do not resemble natural images. We propose an approach that automatically designs multiple layers of different customized filter shapes by repeatedly solving lasso problems. It is applied to customize the filter shape for both bioacoustic applications and gene sequence analysis applications. In those domains with small sample sizes we demonstrate that the customized filters achieve superior classification accuracy, improved convergence behavior in training and reduced sensitivity to hyperparameters.","Authors propose a mechanism for selecting the design of filters in convolutional layers. The basic idea is that convolution should be applied to input feature dimensions that are highly correlated in order to detect rare events. For example, adjacent pixels in images are correlated and edges are rare events of interest to be detected. Authors argue that square filters are therefore appropriate in images. However, in data such as bird songs high correlations might exist between non-adjacent harmonics and a convolution filter should take a weighted summation over these input feature dimensions. Such an operation can thus be thought of computing data-dependent dilated convolutions. Paper theoretically motivates this choice using the idea of Gaussian complexity of the learner (i.e. a CNN in this case). The main idea being that choosing convolution filters that sum over correlated features results in lower Gaussian complexity and thus the learner has higher ability to generalize. While I am no expert in theoretical analysis of learning algorithms – there are parts of proof that look sound, but there are parts that are rather hand wavy (for eg, extension to networks using max-pooling from average pooling). Also, the theory is not directly applicable to choosing filters when number of layers are more than 1. I am willing to overlook the paucity in rigor in some parts of the theoretical arguments because the empirical evidence looks convincing. The method of choosing the filter shape can be briefly summarized as: (a) The covariance matrix of the input features is computed. (b) Using the covariance matrix, feature dimensions with highest correlations are determined by solving equation (7). A hard limit on maximum number of filter dimensions is imposed (typically ~ 10-15). This leads to choice of a single design for all filters in the layer. (c) Authors extend the framework to work with multiple layers in the following way: A subset of feature dimensions cannot account for all variance in the inputs and there is some residual variance. The filter design of the next layer attempts to minimize this residual variance. This process is repeated iteratively by solving eq (8) to obtain filter designs for all the layers. Ideally for determining filter designs of different layers – one should have computed the covariance statistics of outputs of the previous layer. However this assumes that filters of the previous layer are already known and this is not computationally feasible to implement. Authors instead use the method described in (c). A question which comes to my mind is – a single feature design is chosen for each layer. Have the authors considered using the process in (c) to chose different filter designs for different filters in the same layer as opposed to using the same filter design for all the filters? Regarding baselines: B1. It would be great to see a comparison with randomly chosen filter designs. Two comparisons could be made – (1a) A single random design is chosen for each layer. (1b) The design of each filter is chosen randomly (i.e. allowing for different designs of filter within each layer). B2. Since the theory is not really applicable to CNNs with more that one layer – I wonder how much of the benefit is obtained by choosing the filter design just in a single layer v/s all the layers. A good comparison would be when filter design of the first layer are chosen using the described method and filters in higher layers are chosen to be square. B3. Authors mention the use L1 regularization in the baselines. Was the L1 penalty cross-validated? If so, then upto what range? Somethings which are unclear: - “exclude data that represent obvious noise” - DFMax mentioned in the supplementary materials Overall I think this is very interesting idea for filter design. The authors have done a fair set of experiments but I would really like to see results of B1, B2 and the answer to question in B3. I have currently set my rating to a weak reject, but I am happy to raise my ratings to – “Good paper, accept” if the authors provide results of experiments and answers to questions in my comments above.  This work proposes a way how to learn filter shapes for CNNs in an unsupervised manner for multiple tasks by solving a lasso problem. Even though this method does not seem to be applicable for image classification CNNs (as image data generally do not have bias towards anisotropic structures), it gives an empirical methodology to design filter shapes for tasks with different input data structure. Authors show that this method is applicable for spectrogram classification and gene sequence classification. The paper is well written and and is of interest to the community as it presents a unsupervised method applicable to problems with less training data. Authors compare the performance of the proposed method against reasonable baselines (i.e. handcrafted filter sizes) and based on the evaluation it seems to improve the results and help to avoid over-fitting (probably due to reduced filter size and thus number of parameters). In this way it is an interesting combination of unsupervised methods for a supervised training. Unfortunately, I am not able to validate correctness of the theoretical justification. As a side note: * It would be useful to give some reference showing that using spectrogram for sound classification is a reasonable choice  The paper proposes a method for optimising the shape of filters in convolutional neural network layers, i.e. the structure of their receptive fields. CNNs for images almost invariably feature small square filters (e.g. 3x3, 5x5, ...) and this paper provides an algorithm to optimise this aspect of the model architecture (which is often treated as fixed) based on data. It is argued that this is especially useful for data modalities where the assumption of locality breaks down, as in e.g. spectrograms, where correlations between harmonics are often relevant to the task at hand, but they are not local in frequency space. Improved performance is demonstrated on two tasks that are fairly non-standard, but I think that is fine given that the proposed approach probably isn*t useful for the vast majority of popular benchmark datasets (e.g. MNIST, CIFAR-10), where the locality assumption holds and a square filter shape is probably close to optimal anyway. Fig. 1 is a nice demonstration of this. The paper spends quite a bit of space on a theoretical argument for the proposed method based on Gaussian complexity, which is interesting but maybe doesn*t warrant quite so much detail. In contrast, section 3.3 (about how to deal with pooling) is quite handwavy in comparison. This is probably fine but the level of detail in the preceding sections makes it a bit suspicious. I*m also not 100% convinced that the theoretical argument is particularly relevant, because it seems to rely on some assumptions that are clearly untrue for practical CNNs, such as 1-norm weight constraints and the fact that it is probably okay to swap out the L1 norm for the L2 norm. I would also like to see a bit more discussion about Fig. 4, especially about the fact that some of the filter shapes end up having many fewer nonzeros than the algorithm enforces (e.g. 3 nonzeros for layers 6 and 7, whereas the maximum is 13). Of course this is a perfectly valid outcome as the algorithm doesn*t force the solution to have an exact number of nonzeros, but surely the authors will agree that it is a bit surprising/unintuitive? The same figure also features an interesting phase transition between layers 1-4 and 5-8, with the former 4 layers having very similar, almost circular/square filter shapes, and the later having very different, spread out shapes. Some comments about why this happens would be welcome. Regarding my question about computational performance, I still think that this warrants some discussion in the paper as well. For many new techniques, whether they end up being adopted mainly depends on the ratio between the amount of work that goes into implementing them, and the benefit they provide. I*m not convinced that the proposed approach is very practical. My intuition is that creating efficient implementations of various non-square convolutions for each new problem might end up not being worth the effort, but I could be wrong here. Minor comments: - please have the manuscript proofread for spelling and grammar. - there is a bit of repetition in sections 2 and 3, e.g. the last paragraphs of sections 2.1 and 2.2 basically say the same thing, it would be good to consolidate this. - a few things mentioned in the paper that were unclear to me (*syllables*, *exclude data that represent obvious noise*, choice of *max nonzero elements* parameter) have already been adequately addressed by the authors in their response to my questions, but it would be good to include these answers in the manuscript as well. - the comparison in Fig. 5 with L1 regularisation on the filter weights does not seem entirely fair, since the resulting shape would have to be encompassed in a 5x5 window whereas Fig. 4 shows that the filter shapes found by the algorithm often extend beyond that. I appreciate that training nets with very large square filters is problematic in many ways, but the claim *L1 regularization cannot achieve the same effect as filter shaping* is not really convincingly backed up by this experiment.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",6,4,Strengths: 1. The paper proposes a method for learning filter shapes for CNNs in an unsupervised manner for multiple tasks by solving a lasso problem. 2. The method is applied to tasks with different input data structure. 3. The paper is well written and is of interest to the community as it presents a unsupervised method applicable to problems with less training data. Weaknesses: 1. The theoretical justification of the proposed method is not really applicable to CNNs with more than one layer. 2. The paper does not discuss the computational complexity of the proposed method. 3. The paper does not discuss the computational complexity of the proposed method. 4. The paper does not discuss the computational complexity of the proposed method. 5. The paper does not discuss the computational complexity of the proposed method. 6. The paper does not discuss the computational complexity of the proposed method. 7. The paper does not discuss the computational complexity of the proposed method. 8. The paper does not discuss the computational complexity of the proposed method. 9. The paper does not discuss the computational complexity of the proposed method. 10. The paper does not discuss the computational complexity of the proposed method. Rating: 6 Confidence: 3
Lie-Access Neural Turing Machines | OpenReview,"External neural memory structures have recently become a popular tool for algorithmic deep learning (Graves et al. 2014; Weston et al. 2014). These models generally utilize differentiable versions of traditional discrete memory-access structures (random access, stacks, tapes) to provide the storage necessary for computational tasks. In this work, we argue that these neural memory systems lack specific structure important for relative indexing, and propose an alternative model, Lie-access memory, that is explicitly designed for the neural setting. In this paradigm, memory is accessed using a continuous head in a key-space manifold. The head is moved via Lie group actions, such as shifts or rotations, generated by a controller, and memory access is performed by linear smoothing in key space. We argue that Lie groups provide a natural generalization of discrete memory structures, such as Turing machines, as they provide inverse and identity operators while maintaining differentiability. To experiment with this approach, we implement a simplified Lie-access neural Turing machine (LANTM) with different Lie groups. We find that this approach is able to perform well on a range of algorithmic tasks.","The paper introduces a novel memory mechanism for NTMs based on differentiable Lie groups. This allows to place memory elements as points on a manifold, while still allowing training with backpropagation. It*s a more general version of the NTM memory, and possibly allows for training a more efficient addressing schemes. Pros: - novel and interesting idea for memory access - nicely written Cons: - need to manually specify the Lie group to use (it would be better if network could learn the best way of accessing memory) - not clear if this really works better than standard NTM (compared only to simplified version) - not clear if this is useful in practice (no comparison on real tasks)  The Neural Turing Machine and related “external memory models” have demonstrated an ability to learn algorithmic solutions by utilizing differentiable analogues of conventional memory structures. In particular, the NTM, DNC and other approaches provide mechanisms for shifting a memory access head to linked memories from the current read position. The NTM, which is the most relevant to this work, uses a differentiable version of a Turing machine tape. The controller outputs a kernel which “softly” shifts the head, allowing the machine to read and write sequences. Since this soft shift typically “smears” the focus of the head, the controller also outputs a sharpening parameter which compensates by refocusing the distribution. The premise of this work is to notice that while the NTM emulates a differentiable version of a Turing tape, there is no particular reason that one is constrained to follow the topology of a Turing tape. Instead they propose memory stored at a set of points on a manifold and shift actions which form a Lie group. In this way, memory points can have have different relationships to one another, rather than being constrained to Z. This is mathematically elegant and here they empirically test models with the shift group R^2 acting on R^2 and the rotation group acting on a sphere. Overall, the paper is well communicated and a novel idea. The primary limitation of this paper is its limited impact. While this approach is certainly mathematically elegant, even likely beneficial for some specific problems where the problem structure matches the group structure, it is not clear that this significantly contributes to building models capable of more general program learning. Instead, it is likely to make an already complex and slow model such as the NTM even slower. In general, it would seem memory topology is problem specific and should therefore be learned rather than specified. The baseline used for comparison is a very simple model, which does not even having the sharpening (the NTM approach to solving the problem of head distributions becoming ‘smeared’). There is also no comparison with the successor to the NTM, the DNC, which provides a more general approach to linking memories based on prior memory accesses. Minor issues: Footnote on page 3 is misleading regarding the DNC. While the linkage matrix explicitly excludes the identity, the controller can keep the head in the same position by gating the following of the link matrix. Figures on page 8 are difficult to follow.  The paper proposes a new memory access scheme based on Lie group actions for NTMs. Pros: * Well written * Novel addressing scheme as an extension to NTM. * Seems to work slightly better than normal NTMs. * Some interesting theory about the novel addressing scheme based on Lie groups. Cons: * In the results, the LANTM only seems to be slightly better than the normal NTM. * The result tables are a bit confusing. * No source code available. * The difference to the properties of normal NTM doesn*t become too clear. Esp it is said that LANTM are better than NTM because they are differentiable end-to-end and provide a robust relative indexing scheme but NTM are also differentiable end-to-end and also provide a robust indexing scheme. * It is said that the head is discrete in NTM but actually it is in space R^n, i.e. it is already continuous. It doesn*t become clear what is meant here. * No tests on real-world tasks, only some toy tasks. * No comparisons to some of the other NTM extensions such as D-NTM or Sparse Access Memory (SAM) (https://arxiv.org/abs/1610.09027). Although the motivations of other NTM extensions might be different, such comparisons still would have been interesting.  *** Paper Summary *** This paper formalizes the properties required for addressing (indexing) memory augmented neural networks as well as how to pair the addressing with read/write operation. It then proposes a framework in which any Lie group as the addressing space. Experiments on algorithmic tasks are reported. *** Review Summary *** This paper brings unity and formalism in the requirement for memory addressing while maintaining differentiable memories. Its proposal provide a generic scheme to build addressing mechanisms. When comparing the proposed approach with key-value networks, the unbounded number of memory cells and the lack of incentive to reuse indexes might reveal impractical. *** Detailed Review *** The paper reads well, has appropriate relevance to related work. The unified presentation of memory augmented networks is clear and brings unity to the field. The proposed approach is introduced clearly, is powerful and gives a tool that can be reused after reading the article. I do not appreciate that the growing memory is not mentioned as a drawback. It should be stressed and a discussion on the impact it has on efficiency/scalability is needed.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###8: Top 50% of accepted papers, clear accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",6,4,"Strengths: 1. The paper introduces a novel memory mechanism for NTMs based on differentiable Lie groups. This allows to place memory elements as points on a manifold, while still allowing training with backpropagation. 2. The paper is well written. 3. The authors provide a clear and simple explanation of the Lie group approach. 4. The authors provide a clear and simple explanation of the Lie group approach. 5. The authors provide a clear and simple explanation of the Lie group approach. Weaknesses: 1. The paper does not provide a clear comparison with the NTM. 2. The paper does not provide a clear comparison with the NTM. 3. The paper does not provide a clear comparison with the NTM. 4. The paper does not provide a clear comparison with the NTM. 5. The paper does not provide a clear comparison with the NTM. Rating: 6 Confidence: 3"
Lossy Image Compression with Compressive Autoencoders | OpenReview,"We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.","This work proposes a new approach for image compression using auto encoders. The results are impressive, besting the state of the art in this field. Pros: + Very clear paper. It should be possible to replicate these results should one be inclined to do so. + The results, when compared to other work in this field are very promising. I need to emphasize, and I think the authors should have emphasized this fact as well: this is very new technology and it should not be surprising it*s not better than the state of the art in image compression. It*s definitely better than other neural network approaches to compression, though. Cons: - The training procedure seems clunky. It requires multiple training stages, freezing weights, etc. - The motivation behind Figure 1 is a bit strange, as it*s not clear what it*s trying to illustrate, and may confuse readers (it talks about effects on JPEG, but the paper discusses a neural network architecture, not DCT quantization)  This paper proposes an autoencoder approach to lossy image compression by minimizing the weighted sum of reconstruction error and code length. The architecture consists of a convolutional encoder and a sub-pixel convolutional decoder. Experiments compare PSNR, SSIM, and MS-SSIM performance against JPEG, JPEG-2000, and a recent RNN-based compression approach. A mean opinion score test was also conducted. Pros: + The paper is clear and well-written. + The decoder architecture takes advantage of recent advances in convolutional approaches to image super-resolution. + The proposed approaches to quantization and rate estimation are sensible and well-justified. Cons: - The experimental baselines do not appear to be entirely complete. The task of using autoencoders to perform compression is important and has a large practical impact. Though directly optimizing the rate-distortion tradeoff is not an entirely novel enterprise, there are enough differences (e.g. the quantization approach and sub-pixel convolutional decoder) to sufficiently distinguish this from earlier work. I am not an image compression expert but the approach and results both seem compelling. The main shortcoming is that the implementation of Toderici et al. 2016b appears to be incomplete, and there is no comparison to Balle et al. 2016. Overall, I feel that the fact that this architecture achieves competitive performance with JPEG-2000 while simultaneously setting the stage for future work that varies the encoder/decoder size and data domain means the community will find this work to be of significant interest. I have no further specific comments at this time as they were answered sufficiently in the pre-review questions.  The paper proposes a neural approach to learning an image compression-decompression scheme as an auto-encoder. While the idea is certainly interesting and well-motivated, in practice, it turns out to achieve effectively identical rates to JPEG-2000. Now, as the authors argue, there is some value to the fact that this scheme was learned automatically rather than by expert design---which means it has benefits beyond the compression of natural images (e.g., it could be used to automatically learning a compression scheme for signals for which we don*t have as much domain knowledge). However, I still believe that this makes the paper unsuitable for publication in its current form because of the following reasons--- 1. Firstly, the fact that the learned encoder is competitive---and not clearly better---than JPEG 2000 means that the focus of the paper should more be about the aspects in which the encoder is similar to, and the aspects in which it differs, from JPEG 2000. Is it learning similar filters or completely different ones ? For what kinds of textures does it do better and for what kinds does it do worse (the paper could show the best and worst 10 patches at different bit-rates) ? 2. Secondly, I think it*s crucial that the paper demonstrate that the benefits come from a better coding scheme (as opposed to just a better decoder), as suggested in my initial pre-review question. How would a decoder trained on JPEG-2000 codes (and perhaps also on encoded random projections) do worse or better ? 3. Finally, I think the fact that it does as well/worse than JPEG-2000 significantly diminishes the case for using a *deep* auto-encoder. JPEG-2000 essentially uses a wavelet transform, which is a basis that past studies have shown could be recovered using a simple sparse dictionary algorithm like K-SVD. This is why I feel that the method needs to clearly outperform JPEG-2000, or show comparisons to (or atleast discuss) a well-crafted traditional/generative model-based baseline.","['Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###7: Good paper, accept', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",8,5,"Strengths: 1. The paper proposes a novel approach to lossy image compression using autoencoders. 2. The results are impressive, besting the state of the art in this field. 3. The paper is clear and well-written. Weaknesses: 1. The training procedure seems clunky. It requires multiple training stages, freezing weights, etc. 2. The motivation behind Figure 1 is a bit strange, as it*s not clear what it*s trying to illustrate, and may confuse readers (it talks about effects on JPEG, but the paper discusses a neural network architecture, not DCT quantization)  Rating: 8
Confidence: 4"
Riemannian Optimization for Skip-Gram Negative Sampling | OpenReview,"Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in *word2vec* software, is usually optimized by stochastic gradient descent. It can be shown that optimizing for SGNS objective can be viewed as an optimization problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.","The paper considers Grassmannian SGD to optimize the skip gram negative sampling (SGNS) objective for learning better word embeddings. It is not clear why the proposed optimization approach has any advantage over the existing vanilla SGD-based approach - neither approach comes with theoretical guarantees - the empirical comparisons show marginal improvements. Furthermore, the key idea here - that of projector splitting algorithm - has been applied on numerous occasions to machine learning problems - see references by Vandereycken on matrix completion and by Sepulchre on matrix factorization. The computational cost of the two approaches is not carefully discussed. For instance, how expensive is the SVD in (7)? One can always perform an efficient low-rank update to the SVD - therefore, a rank one update requires O(nd) operations. What is the computational cost of each iteration of the proposed approach?  Dear authors, The authors* response clarified some of my confusion. But I still have the following question: -- The response said a first contribution is a different formulation: you divide the word embedding learning into two steps, step 1 looks for a low-rank X (by Riemannian optimization), step 2 factorizes X into two matrices (W, C). You are claiming that your model outperforms previous approaches that directly optimizes over (W, C). But since the end result (the factors) is the same, can the authors provide some intuition and justification why the proposed method works better? As far as I can see, though parameterized differently, the first step of your method and previous methods (SGD) are both optimizing over low-rank matrices. Admittedly, Riemannian optimization avoids the rotational degree of freedom (the invertible matrix S you are mentioning in sec 2.3), but I am not 100% certain at this point this is the source of your gain; learning curves of objectives would help to see if Riemannian optimization is indeed more effective. -- Another detail I could not easily find is the following. You said a disadvantage of other approaches is that their factors W and C do not directly reflect similarity. Did you try to multiply the factors W and C from other optimizers and then factorize the product using the method in section 2.3, and use the new W for your downstream tasks? I am not sure if this would cause much difference in the performance. Overall, I think it is always interesting to apply advanced optimization techniques to machine learning problems. The current paper would be stronger from the machine learning perspective, if more thorough comparison and discussion (as mentioned above) are provided. On the other hand, my expertise is not in NLP and I leave it to other reviewers to decide the significance in experimental results.  This paper presents a principled optimization method for SGNS (word2vec). While the proposed method is elegant from a theoretical perspective, I am not sure what the tangible benefits of this approach are. For example, does using Riemannian optimization allow the model to converge faster than the alternatives? The evaluation doesn*t show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see *Improving Distributional Similarity with Lessons Learned from Word Embeddings*, (Levy et al., 2015)). The theoretical connection to Riemannian optimization is nice though, and it might be useful for understanding related methods in the future.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",4,4,Strengths: 1. The paper presents a principled optimization method for SGNS (word2vec). 2. The theoretical connection to Riemannian optimization is nice. Weaknesses: 1. The paper does not provide a clear theoretical justification for why Riemannian optimization is better than SGD. 2. The experimental results are not very convincing. 3. The paper does not discuss the computational cost of the proposed approach. Rating: 6 Confidence: 3
DeepCoder: Learning to Write Programs | OpenReview,"We develop a first line of attack for solving programming competition-style problems from input-output examples using deep learning. The approach is to train a neural network to predict properties of the program that generated the outputs from the inputs. We use the neural network*s predictions to augment search techniques from the programming languages community, including enumerative search and an SMT-based solver. Empirically, we show that our approach leads to an order of magnitude speedup over the strong non-augmented baselines and a Recurrent Neural Network approach, and that we are able to solve problems of difficulty comparable to the simplest problems on programming competition websites.","This is a good paper, well written, that presents a simple but effective approach to predict code properties from input output pairs. The experiments show superiority to the baseline, with speedup factors between one to two orders of magnitude. This is a solid gain! The domain of programs is limited, so there is more work to do in trying such ideas on more difficult tasks. Using neural nets to augment the search is a good starting point and a right approach, instead of generating full complex code. I see this paper as being above the threshold for acceptance.  The paper presents a technique to combine deep learning style input-output training with search techniques to match the input of a program to the provided output. Orders of magnitude speedup over non-augmented baselines are presented. Summary: ——— The proposed search for source code implementations based on a rather small domain specific language (DSL) is compelling but also expected to some degree Quality: The paper is well written. Clarity: Some of the derivations and intuitions could be explained in more detail but the main story is well described. Originality: The suggested idea to speed up search based techniques using neural nets is perfectly plausible. Significance: The experimental setup is restricted to smaller scales but the illustrated improvements are clearly apparent. Details: ———— 1. The employed test set of 100 programs seems rather small. in addition the authors ensure that the test set programs are semantically disjoint from the training set programs. Could the authors provide additional details about the small size of the test set and how to the disjoint property is enforced? 2. The length of the programs is rather small at this point in time. A more detailed ablation regarding the runtime seems useful. The search based procedure is probably still the computationally most expensive part. Hence the neural net provides some additional prior information rather than tackling the real task.  This paper presents an approach to learn to generate programs. Instead of directly trying to generate the program, the authors propose to train a neural net to estimate a fix set of attributes, which then condition a search procedure. This is an interesting approach, which make sense, as building a generative model of programs is a very complex task. Faster computation times are shown in the experimental section with respect to baselines including DFS, Enumeration, etc. in a setup with very small programs of length up to 5 instructions have to be found. It is not clear to me how the proposed approach scales to larger programs, where perhaps many attributes will be on. Is there still an advantage? The authors use as metric the time to find a single program, whose execution will result in the set of 5 input-output pairs given as input. However, as mentioned in the paper, one is not after a generic program but after the best program, or a rank list of all programs (or top-k programs) that result in a correct execution. Could the authors show experiments in this setting? would still be useful to have the proposed approach? what would the challenges be in this more realistic scenario? In the second experiment the authors show results where the length of the program at training time is different than the length at test time. However, the results are shown when only 20% of the programs are finished. Could you show results for finding all programs? The paper is missing an analysis of the results. What type of programs are difficult? how often is the NNet wrong? how does this affect speed? what are the failure modes of the proposed method? The authors proposed to have a fix-length representation of the each input-output pair, and then use average pooling to get the final representation. However, why would average pooling make sense here? would it make more sense to combine the predictions at the decoder, not the encoder? Learning from only 5 executions seems very difficult to me. For programs so small it might be ok, but going to more difficult and longer programs this setting does not seem reasonable. In summary an interesting paper. This paper tackles a problem that is outside my area of expertise so I might have miss something important.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']",6,4,"Strengths: 
1. The paper presents a novel approach to solve programming problems using deep learning. 
2. The authors provide a clear and well-written paper. 
3. The experimental results are well presented and show a significant speedup over the baseline methods. 
4. The authors discuss the limitations of their approach and provide some ideas for future work. 

Weaknesses: 
1. The paper is limited to a small domain (a simple DSL) and the results are not generalizable to more complex programs. 
2. The authors do not discuss the limitations of their approach in terms of the number of executions. 
3. The paper does not discuss the failure modes of the proposed method. 
4. The authors do not provide a clear analysis of the results. 

Rating: 8
Confidence: 4"
EPOpt: Learning Robust Neural Network Policies Using Model Ensembles | OpenReview,"Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including to unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from the target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning.","This paper explores ensemble optimisation in the context of policy-gradient training. Ensemble training has been a low-hanging fruit for many years in the this space and this paper finally touches on this interesting subject. The paper is well written and accessible. In particular the questions posed in section 4 are well posed and interesting. That said the paper does have some very weak points, most obviously that all of its results are for a very particular choice of domain+parameters. I eagerly look forward to the journal version where these experiments are repeated for all sorts of source domain/target domain/parameter combinations.   Paper addresses systematic discrepancies between simulated and real-world policy control domains. Proposed method contains two ideas: 1) training on an ensemble of models in an adversarial fashion to learn policies that are robust to errors and 2) adaptation of the source domain ensemble using data from a (real-world) target domain. > Significance Paper addresses and important and significant problem. The approach taken in addressing it is also interesting > Clarity Paper is well written, but does require domain knowledge to understand. My main concerns were well addressed by the rebuttal and corresponding revisions to the paper.","['Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###7: Good paper, accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",8,4,"Strengths: 1. The paper addresses a significant problem in reinforcement learning, namely the discrepancy between the simulated and real-world policy control domains. 2. The approach taken in addressing this problem is interesting and novel. 3. The paper is well written and accessible. Weaknesses: 1. The paper only reports results for a very particular choice of domain+parameters. This makes it difficult to assess the generalizability of the results. 2. The paper does not provide a clear theoretical justification for the proposed method. 3. The paper does not provide a clear comparison with other methods. Rating: 6 Confidence: 3"
Nonparametric Neural Networks | OpenReview,"Automatically determining the optimal size of a neural network for a given task without prior information currently requires an expensive global search and training many networks from scratch. In this paper, we address the problem of automatically finding a good network size during a single training cycle. We introduce {it nonparametric neural networks}, a non-probabilistic framework for conducting optimization over all possible network sizes and prove its soundness when network growth is limited via an penalty. We train networks under this framework by continuously adding new units while eliminating redundant units via an penalty. We employ a novel optimization algorithm, which we term ``Adaptive Radial-Angular Gradient Descent** or {it AdaRad}, and obtain promising results.","This paper proposes a nonparametric neural network model, which automatically learns the size of the model during the training process. The key idea is to randomly add zero units and use sparse regularizer to automatically null out the weights that are irrelevant. The idea sounds to be a random search approach over discrete space with the help of sparse regularization to eliminate useless units. This is an important problem and the paper gives interesting results. My main comments are listed below: What is the additional computation complexity of the algorithm? The decomposition of each fan-in weights into a parallel component and an orthogonal component and the transformation into radial-angular coordinates may require a lot of extra computation time. The authors may need to discuss the extra amount of operations relative to the parametric neural network. Furthermore, it would be useful to show some running time experiments. It is observed that nonparametric networks return small networks on the convex dataset so that it is inferior to parametric networks. Any insight on this?  I agree with reviewer 2 on the interesting part of the paper. The idea of removing or adding units is definitely an interesting direction, that will make a model grow or shrink along the lines required by the problem and the data, not the user prior knowledge. The authors offer an interesting theoretical result that proves that under fan out or fan in regularization the optimum of the error function is achieved for finite number of parameters - so the net does not grow indefinitely, until it over-fits perfectly the data. That reminds me of more traditional approaches such as Lasso or Elastic Net, in which the regularization produces sparse weights. I would have like more intuition to be given for this theorem. It is a nice result, somewhat expected (at last for me it is intuitive) and I would have liked such intuition to be given some space in the paper. For example, less discussion of prior work (that is nice too, but not as important as discussing and studying the main result of the paper) could make more room for addressing the theoretical results. Please also see below (point 2) for some suggestions. I have a few other comments to make: 1. An interesting experiment would be to show that a model such as yours, where the nodes (neurons) are added or removed automatically can outperform a net with the same number of nodes (at the end, after complete learning), in which the size and number of nodes per layer are fixed from the start. This would prove the efficiency of the idea. This is where your method is interesting: do you save nodes that are not needed and replace them with nodes that are needed? Do you optimize performance vs. memory? I understand that experiments along this line are given in Figure 2, with mixed results. The Figure i must say, is not very clear, but it is possible to interpret under careful inspection. In some the non-parametric nets are doing better and others are doing worse than the parametric ones. Even in such case i could see the usefulness of the method as it helps discovering the structure. What i don*t fully understand is why they can do better sometimes than the end net which could be trained from scratch: why is the nonparametric version of learning better than the parametric version, when the final net is known in advance? Could you give more insight? 2. Can you better discuss the meaning and implications of Theorem 1. I feel this theorem is just put there with no proper discussion. Beyond the proofs, from the Appendix, what is the key insight of the Theorem? What does it say, in plain English? To me, the conclusion seems almost natural and obvious. Is there some powerful insight? As i have mentioned previously, i feel this theoretical result deserves more space, with even more experiments to back it up. For example, can regularizer parameter lambda be predicted given the data - is there a property in the data that can help guessing the right lambda? My feeling is that lambda is the key factor for determining the final net structure. Is this true? How much does the structure of the final net depend on the initialization? Do you get different nets if you start from different random weights? How different are they? What happens when fan in and fan out regularizers are combined? Do you still have the same theoretical result? I have a few additional questions: 1. Why do you say that adding zero units changes the regularizer value? For example, does L2 norm change if you add zero values? 2. Zero units are defined as having either the fan in or the fan out weights being zero. I think that what you meant is that both fan in and fan out weights are zero, otherwise you cannot remove the unit and keep the same output f. This should be clarified better I think. I changed my rating to 7, while hoping that the authors will address my comments above.  This paper addresses the problem of allowing networks to change the number of units that are used during training. This is done in a simple but elegant and well-motivated way. Units with zero input or output weights are added or removed during training, while a group sparsity norm for regularization is used to encourage unit weights to go to zero. The main theoretical contribution is to show that with proper regularization, the loss is minimized by a network with a finite number of units. In practice, this result does not guarantee that the resulting network will not over- or under-fit the training data, but some initial experiments show that this does not seem to be the case. One potential advantage of approaches that learn the number of units to use in a network is to ease the burden of tuning hyperparameters. One disadvantage of this approach (and maybe any such approach) is that it does not really solve this problem. The network still has several hyperparameters that implicitly control the number of units that will emerge, including parameters that control how often new units are added and how rapidly weights may decay to zero. It is not clear whether these hyperparameters will be easier or harder to tune than the ones in standard approaches. In fairness, the authors do not claim that they have made training easier, but it is a little disappointing that this does not seem to be the case. The authors do emphasize that they are able to train networks that use fewer units to achieve comparable performance to networks trained parametrically. This is potentially important, because smaller networks can reduce run-time at testing, and power consumption and memory footprint, which is important on mobile devices in particular. However, the authors do not compare experimentally to existing approaches that attempt to reduce the size of parametrically trained networks (eg., by pruning trained networks) so it is not clear whether this approach is really competitive with the best current approaches to reducing the size of trained networks. Another potential disadvantage of the proposed approach is that the same hyperparameters control both the number of units that will appear in the network and the training time. Therefore, training might potentially be much slower for this approach than for a parametric approach with fixed hyperparameters. In practice, many parametric approaches require methods like grid search to choose hyperparameters, which can be very slow, but in many other cases experience with similar problems can make the choice of hyperparameters relatively easy. This means that the cost of grid search is not always paid, but the slowness of the authors’ approach may be endemic. The authors do not discuss how this issue will scale as much larger networks are trained. It is a concern that this approach may not be practical for large-scale networks, because training will be very slow. In general, the experiments are helpful and encouraging, but not comprehensive or totally convincing. I would want to see experiments on much larger problems before I was convinced that this approach can really be practical or widely useful. Overall, I found this to be an interesting and clearly written paper that makes a potentially useful point. The overall vision of building networks that can grow and adapt through life-long learning is inspiring, and this type of work might be needed to realize such a vision. But the current results remain pretty speculative.","['Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",7,3,Strengths: 1. The paper proposes a novel approach to neural network architecture learning. The idea of adding or removing units dynamically during training is interesting and could be useful in certain situations. 2. The paper is well-written and easy to follow. The authors provide a clear and concise description of the algorithm and its theoretical underpinnings. 3. The paper is well-organized and easy to read. The authors provide a clear and concise description of the algorithm and its theoretical underpinnings. 4. The paper is well-organized and easy to read. The authors provide a clear and concise description of the algorithm and its theoretical underpinnings. 5. The paper is well-organized and easy to read. The authors provide a clear and concise description of the algorithm and its theoretical underpinnings. Weaknesses: 1. The paper does not provide a clear motivation for the proposed approach. What problem does it address? Why is this approach necessary? 2. The paper does not provide a clear comparison to existing approaches. How does this approach compare to other approaches to neural network architecture learning? 3. The paper does not provide a clear discussion of the limitations of the proposed approach. What are the limitations of this approach? How does it compare to other approaches? 4. The paper does not provide a clear discussion of the potential applications of the proposed approach. What are the potential applications of this approach? How does it compare to other
On orthogonality and learning recurrent networks with long term dependencies | OpenReview,"It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and can therefore be a desirable property; however, we find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance. This paper explores the issues of optimization convergence, speed and gradient stability using a variety of different methods for encouraging or enforcing orthogonality. In particular we propose a weight matrix factorization and parameterization strategy through which we we can bound matrix norms and therein control the degree of expansivity induced during backpropagation.","This paper investigates the impact of orthogonal weight matrices on learning dynamics in RNNs. The paper proposes a variety of interesting optimization formulations that enforce orthogonality in the recurrent weight matrix to varying degrees. The experimental results demonstrate several conclusions: enforcing exact orthogonality does not help learning, while enforcing soft orthogonality or initializing to orthogonal weights can substantially improve learning. While some of the optimization methods proposed currently require matrix inversion and are therefore slow in wall clock time, orthogonal initialization and some of the soft orthogonality constraints are relatively inexpensive and may find their way into practical use. The experiments are generally done to a high standard and yield a variety of useful insights, and the writing is clear. The experimental results are based on using a fixed learning rate for the different regularization strengths. Learning speed might be highly dependent on this, and different strengths may admit different maximal stable learning rates. It would be instructive to optimize the learning rate for each margin separately (maybe on one of the shorter sequence lengths) to see how soft orthogonality impacts the stability of the learning process. Fig. 5, for instance, shows that a sigmoid improves stability—but perhaps slightly reducing the learning rate for the non-sigmoid Gaussian prior RNN would make the learning well-behaved again for weightings less than 1. Fig. 4 shows singular values converging around 1.05 rather than 1. Does initializing to orthogonal matrices multiplied by 1.05 confer any noticeable advantage over standard orthogonal matrices? Especially on the T=10K copy task? “Curiously, larger margins and even models without sigmoidal constraints on the spectrum (no margin) performed well as long as they were initialized to be orthogonal suggesting that evolution away from orthogonality is not a serious problem on this task.” This is consistent with the analysis given in Saxe et al. 2013, where for deep linear nets, if a singular value is initialized to 1 but dies away during training, this is because it must be zero to implement the desired input-output map. More broadly, an open question has been whether orthogonality is useful as an initialization, as proposed by Saxe et al., where its role is mainly as a preconditioner which makes optimization proceed quickly but doesn’t fundamentally change the optimization problem; or whether it is useful as a regularizer, as proposed by Arjovsky et al. 2015 and Henaff et al. 2015, that is, as an additional constraint in the optimization problem (minimize loss subject to weights being orthogonal). These experiments seem to show that mere initialization to orthogonal weights is enough to reap an optimization speed advantage, and that too much regularization begins to hurt performance—i.e., substantially changing the optimization problem is undesirable. This point is also apparent in Fig. 2: In terms of the training loss on MNIST (Fig. 2), no margin does almost indistinguishably from a margin of 1 or .1. However in terms of accuracy, a margin of .1 is best. This shows that large or nonexistent margins (i.e., orthogonal initializations) enable fast optimization of the training loss, but among models that attain similar training loss, the more nearly orthogonal weights perform better. This starts to separate out the optimization speed advantage conferred by orthogonality from the regularization advantage it confers. It may be useful to more explicitly discuss the initialization vs regularization dimension in the text. Overall, this paper contributes a variety of techniques and intuitions which are likely to be useful in training RNNs.  The paper is well-motivated, and is part of a line of recent work investigating the use of orthogonal weight matrices within recurrent neural networks. While using orthogonal weights addresses the issue of vanishing/exploding gradients, it is unclear whether anything is lost, either in representational power or in trainability, by enforcing orthogonality. As such, an empirical investigation that examines how these properties are affected by deviation from orthogonality is a useful contribution. The paper is clearly written, and the primary formulation for investigating soft orthogonality constraints (representing the weight matrices in their SVD factorized form, which gives explicit control over the singular values) is clean and natural, albeit not necessarily ideal from a practical computational standpoint (as it requires maintaining multiple orthogonal weight matrices each requiring an expensive update step). I am unaware of this approach being investigated previously. The experimental side, however, is somewhat lacking. The paper evaluates two tasks: a copy task, using an RNN architecture without transition non-linearities, and sequential/permuted sequential MNIST. These are reasonable choices for an initial evaluation, but are both toy problems and don*t shed much light on the practical aspects of the proposed approaches. An evaluation in a more realistic setting would be valuable (e.g., a language modeling task). Furthermore, while investigating pure RNN*s makes sense for evaluating effects of orthogonality, it feels somewhat academic: LSTMs also provide a mechanism to capture longer-term dependencies, and in the tasks where the proposed approach was compared directly to an LSTM, it was significantly outperformed. It would be very interesting to see the effects of the proposed soft orthogonality constraint in additional architectures (e.g., deep feed-forward architectures, or whether there*s any benefit when embedded within an LSTM, although this seems doubtful). Overall, the paper addresses a clear-cut question with a well-motivated approach, and has interesting findings on some toy datasets. As such I think it could provide a valuable contribution. However, the significance of the work is restricted by the limited experimental settings (both datasets and network architectures).  Vanishing and exploding gradients makes the optimization of RNNs very challenging. The issue becomes worse on tasks with long term dependencies that requires longer RNNs. One of the suggested approaches to improve the optimization is to optimize in a way that the transfer matrix is almost orthogonal. This paper investigate the role of orthogonality on the optimization and learning which is very important. The writing is sound and clear and arguments are easy to follow. The suggested optimization method is very interesting. The main shortcoming of this paper is the experiments which I find very important and I hope authors can update the experiment section significantly. Below I mention some comments on the experiment section: 1- I think the experiments are not enough. At the very least, report the result on the adding problem and language modeling task on Penn Treebank. 2- I understand that the copying task becomes difficult with non-lineary. However, removing non-linearity makes the optimization very different and therefore, it is very hard to conclude anything from the results on the copying task. 3- I was not able to find the number of hidden units used for RNNs in different tasks. 4- Please report the running time of your method in the paper for different numbers of hidden units, compare it with the SGD and mention the NN package you have used. 5- The results on Table 1 and Table 2 might also suggest that the orthogonality is not really helpful since even without a margin, the numbers are very close compare to the case when you find the optimal margin. Am I right? 6- What do we learn from Figure 2? It is left without any discussion.","['Rating:###7: Good paper, accept', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",7,4,"Strengths: 1. The paper investigates the role of orthogonality in RNNs. 2. The paper proposes a variety of interesting optimization formulations that enforce orthogonality in the recurrent weight matrix to varying degrees. 3. The experimental results demonstrate several conclusions: enforcing exact orthogonality does not help learning, while enforcing soft orthogonality or initializing to orthogonal weights can substantially improve learning. 4. The writing is clear and the arguments are easy to follow. Weaknesses: 1. The paper is limited to a very small set of experiments. 2. The experiments are not enough. At the very least, report the result on the adding problem and language modeling task on Penn Treebank. 3. I was not able to find the number of hidden units used for RNNs in different tasks. 4. Please report the running time of your method in the paper for different numbers of hidden units, compare it with the SGD and mention the NN package you have used. 5. The results on Table 1 and Table 2 might also suggest that the orthogonality is not really helpful since even without a margin, the numbers are very close compare to the case when you find the optimal margin. Am I right? 6. What do we learn from Figure 2? It is left without any discussion. Rating: 8
Confidence: 4"
An Analysis of Feature Regularization for Low-shot Learning | OpenReview,"Low-shot visual learning, the ability to recognize novel object categories from very few, or even one example, is a hallmark of human visual intelligence. Though successful on many tasks, deep learning approaches tends to be notoriously data-hungry. Recently, feature penalty regularization has been proved effective on capturing new concepts. In this work, we provide both empirical evidence and theoretical analysis on how and why these methods work. We also propose a better design of cost function with improved performance. Close scrutiny reveals the centering effect of feature representation, as well as the intrinsic connection with batch normalization. Extensive experiments on synthetic datasets, the one-shot learning benchmark “Omniglot”, and large-scale ImageNet validate our analysis.","Summary === This paper extends and analyzes the gradient regularizer of Hariharan and Girshick 2016. In that paper a regularizer was proposed which penalizes gradient magnitudes and it was shown to aid low-shot learning performance. This work shows that the previous regularizer is equivalent to a direct penalty on the magnitude of feature values weighted differently per example. The analysis goes to to provide two examples where a feature penalty favors a better representation. The first example addresses the XOR problem, constructing a network where a feature penalty encourages a representation where XOR is linearly separable. The second example analyzes a 2 layer linear network, showing improved stability of a 2nd order optimizer when the feature penalty is added. One last bit of analysis shows how this regularizer can be interpreted as a Gaussian prior on both features and weights. Since the prior can be interpreted as having a soft whitening effect, the feature regularizer is like a soft version of Batch Normalization. Experiments show small improvements on a synthetic XOR test set. On the Omniglot dataset feature regularization is better than most baselines, but is worse than Moment Matching Networks. An experiment on ImageNet similar to Hariharan and Girshick 2016 also shows effective low-shot learning. Strengths === * The core proposal is a simple modification of Hariharan and Girshick 2016. * The idea of feature regularization is analyzed from multiple angles both theoretically and empirically. * The connection with Batch Normalization could have broader impact. Weaknesses === * In section 2 the gradient regularizer of Hariharan and Girshick is introduced. While introducing the concept, some concern is expressed about the motivation: *And it is not very clear why small gradients on every sample produces good generalization experimentally.* This seems to be the central issue to me. The paper details some related analysis, it does not offer a clear answer to this problem. * The purpose and generality of section 2.1 is not clear. The analysis provides a specific case (XOR with a non-standard architecture) where feature regularization intuitively helps learn a better representation. However, the intended take-away is not clear. The take-away may be that since a feature penalty helps in this case it should help in other cases. I am hesitant to buy that argument because of the specific architecture used in this section. The result seems to rely on the choice of an x^2 non-linearity, which is not often encountered in recent neural net literature. The point might also be to highlight the difference between a weight penalty and a feature penalty because the two seem to encourage different values of b in this case. However, there is no comparison to a weight penalty on b in section 2.1. * As far as I can tell, eq. 3 depends on either assuming an L2 or cross-entropy loss. A more general class of losses for which eq. 3 holds is not provided. This should be made clear before eq. 3 is presented. * The Omniglot and ImageNet experiments are performed with Batch Normalization, yet the paper points out that feature regularization may be similar in effect to Batch Norm. Since the ResNet CNN baseline includes Batch Norm and there are clear improvements over that baseline, the proposed regularizer has a clear additional positive effect. However, results should be provided without Batch Norm so a 1-1 comparison between the two methods can be performed. * The ImageNet experiment should be more like Hariharan and Girshick. In particular, the same split of classes should be used (provided in the appendix) and performance should be measured using n > 1 novel examples per class (using k nearest neighbors). Minor: * A brief comparison to Matching Networks is provided in section 3.2, but the performance of Matching Networks should also be reported in Table 1. * From the approach section: *Intuitively when close to convergence, about half of the data-cases recommend to update a parameter to go left, while the other half recommend to go right.* Could the intuition be clarified? There are many directions in high dimensional space and many ways to divide them into two groups. * Is the SGM penalty of Hariharan and Girshick implemented for this paper or using their code? Either is acceptable, but clarification would be appreciated. * Should the first equal sign in eq. 13 be proportional to, not equal to? * The work is dense in nature, but I think the presentation could be improved. In particular, more detailed derivations could be provided in an appendix and some details could be removed from the main version in order to increase focus on the results (e.g., the derviation in section 2.2.1). Overall Evaluation === This paper provides an interesting set of analyses, but their value is not clear. There is no clear reason why a gradient or feature regularizer should improve low-shot learning performance. Despite that, experiments support that conclusion, the analysis is interesting by itself, and the analysis may help lead to a clearer explanation. The work is a somewhat novel extension and analysis of Hariharan and Girshick 2016. Some points are not completely clear, as mentioned above.  This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm, showing that it is equivalent to another proposed regularization, gradient magnitude loss. They then argue that: 1) it is helpful to low-shot learning, 2) it is numerically stable, 3) it is a soft version of Batch Normalization. Finally, they demonstrate experimentally that such a regularization improves performance on low-shot tasks. First, this is a nice analysis of some simple models, and proposes interesting insights in some optimization issues. Unfortunately, the authors do not demonstrate, nor argue in a convincing manner, that such an analysis extends to deep non-linear computation structures. I feel like the authors could write a full paper about *results can be derived for ?(x) with convex differentiable non-linear activation functions such as ReLU*, both via analysis and experimentation to measure numerical stability. Second, the authors again show an interesting correspondance to batch normalization, but IMO fail to experimentally show its relevance. Finally, I understand the appeal of the proposed method from a numerical stability point of view, but am not convinced that it has any effect on low-shot learning in the high dimensional spaces that deep networks are used for. I commend the authors for contributing to the mathematical understanding of our field, but I think they have yet to demonstrate the large scale effectiveness of what they propose. At the same time, I feel like this paper does not have a clear and strong message. It makes various (interesting) claims about a number of things, but they seem more or less disparate, and only loosely related to low-shot learning. notes: - *an expectation taken with respect to the empirical distribution generated by the training set*, generally the training set is viewed as a *montecarlo* sample of the underlying, unknown data distribution mathcal{D}. - *we can see that our model learns meaningful representations*, it gets a 6.5% improvement on the baseline, but there is no analysis of the meaningfulness of the representations. - *Table 13.2* should be *Table 2*. - please be mindful of formatting, some citations should be parenthesized and there are numerous extraneous and missing spacings between words and sentences.  The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net. Although the equations suggest a weighting per example, dropping this weight (alpha_i) works equally well. The proposed approach relates to Batch Norm and weight decay. Experiments are given on *low-shot* settting. There seem to be two stories in the paper: feature penalty as a soft batch norm version, and low-shot learning; why is feature penalty specifically adapted to low-shot learning and not a more classical supervised task? Regarding your result on Omniglot, 91.5, I believe it is still about 2% worse than the Matching Networks, which you refer to but don*t put in Table 1. Why? Overall, the idea is simple but feels like preliminary: while it is supposed to be a *soft BN*, BN itself gets better performance than feature penalty, and both together give even better results. Is something still missing in the explanation? -- edits after revised version: Thank you for adding more information to the paper. I feel it is still too long but hopefully you can reduce it to 9 pages as promised. However, I*m still not convinced the paper is ready to be accepted, mainly for the following reasons: - on Omniglot, the paper is still significantly far from the current state of the art. - the new experiments do not really confirm/infirm the relationship with BN. - you added an explanation of why FP works for low-shot setting, by showing it controls the VC dimension and hence is good to control overfitting with a small number of training examples, but this discussion is basic and does not really shed more light than the obvious. I*m pushing up your score from 4 to 5 for the improved version, but I still think it is below acceptance level.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",6,3,"Strengths: 
1. The paper provides a nice analysis of the gradient regularization of Hariharan and Girshick 2016. 
2. The analysis is interesting and provides a good understanding of the underlying mechanisms. 
3. The paper is well-written and easy to follow. 
4. The experiments are well-designed and provide a good understanding of the results. 
5. The paper is well-structured and easy to read. 

Weaknesses: 
1. The paper does not provide a clear motivation for the choice of the regularization. 
2. The analysis is not very general and is limited to a specific architecture. 
3. The paper does not provide a clear comparison to other methods. 
4. The paper is too long and some sections could be removed. 
5. The paper does not provide a clear conclusion. 

Rating: 6
Confidence: 3"
Conditional Image Synthesis With Auxiliary Classifier GANs | OpenReview,"Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.","Apologies for the late review. This submission proposes method for class-conditional generative image modeling using auxiliary classifiers. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution. The discriminator has two outputs and two corresponding objectives: determine whether a sample is real or generated, and independently to predict the (real or sampled) class label corresponding to the sample. Figure 2. nicely illustrates related methods - this particular method bears similarities to InfoGANs and Semi-supervised GANs. Compared to infogans, this method also encourages correspondence between the latent c and the real class labels for the real examples (whereas infogans are presented as fully unsupervised). The authors attempt at evaluating the method quantitatively by looking at the discriminability and diversity of samples. It is found - not surprisingly - that higher resolution improves discriminability (because more information is present). Discriminability: Figure 3 doesn’t have legends so it is a bit hard to understand what is going on. Furthermore, my understanding is that when evaluating discriminability the authors downsample and then bicubically upsample the image, which is much more like a blurring, very different from retraining all the models to work on low resolution in the first place. Diversity: The authors try to quantitatively evaluate diversity of samples by measuring the average MS-SSIM between randomly selected pairs of points within each class. I think this method is significantly flawed and limited, for reasons mentioned in (Theis et al, 2015, A note on the evaluation…). In its behaviour, MS-SSIM is not that dissimilar from Euclidean distance - although it is nonlinear and is bounded between -1 and 1. Evaluating diversity/entropy of samples in high dimensions is very hard, especially if the distributions involved are non-trivial for example concentrated around manifolds. Consider for example a generative model which randomly samples just two images. Assuming that the MSSSIM between these two images is -1, this generative model can easily achieve an average MSSSIM score of 0, implying a conclusion that this model has more diversity than the training data itself. Conversely, SSIM is designed not to be sensitive to contrast and average pixel intensity, so if a model is diverse in this sense, that will be ignored by this measure. Overall, the paper proposes a new way to incorporate class labels into training GAN-type models. As far as I know the particular algorithm is novel, but I consider it incremental compared to what has been done before. I think the proposed evaluation metrics are flawed, especially when evaluating the diversity of the samples for the aforementioned reasons.  This is a clear, easy to read, highly relevant paper that improves GAN training for images and explores evaluation criterion on GANs. The main contributions are as follows: - Adding an auxiliary classifier head to a GAN discriminator and training a classification objective in addition to the real/fake objective improves performance. Generator is conditioned on 1-hot encoding of class and is trained to generate the specified class. - Training different models on different subsets of imagenet classes improves performance. - They motivate evaluating GAN images by using a perceptual similarity metric (MS-SSIM) on pairs of samples to quantify diversity in the samples (and detect mode collapse) - They show this metric correlates with a discriminability metric (classification accuracy of pre-trained imagenet model on generated samples) . The overall novelty of this approach is somewhat lacking in that previous methods have proposed training a classifier head on the discriminator and the discriminability metric proposed is simply the inception score of [1] except with class information. However, I think there is still a contribution to be made my putting these tricks together and successfully demonstrating image synthesis gains. Questions for the authors: (1) Why do you think splitting the imagenet training into 100 different models improves performance? Is the issue with the representation of the class? In other words, if an encoding more meaningful that 1-hot vector was used do you still think 100 models would be needed. Ideally we should hope that a generative model can leverage information from different classes to help with the generation of a particular class and also text-image synthesis models [2] have been quite successful when trained on diverse datasets (and these are conditioned on a semantically meaningful text encoding) which suggests to be that the issue is with the representation. (2) In section 3 the AC-GAN classification objective (omitting expectation for brevity) is given as L_S = log P(C=c|X_real) + log P(C=c|X_fake) and you say that both the discriminator and generator are trained to maximize this quantity. Obviously the generator would want to maximize log P(C=c|X_fake) for its given conditioning class c. But can you explain why you would want the discriminator to also maximize the classification accuracy of generated samples? Why not do something similar to the CatGAN paper [3] and train the discriminator to be as uncertain as possible about the generated examples. Seems counterintuitive to me to have both the generator and discriminator trying to optimize the same classification objective rather than not be adversarial wrt to this loss as well as the real/fake loss. Overall, this paper makes a clear contribution to GAN research both in terms of image quality and evaluation metrics and I would recommend it for acceptance. [1] Salimans et al. Improved Techniques for Training GANs (https://arxiv.org/abs/1606.03498) [2] Reed et al. Generative Adversarial Text to Image Synthesis (https://arxiv.org/abs/1605.05396) [3] Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks (https://arxiv.org/abs/1511.06390)  This paper introduces a class-conditional GAN as a generative model for images. It introduces two main diagnostic tools for training GANs: one to assess whether a model is making full use of its output resolution and another to measure the diversity of generated samples. Experiments are conducted on the CIFAR-10 and ImageNet datasets. Pros: + The paper is clear and well-written. + Experiments performed in the relatively under-explored 128 x 128 ImageNet setting. + The proposed MS-SSIM diversity metric appears to be a useful tool for detecting convergence issues in class-conditional GAN models. Cons: - AC-GAN model itself is of limited novelty relative to other GAN approaches that condition on class. - Diversity metric is of limited use for training non class-conditional GANs. - No experimental comparison of AC-GAN to other class-conditional models. To my knowledge training GANs on large, diverse images such as 128 x 128 ImageNet images is under-explored ([1] contains just a few samples in this setting). Though the model is not very novel and a comparison to other class-conditional models is lacking, I feel the community will find the diagnostic tools and the thorough exploration of the ImageNet-trained model to be of interest. * Section 4.2: MS-SSIM is traditionally defined for grayscale images only. How do you extend MS-SSIM to color images in your work? Were they computed channel-wise across R,G, and B? * Section 4.4: It is difficult to tell whether a single AC-GAN was trained for all of CIFAR-10 or one for each group. If single, why were the samples split into groups for computing Inception Score? And if multiple, the comparison to Salimans et al. is not a direct one. Also it would be helpful to include the real data Inception score as a point of comparison. * Appendix D: The caption of Figure 9 states that the same number of training steps was taken for each model. From this it seems possible that the models with more classes simply did not converge yet. [1] Salimans, Tim, et al. *Improved techniques for training GANs.* Advances in Neural Information Processing Systems. 2016.","['Rating:###3: Clear rejection', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",3,4,"Strengths: 
1. The paper introduces a class-conditional GAN as a generative model for images. It introduces two main diagnostic tools for training GANs: one to assess whether a model is making full use of its output resolution and another to measure the diversity of generated samples. Experiments are conducted on the CIFAR-10 and ImageNet datasets. 
2. The paper is clear and well-written. 
3. Experiments performed in the relatively under-explored 128 x 128 ImageNet setting. 
4. The proposed MS-SSIM diversity metric appears to be a useful tool for detecting convergence issues in class-conditional GAN models. 

Weaknesses: 
1. AC-GAN model itself is of limited novelty relative to other GAN approaches that condition on class. 
2. Diversity metric is of limited use for training non class-conditional GANs. 
3. No experimental comparison of AC-GAN to other class-conditional models. 
4. Training GANs on large, diverse images such as 128 x 128 ImageNet images is under-explored. 
5. The paper does not provide a clear explanation of the choice of the number of classes to split the ImageNet dataset into. 
6. The paper does not provide a clear explanation of the choice of the number of training steps for each model. 
7. The paper does not provide a clear explanation of the choice of the batch size for each model"
#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning | OpenReview,"Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.","This paper introduces a new way of extending the count based exploration approach to domains where counts are not readily available. The way in which the authors do it is through hash functions. Experiments are conducted on several domains including control and Atari. It is nice that the authors confirmed the results of Bellemare in that given the right *density* estimator, count based exploration can be effective. It is also great the observe that given the right features, we can crack games like Montezuma*s revenge to some extend. I, however, have several complaints: First, by using hashing, the authors did not seem to be able to achieve significant improvements over past approaches. Without *feature engineering*, the authors achieved only a fraction of the performance achieved in Bellemare et al. on Montezuma*s Revenge. The proposed approaches In the control domains, the authors also does not outperform VIME. So experimentally, it is very hard to justify the approach. Second, hashing, although could be effective in the domains that the authors tested on, it may not be the best way of estimating densities going forward. As the environments get more complicated, some learning methods, are required for the understanding of the environments instead of blind hashing. The authors claim that the advantage of the proposed method over Bellemare et al. is that one does not have to design density estimators. But I would argue that density estimators have become readily available (PixelCNN, VAEs, Real NVP, GANs) that they can be as easily applied as can hashing. Training the density estimators is not difficult problem as more.  The paper proposes a new exploration scheme for reinforcement learning using locality-sensitive hashing states to build a table of visit counts which are then used to encourage exploration in the style of MBIE-EB of Strehl and Littman. Several points are appealing about this approach: first, it is quite simple compared to the current alternatives (e.g. VIME, density estimation and pseudo-counts). Second, the paper presents results across several domains, including classic benchmarks, continuous control domains, and Atari 2600 games. In addition, there are results for comparison from several other algorithms (DQN variants), many of which are quite recent. The results indicate that the approach clearly improves over the baseline. The results against other exploration algorithms are not as clear (more dependent on the individual domain/game), but I think this is fine as the appeal of the technique is its simplicity. Third, the paper presents results on the sensitivity to the granularity of the abstraction. I have only one main complaint, which is it seems there was some engineering involved to get this to work, and I do not have much confidence in the robustness of the conclusions. I am left uncertain as to how the story changes given slight perturbations over hyper-parameter values or enabling/disabling of certain choices. For example, how critical was using PixelCNN (or tying the weights?) or noisifying the output in the autoencoder, or what happens if you remove the custom additions to BASS? The granularity results show that the choice of resolution is sensitive, and even across games the story is not consistent. The authors decide to use state-based counts instead of state-action based counts, deviating from the theory, which is odd because the reason to used LSH in the first place is to get closer to what MBIE-EB would advise via tabular counts. There are several explanations as to why state-based versus state-action based counts perform similarly in Atari; the authors do not offer any. Why? It seems like the technique could be easily used in DQN as well, and many of the variants the authors compare to are DQN-based, so omitting DQN here again seems strange. The authors justify their choice of TRPO by saying it ensures safe policy improvement, though it is not clear that this is still true when adding these exploration bonuses. The case study on Montezuma*s revenge, while interesting, involves using domain knowledge and so does not really fit well with the rest of the paper. So, in the end, simple and elegant idea to help with exploration tested in many domains, though I am not certain which of the many pieces are critical for the story to hold versus just slightly helpful, which could hurt the long-term impact of the paper. --- After response: Thank you for the thorough response, and again my apologies for the late reply. I appreciate the follow-up version on the robustness of SimHash and state counting vs. state-action counting. The paper addresses an important problem (exploration), suggesting a *simple* (compared to density estimation) counting method via hashing. It is a nice alternative approach to the one offered by Bellemare et al. If discussion among reviewers were possible, I would now try to assemble an argument to accept the paper. Specifically, I am not as concerned about beating the state of the art in Montezuma*s as Reviewer3 as the merit of the current paper is one the simplicity of the hashing and on the wide comparison of domains vs. the baseline TRPO. This paper shows that we should not give up on simple hashing. There still seems to be a bunch of fiddly bits to get this to work, and I am still not confident that these results are easily reproducible. Nonetheless, it is an interesting new contrasting approach to exploration which deserves attention. Not important for the decision: The argument in the rebuttal concerning DQN & A3C is a bit of a straw man. I did not mention anything at all about A3C, I strictly referred to DQN, which is less sensitive to parameter-tuning than A3C. Also, Bellemare 2016 main result on Montezuma used DQN. Hence the omission of these techniques applied to DQN still seems a bit strange (for the Atari experiments). The figure S9 from Mnih et al. points to instances of asynchronous one-step Sarsa with varied thread counts.. of course this will be sensitive to parameters: it is both asynchronous online algorithms *and* the parameter varied is the thread count! This is hardly indicative of DQN*s sensitivity to parameters, since DQN is (a) single-threaded (b) uses experience replay, leading to slower policy changes. Another source of stability, DQN uses a target network that changes infrequently. Perhaps the authors made a mistake in the reference graph in the figure? (I see no Figure 9 in https://arxiv.org/pdf/1602.01783v2.pdf , I assume the authors meant Figure S9)  This paper proposed to use a simple count-based exploration technique in high-dimensional RL application (e.g., Atari Games). The counting is based on state hash, which implicitly groups (quantizes) similar state together. The hash is computed either via hand-designed features or learned features (unsupervisedly with auto-encoder). The new state to be explored receives a bonus similar to UCB (to encourage further exploration). Overall the paper is solid with quite extensive experiments. I wonder how it generalizes to more Atari games. Montezuma’s Revenge may be particularly suitable for approaches that implicitly/explicitly cluster states together (like the proposed one), as it has multiple distinct scenarios, each with small variations in terms of visual appearance, showing clustering structures. On the other hand, such approaches might not work as well if the state space is fully continuous (e.g. in RLLab experiments). The authors did not answer my question about why the hash code needs to be updated during training. I think it is mainly because the code still needs to be adaptive for a particular game (to achieve lower reconstruction error) in the first few iterations . After that stabilization is the most important. Sec. 2.3 (Learned embedding) is quite confusing (but very important). I hope that the authors could make it more clear (e.g., by writing an algorithm block) in the next version.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",4,3,"Strengths: 1. The paper presents a simple and elegant approach to exploration in high-dimensional state spaces. 2. The authors provide a wide range of experiments, including Atari games, Montezuma*s Revenge, and continuous control domains. 3. The paper is well-written and easy to follow. Weaknesses: 1. The paper does not provide a clear theoretical justification for the choice of hash function. 2. The authors do not discuss the sensitivity of the results to the choice of hash function. 3. The paper does not provide a clear theoretical justification for the choice of state vs. state-action based counts. 4. The paper does not discuss the sensitivity of the results to the choice of hash function. 5. The paper does not discuss the sensitivity of the results to the choice of hash function. 6. The paper does not provide a clear theoretical justification for the choice of TRPO. 7. The paper does not discuss the sensitivity of the results to the choice of TRPO. 8. The paper does not provide a clear theoretical justification for the choice of TRPO. 9. The paper does not discuss the sensitivity of the results to the choice of TRPO. 10. The paper does not provide a clear theoretical justification for the choice of TRPO. Rating: 7 Confidence: 3"
A Neural Stochastic Volatility Model | OpenReview,"In this paper, we show that the recent integration of statistical models with recurrent neural networks provides a new way of formulating volatility models that have been popular in time series analysis and prediction. The model comprises a pair of complementary stochastic recurrent neural networks: the generative network models the joint distribution of the stochastic volatility process; the inference network approximates the conditional distribution of the latent variables given the observable ones. Our focus in this paper is on the formulation of temporal dynamics of volatility over time under a stochastic recurrent neural network framework. Our derivations show that some popular volatility models are a special case of our proposed neural stochastic volatility model. Experiments demonstrate that the proposed model generates a smoother volatility estimation, and largely outperforms a widely used GARCH model on several metrics about the fitness of the volatility modelling and the accuracy of the prediction.","The authors propose a recurrent variational neural network approach to modelling volatility in financial time series. This model consists of an application of Chung et al.’s (2015) VRNN model to volatility forecasting, wherein a Variational Autoencoder (VAE) structure is repeated at each time step of the series. The paper is well written and easy to follow (although this reviewer suggests applying a spelling checking, since the paper contains a number of harmless typos). The paper’s main technical contribution is to stack two levels of recurrence, one for the latent process and one for the observables. This appears to be a novel, if minor contribution. The larger contribution is methodological, in areas of time series modelling that are both of great practical importance and have hitherto been dominated by rigid functional forms. The demonstration of the applicability and usefulness of general-purpose non-linear models for volatility forecasting would be extremely impactful. I have a few comments and reservations with the paper: 1) Although not mentioned explicitly, the authors’ framework are couched in terms of carrying out one-timestep-ahead forecasts of volatility. However, many applications of volatility models, for instance for derivative pricing, require longer-horizon forecasts. It would be interesting to discuss how this model could be extended to forecast at longer horizons. 2) In Section 4.4, there’s a mention that a GARCH(1,1) is conditionally deterministic. This is true only when forecasting 1 time-step in the future. At longer horizons, the GARCH(1,1) volatility forecast is not deterministic. 3) I was initially unhappy with the limitations of the experimental validation, limited to comparison with a baseline GARCH model. However, the authors provided more comparisons in the revision, which adds to the quality of the results, although the models compared against cannot be considered state of the art. It would be well advised to look into R packages such as `stochvol’ and ‘fGarch’ to get implementations of a variety of models that can serve as useful baselines, and provide convincing evidence that the modelled volatility is indeed substantially better than approaches currently entertained by the finance literature. 4) In Section 5.2, more details should be given on the network, e.g. number of hidden units, as well as the embedding dimension D_E (section 5.4) 5) In Section 5.3, more details should be given on the data generating process for the synthetic data experiments. 6) Some results in the appendix are very puzzling: around jumps in the price series, which are places where the volatility should spike, the model reacts instead by huge drops in the volatility (Figure 4(b) and (c), respectively around time steps 1300 and 1600). This should be explained and discussed. All in all, I think that the paper provides a nice contribution to the art of volatility modelling. In spite of some flaws, it provides a starting point for the broader impact of neural time series processing in the financial community.  The authors propose a recurrent neural network approach for constructing a stochastic volatility model for financial time series. They introduce an inference network based on a recurrent neural network that computes the approximation to the posterior distribution for the latent variables given the past data. This variational approximation is used to maximize the marginal likelihood in order to learn the parameters of the model. The proposed method is validated in experiments with synthetic and real-world time series, showing to outperform parametric GARCH models and a Gaussian process volatility model. Quality: The method proposed seems technically correct, with the exception that in equation (19) the inference model is doing filtering and not smoothing, in the sense that the posterior for z_t* only depends on those other z_t and x_t values with t<t*, but in the true posterior p(Z|X) each z_t depends on all the X. This means the proposed learning method is inefficient. The reference below shows how to perform smoothing too and the results in that paper show indeed that smoothing produces better results for learning the model. Sequential Neural Models with Stochastic Layers Fraccaro, Marco and So nderby, So ren Kaae and Paquet, Ulrich and Winther, Ole In NIPS 2016. It is not clear if the method proposed in the above reference would perform better just because of using smoothing when learning the model parameters. Clarity: The paper is clearly written and easy to read. For the results on real-world data, in Table 1, how is the NLL computed? Is the average NLL across the 162 time series? Originality: The method proposed is not very original. Previous work has already used the variational approach with the reparametrization trick to learn recurrent neural networks with stochastic units (see the reference above). It seems that the main contribution of the authors is to apply this type of techniques to the problem of modeling financial time series. Significance: The results shown are significant, the method proposed by the authors outperforms previous approaches. However, there is a huge amount of techniques available for modeling financial time-series. The number of GARCH variants is probably close to hundreds, each one claiming to be better than the others. This makes difficult to quantify how important the results are.  Thank you for an interesting read. I found the application of VRNN type generative model to financial data very promising. But since I don*t have enough background knowledge to judge whether the performance gap is significant or not, I wouldn*t recommend acceptance at this stage. To me, the biggest issue for this paper is that I*m not sure if the paper contains significant novelty. The RNN-VAE combination has been around for more than a year and this paper does not propose significant changes to it. Maybe this paper fits better to an application targeting conference, rather than ICLR. But I*m not exactly sure about ICLR*s acceptance criteria, and maybe the committee actually prefer great performances and interesting applications?","['Rating:###6: Marginally above acceptance threshold', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",6,4,"Strengths: 1. The paper is well written and easy to follow. 2. The authors provide a clear and concise description of the model. 3. The results are well presented and easy to understand. Weaknesses: 1. The paper is not very original. Previous work has already used the variational approach with the reparametrization trick to learn recurrent neural networks with stochastic units. 2. The paper does not provide a clear comparison with other models. 3. The results are not very impressive. The authors claim that the model outperforms previous approaches, but the results are not very convincing. Rating: 6 Confidence: 3"
Learning Curve Prediction with Bayesian Neural Networks | OpenReview,"Different neural network architectures, hyperparameters and training protocols lead to different performances as a function of time. Human experts routinely inspect the resulting learning curves to quickly terminate runs with poor hyperparameter settings and thereby considerably speed up manual hyperparameter optimization. Exploiting the same information in automatic Bayesian hyperparameter optimization requires a probabilistic model of learning curves across hyperparameter settings. Here, we study the use of Bayesian neural networks for this purpose and improve their performance by a specialized learning curve layer.","The paper addresses the problem of predicting learning curves. The key difference from prior work is that (1) the authors learn a neural network that generalizes across hyperparameter settings and (2) the authors use a Bayesian neural network with SGHMC. The authors demonstrate that the proposed approach is effective on extrapolating partially observed curves as well as predicting unobserved learning curves on various architectures (FC, CNN, LR and VAE). This seems very promising for Bayesian optimization, I*d love to see an experiment that evaluates the relative advantage of this proposed method :) Have you thought about ways to handle learning rate decays? Perhaps you could run the algorithm on a random subset of data and extrapolate from that? I was thinking of other evaluation measures in addition to MSE and LL. In practice, we care about the most promising run. Would it make sense to evaluate how accurately each method identified the best run? Minor comments: Fonts are too small and almost illegible on my hard copy. Please increase the font size for legends and axes in the figures. Fig 6: not all figures seem to have six lines. Are the lines overlapping in some cases?  This paper is about using Bayesian neural networks to model learning curves (that arise from training ML algorithms). The application is hyper-parameter optimization: if we can model the learning curve, we can terminate bad runs early and save time. The paper builds on existing work that used parametric learning curves. Here, the parameters of these learning curves form the last layer of a Bayesian neural network. This seems like a totally sensible idea. I think the main strength of this paper is that it addresses an actual need. Based on my personal experience, there is high demand for a working system to do early termination in hyperparameter optimization. What I*d like to know, which I wish I*d asked during pre-review questions, is whether the authors plan to release their code. Do you? I sincerely hope so, because I think the code would be a significant part of the paper*s contribution, since the nature of the paper is more practical than conceptual. The experiments in the paper seem thorough but the results are a bit underwhelming. I*m less interested in the part about whether the learning curves are actually modeled well, and more interested in the impact on hyperparameter optimization. I was hoping to see BIG speedups as a result of using this method, but I am left feeling unsure how big the speedup really is. Instead of *objective function vs. iterations* I would be more interested in the inverse plot: number of iterations needed to get to a fixed objective function value. Since what I*m really interested in is how much time I can save. Ideally there would also be some mention of real time as sometimes these hyperparameter optimization methods are themselves so slow that they end up being unusable. Finally, one figure that I feel is missing is a histogram of termination times over different runs. This would provide me with more intuition than all the other figures. Because it would tell me, what fraction of runs are being terminated early. And, how early? Right now I have no sense of this, except that at least *some* runs are clearly being terminated early, since this is neccessary for the proposed method to outperform other methods. Overall, I think this paper merits acceptance because it is a solid effort on an interesting problem. The progress is fairly incremental but I can live with that.  This paper proposes a new Bayesian neural network architecture for predicting the values of learning curves during the training of machine learning models. This is an exploratory paper, in that the ultimate goal is to use this method in a Bayesian optimization system, but for now the experiments are limited to assessing the quality of the predictions. This builds on previous work in Domhan, 2015, however in this work the model incorporates information from all tested hyperparameter settings rather than just extrapolating from a single learning curve. This paper also explores two MCMC methods for inference: SGLD and SGHMC, but I couldn’t tell if either of these were tested in Domhan, 2015 as well. The performance seems overall positive, particularly in the initial phase of each curve where there is very little information. In this case, as expected, sharing knowledge across curves helps. One regime which did not seem to be tested, but might be very informative, is when some curves in the training set have been mostly, or fully observed. This might be a case where sharing information really helps. Something that concerns me about this approach is the timing. The authors stated that to train the network takes about 20-60 seconds. In the worst case, with 100 epochs, this results in a little over 1.5 hours spent training the Bayesian network. This is a non-trivial fraction of the several hours it takes to train the model being tuned. The Bayesian network makes many separate predictions, as shown in Figure 2. It would be interesting to see how accurate some of these individual pieces are. For example, did you bound the asymptotic value of the learning curve, since you mostly predicted accuracy? If not, did the value tend to lie in [0,1]? Below are some minor questions/comments. Figure 1 axes should read “validation accuracy” Figure 6 can you describe LastSeenValue (although it seems self-explanatory, it’s good to be explicit) in the bottom left figure, and why isn’t it used anywhere else as a baseline? Figure 7 and Table 1 are you predicting just the final value of the curves? Or every value along each curve, conditioned on the previous values? Why do you only use 5 basis functions? Does this sufficiently capture all of the flexibility of these learning curves? Would more basis functions help or hurt?","['Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",7,4,Strengths: 1. The authors propose a novel Bayesian neural network architecture for learning curve prediction. 2. The architecture is well motivated and is based on the idea of modeling the learning curve as a function of the current state of the model. 3. The authors present a clear and well-organized paper. 4. The paper is well-written and easy to follow. Weaknesses: 1. The paper does not provide a clear motivation for why Bayesian neural networks are better suited for this task than other approaches. 2. The authors do not provide a clear comparison to other approaches. 3. The paper does not provide a clear evaluation of the performance of the proposed method. 4. The paper does not provide a clear explanation of the hyperparameters used in the experiments. 5. The paper does not provide a clear explanation of the learning curve model used in the experiments. 6. The paper does not provide a clear explanation of the prior distribution used in the experiments. 7. The paper does not provide a clear explanation of the evaluation metric used in the experiments. 8. The paper does not provide a clear explanation of the experimental setup. 9. The paper does not provide a clear explanation of the results. 10. The paper does not provide a clear explanation of the conclusions. Rating: 6 Confidence: 3
HyperNetworks | OpenReview,"This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network. We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.","A well known limitation in deep neural networks is that the same parameters are typically used for all examples, even though different examples have very different characteristics. For example, recognizing animals will likely require different features than categorizing flowers. Using different parameters for different types of examples has the potential to greatly reduce underfitting. This can be seen in recent results with generative models, where image quality is much better for less diverse datasets. However, it is difficult to use different parameters for different examples because we typically train using minibatches, which relies on using the same parameters for all examples in a minibatch (i.e. doing matrix multiplies in a fully-connected network). The hypernetworks paper cleverly proposes to get around this problem by adapting different *parameters* for different time steps in recurrent networks and different. The basic insight is that a minibatch will always include many different examples from the same time step or spatial position, so there is no computational issue involved with using different *parameters*. In this paper, the *parameters* are modified for different positions based on the output from a hypernetwork which conditions on the time step. Hypothetically, this hypernetwork could also condition on other features that are shared by all sequences in the minibatch. I expect this method to become standard for training RNNs, especially where the length of the sequences is the same during the training and testing phases. Penn Treebank is a highly competitive baseline, so the SOTA result reported here is impressive. The experiments on convolutional networks are less experimentally impressive. I suspect that the authors were aiming to achieve state of the art results here but settled with achieving a reduction in the number of parameters. It might even be worthwhile to consider a synthetic experiment where two completely different types of image are appended (i.e. birds on the left and flowers on the right) and show that the hypernetwork helps in this situation. It may be the case that for convnets, the cases where hypernetworks help are very specific. For RNNs, it seems to be the case that explicitly changing the nature of the computation depending on the position in the sequence greatly improves generalization. While a usual RNN could learn to store a counter (indicating the position in the sequence), the hypernetwork could be a more efficient way to add capacity. Applications to time series forecasting and modeling could be an interesting area for future work.  Although the trainable parameters might be reduced significantly, unfortunately the training and recognition speech cannot be reduced in this way. Unfortunately, as the results show, the authors could not get better results with less parameters. However, the proposed structure with even more number of parameters shows significant gain e.g. in LM. The paper should be reorganized, and shortened. It is sometimes difficult to follow and sometimes inconsistent. E.g.: the weights of the feedforward network depend only on an embedding vector (see also my previous comments on linear bottlenecks), whereas in recurrent network the generated weights also depend on the input observation or its hidden representation. Could the authors provide the num. of trainable parameters for Table 6? Probably presenting less results could also improve the readability. Only marginal accept due to the writing style.  This paper proposes an interesting new method for training neural networks, i.e., a hypernetwork is used to generate the model parameters of the main network. The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task. In particular, the hyperLSTM with non-shared weights can achieve excellent results compared to conventional LSTM and its variants on a couple of LM talks, which is very inspiring. --pros This work demonstrates that it is possible to generate the neural network model parameters using another network that can achieve competitive results by a few relative large scale experiments. The idea itself is very inspiring, and the experiments are very solid. --cons The paper would be much stronger if it was more focused. In particular, it is unclear what is the key advantage of this hypernetwork approach. It is argued that in the paper that can achieve competitive results using smaller number of trainable model parameters. However, in the running time, the computational complexity is the same as the standard main network for static networks, such as ConvNet, and the computational cost is even larger for dynamic networks such as LSTMs. The improvements of hyperLSTMs over conventional LSTM and its variants seem mainly come from increasing the number of model parameters. --minor question, The ConvNet and LSTM used in the experiments do not have a large softmax layer. For most of the word-level tasks for either LM or MT, the softmax layer could be more than 100K. Is it going to be challenging for the hyperNetwork generate large number of weights for that case, and is it going to slowing the training down significantly?  *** Paper Summary *** The paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation associated with the layer. Two instances are presented (i) a CNN where each layer weight is computed from a lower dimensional layer embedding vector; (ii) an RNN where each layer weight is computed from a secondary RNN state. *** Review Summary *** Pros: - I like the idea of bringing multiplicative RNNs and their predecessors back into the spotlight. - LM and MT results are excellent. Cons: - The paper could be better written. It is too long for the conference format and need refocussing. - On related work, the relation with multiplicative RNN and their generic tensor product predecessor (Order 2 networks, wrt C. Lee Giles definition) should be mentioned in the related work section and the differences with earlier research need to be explained and motivated (by the way it is better to say that something is revisiting an old idea or training it at modern scale/on modern tasks than ommitting it). - on focus, it is not clear if your goal is to achieve better performance or more compact networks. In the RNN section you lean toward the former, in the CNN section you seem to lean toward the latter. I would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. The relation with multiplicative/order 2 networks and eventual differences need to be explained. *** Detailed Review *** Multiplicative networks are an extremely powerfull architecture and bringing them back into the spotlight is excellent. This paper has excellent results but suffer poor presentation, lack of a clear focus. It spends time on details and ommit important points. In its current form, it is much too long to long and his not self contained without the appendices. Spending more time on multiplicative RNNs, order 2 networks at the begining of the paper would be excellent. This will let you highlight the difference between this paper and earlier work. It would also be necessary to spend a little time on why multiplicative RNN were less used than gated RNN: it seems that the optimization problem their training involve is tricker and it would be helpful to explain whether you had a harder time tweaking optimization parameters or whether you needed longer training sessions compared to LSTMs, regular CNN. On name, I am not sure that *hypernetwork* help the reader understand better what the proposed architecture compared to multiplicative interactions. In section 3.2, you seem to imply that there are different settings of hypernetworks that allow to vary from an RNN to a CNN, this is not clear to me, maybe you could show how this would work on a simple temporal problem with equations. The work on CNN and RNN are rather disconnected to me: for CNN, you seem to be interested in a low rank structure of the weights, showing that similar performance can be achieved with less weights. It is not clear to me why to pursue that goal. Do you expect speedups? less memory for embedded applications? In that case you should compare with alternative strategies, e.g. model compression (Caruana et al 2006, aka Dark Knowledge, Hinton et al 2014) or hashed networks (Chen et al 2015). For RNN, you seem to target better perplexity/BLEU and model compactness is not a priority. Instead of making the weights have a simpler structure, you make them richer, i.e. dependent over time. It seems in that case models might be bigger and take longer to train. You might want to comment on training time, inference time, memory requirement in that case, as you highlight it might be an important goal in the CNN section. Overall, I am not sure it helps to have this mixed message. I would rather see the paper fit in the conference format with the RNN results alone and a clearer explanation and defers the publications of the CNN results when a proper comparison with memory concerned methods is performed. Some of the discussions are not clear to me, I am not sure what message the reader should get from Figure 2 or from the discussion on saturation statistics (p10, Figure 5). Similarly, I am not sure if Figure 4 is showing anything: everything should change more drastically at word boundaries even in a regular LSTM (states, gates units should look very different before/after a space); without such a comparison it is hard to see if this is unique to your network. The results on handwriting generation are harder to compare for me. Log-loss are hard to understand, I have no sense whether the difference between models is significant (what would be the variance in this metric under boostrap sampling of the training set?). I am not sold either on qualitative metric were human can assess quality but human cannot evaluate if the network is repeating the training set. Did you thing at precision/recall metric for ink, possibly with some spatial tolerance ? (e.g. evaluation of segmentation tasks in vision). The MT experiments are insufficiently discussed in the main text. Overall, I would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. You need to properly discuss the relation to multiplicative/order 2 networks and highlight the differences. Unclear discussion can be eliminated to make the experimental setup and the results presentation clearer in the main text. *** References *** M.W. Goudreau, C.L. Giles, S.T. Chakradhar, D. Chen, *First-Order Vs. Second-Order Single Layer Recurrent Neural Networks,*IEEE Trans. on Neural Networks, 5 (3), p. 511, 1994. Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil, *Model Compression,* The Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2006), August 2006, pp. 535-541. Dark knowledge, G Hinton, O Vinyals, J Dean 2014 W. Chen, J. Wilson, S. Tyree, K. Weinberger and Y. Chen, Compressing Neural Networks with the Hashing Trick, Proc. International Conference on Machine Learning (ICML-15)","['Rating:###9: Top 15% of accepted papers, strong accept', 'Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###7: Good paper, accept', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",9,4,"Strengths: 
1. The paper proposes an interesting new method for training neural networks, i.e., a hypernetwork is used to generate the model parameters of the main network. The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task. In particular, the hyperLSTM with non-shared weights can achieve excellent results compared to conventional LSTM and its variants on a couple of LM tasks, which is very inspiring.
2. The experiments are very solid.

Weaknesses: 
1. The paper would be much stronger if it was more focused. In particular, it is unclear what is the key advantage of this hypernetwork approach. It is argued that in the paper that can achieve competitive results using smaller number of trainable model parameters. However, in the running time, the computational complexity is the same as the standard main network for static networks, such as ConvNet, and the computational cost is even larger for dynamic networks such as LSTMs. The improvements of hyperLSTMs over conventional LSTM and its variants seem mainly come from increasing the number of model parameters.
2. The paper could be better written. It is too long for the conference format and need refocussing. On related work, the relation with multiplicative RNN and their generic tensor product predecessor (Order 2 networks, wrt C. Lee Giles definition) should be mentioned in the related work section and the differences with earlier research need to be explained and motivated ("
Skip-graph: Learning graph embeddings with an encoder-decoder model | OpenReview,"In this work, we study the problem of feature representation learning for graph-structured data. Many of the existing work in the area are task-specific and based on supervised techniques. We study a method for obtaining a generic feature representation for a graph using an unsupervised approach. The neural encoder-decoder model is a method that has been used in the natural language processing domain to learn feature representations of sentences. In our proposed approach, we train the encoder-decoder model to predict the random walk sequence of neighboring regions in a graph given a random walk along a particular region. The goal is to map subgraphs — as represented by their random walks — that are structurally and functionally similar to nearby locations in feature space. We evaluate the learned graph vectors using several real-world datasets on the graph classification task. The proposed model is able to achieve good results against state-of- the-art techniques.","This paper proposes an unsupervised graph embedding learning method based on random walk and skip-thought model. They show promising results compared to several competitors on four chemical compound datasets. Strength: 1, The idea of learning the graph embedding by applying skip-thought model to random walk sequences is interesting. 2, The paper is well organized. Weakness: 1, As the current datasets are small (e.g., the average number of nodes per graph is around 30), it would be great to explore larger graph datasets to further investigate the method. 2, Comparisons with recent work like LINE and node2vec are missing. You can compare them easily by applying the same aggregation strategy to their node embeddings. Detailed Questions: 1, The description about how to split the random walk sequence into 3 sub-sequences is missing. Also, the line “l_min >= (n_k - 1), … >= l_max” in section 2.2.2 is a mistake. 2, Can you provide the standard deviations of the 5-fold cross validation in Table 2? I’m curious about how stable the algorithm is.  This paper studies the graph embedding problem by using the encoder-decoder method. The experimental study on real network data sets show the features extracted by the proposed model is good for classification. Strong points of this paper: 1. The idea of using the methods from natural language processing to graph mining is quite interesting. 2. The organization of the paper is clear Weak points of this paper: 1. Comparisons with state-of-art methods (Graph Kernels) is missing. 2. The problem is not well motivated, are there any application of this. What is the different from the graph kernel methods? The comparison with graph kernel is missing. 3. Need more experiment to demonstrate the power of their feature extraction methods. (Clustering, Search, Prediction etc.) 4. Presentation of the paper is weak. There are lots of typos and unclear statements. 5. The author mentioned about the graph kernel things, but in the experiment they didn*t compare them. Also, only compare the classification accuracy by using the proposed method is not enough.  Authors take the skip-graph architecture (Kiros 2015) and apply it to classifying labeled graphs (molecular graphs). They do it by creating many sentences by walking the graph randomly, and asking the model to predict previous part and next part from the middle part. Activations of the decoder part of this model on a walk generated from a new graph are used as features for a binary classifier use to predict whether the molecule has anti-cancer properties. Paper is well written, except that evaluation section is missing details of how the embedding is used for actual classification (ie, what classifier is used) Unfortunately I*m not familiar with the dataset and how hard it is to achieve the results they demonstrate, that would be the important factor to weight on the papers acceptance.  The paper presents a method to learn graph embeddings in a unsupervised way using random walks. It is well written and the execution appears quite accurate. The area of learning whole graph representations does not seem to be very well explored in general, and the proposed approach enjoys having very few competitors. In a nutshell, the idea is to linearize the graph using random walks and to compute the embedding of the central segment of each walk using the skip-thought criterion. Being not an expert in biology, I can not comment whether or not this makes sense, but the gains reported in Table 2 are quite significant. An anonymous public comment compared this work to a number of others in which the problem of learning representations of nodes is considered. While this is arguably a different goal, one natural baseline would be to pool these representations using mean- or max- pooling. It would very interesting to do such a comparison, especially given that the considered approach heavily relies on pooling (see Figure 3(c)) To sum up, I think it is a nice paper, and with more baselines I would be ready to further increase the numerical score.","['Rating:###5: Marginally below acceptance threshold', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###1: The reviewer*s evaluation is an educated guess', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",5,4,"Strengths: 1, The idea of learning the graph embedding by applying skip-thought model to random walk sequences is interesting. 2, The paper is well organized. Weaknesses: 1, As the current datasets are small (e.g., the average number of nodes per graph is around 30), it would be great to explore larger graph datasets to further investigate the method. 2, Comparisons with recent work like LINE and node2vec are missing. You can compare them easily by applying the same aggregation strategy to their node embeddings. Detailed Questions: 1, The description about how to split the random walk sequence into 3 sub-sequences is missing. Also, the line “l_min >= (n_k - 1), … >= l_max” in section 2.2.2 is a mistake. 2, Can you provide the standard deviations of the 5-fold cross validation in Table 2? I’m curious about how stable the algorithm is. Rating: 8
Confidence: 4
Strengths: 1. The idea of using the methods from natural language processing to graph mining is quite interesting. 2. The organization of the paper is clear Weaknesses: 1. Comparisons with state-of-art methods (Graph Kernels) is missing. 2. The problem is not well motivated, are there any application of this. What is the different from the graph kernel methods? The comparison with graph kernel is missing"
Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data | OpenReview,"Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as **teachers** for a **student** model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student*s privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student*s training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning.","This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing.  This paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as ``teachers** model, which will train a ``student** model to predict an output chosen by noisy voting among all of the teachers. The theoretical results are nice but also intuitive. Since teachers* result are provided via noisy voting, the student model may not duplicate the teacher*s behavior. However, the probabilistic bound has quite a number of empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not. The experiments on MNIST and SVHN are good. However, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications.  Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough. One caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance. Another caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work. Finally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used. Other comments: Discussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as “teacher-learner”). The only time the private labeled data is used is when learning the “primary ensemble.” A *secondary ensemble* is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble. G. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013. Section C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. The paper is extremely well-written, for the most part. Some places needing clarification include: - Last paragraph of 3.1. “all teachers….get the same training data….” This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database. - 4.1: The authors state: “The number n of teachers is limited by a trade-off between the classification task’s complexity and the available data.” However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution. - Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.","['Rating:###9: Top 15% of accepted papers, strong accept', 'Rating:###7: Good paper, accept', 'Rating:###9: Top 15% of accepted papers, strong accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",9,4,"Strengths: 
1. The paper clearly and thoroughly discusses the problem of achieving privacy for sensitive data in machine learning. 
2. The authors propose a novel approach to achieve privacy for sensitive data in machine learning. 
3. The approach is well explained and is easy to understand. 
4. The authors provide a good discussion of the related work. 
5. The paper is well-written and is easy to read. 

Weaknesses: 
1. The paper does not provide a clear theoretical bound on the privacy of the proposed approach. 
2. The paper does not provide a clear theoretical bound on the learning performance of the proposed approach. 
3. The paper does not provide a clear theoretical bound on the privacy of the proposed approach when the teachers are trained on disjoint subsets of sensitive data. 
4. The paper does not provide a clear theoretical bound on the privacy of the proposed approach when the teachers are trained on disjoint subsets of sensitive data and the student is trained on public data labeled by teachers through noisy voting. 
5. The paper does not provide a clear theoretical bound on the privacy of the proposed approach when the teachers are trained on disjoint subsets of sensitive data and the student is trained on public data labeled by teachers through noisy voting. 

Rating: 8
Confidence: 4"
Song From PI: A Musically Plausible Network for Pop Music Generation | OpenReview,"We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.","In this paper, the authors build music-theoretical structure directly into a music generating LSTM. Even though such simple rules should be learnable from data, this surely is a neat idea for the limited-data regime. Further, it seems like a desirable model when thinking about musicians, for instance, wanting to train on own (and thus limited) source material. They consider a dataset of 100 hours of midi and add multiple priors, drawn from basic music theory. The priors as such are OK, but some of them seem rather heuristic and should, in my opinion, be learned from data and it should be discussed how the performance changes if you remove them. Further, the authors evaluate their study on an artificial benchmark, consisting of a behavioral experiment where 27 subjects judge songs generated by Magenta and their approach in a questionable side-by-side evaluation. Using this as a performance criterion is problematic, as no details about the subjects are given away and no attempt is made to assess the statistical significance of such results, let alone discussing the difficulty of pairing the songs. Further, I assume there are standard behavioral batteries, concerned with assessing music preferences that should have been used or at least addressed. Introducing the neural Karaoke and dancing is fun, but does not have much scientific value at this point, as it does not seem to work in a meaningful way. I would recommend to either improve results of the latter drastically or add it as an extra to a blog post and remove it from the paper. It is good that the authors make an attempt to encode general prior knowledge into their architecture, but I am not convinced by the results and the heuristic choices being made. Further, it is still not 100% clear to me how the weighted probability distribution is constructed for the scales and how strong the prior it effectively incorporates is. If it is very strong, it is not surprising to me, that the songs sound relatively coherent, as in scale playing with rejected outliers have to sound somewhat coherent. I am not familiar enough with the Magenta baseline system and it is problematic that the baseline is not explained well. If the baseline does not take explicit scale priors into account, it does make sense that it sounds less coherent by definition. This has to be discussed and the effect of the introduced priors has to be evaluated. Finally, the question remains if this will generalize to datasets with more than 4 dominant scales and why the authors chose their thresholds the way they did. Did the model perform worse if one chooses to include more scales? How do you know, that the heavy tail of such a distribution is not desirable and important for natural sounding music, did you investigate this? The multitrack idea is great. However, I am not convinced it works in this case. The results sound more like assigning a couple of notes in the melodies to rhythmic sounds, but they do not interact with the melody, they just move along, as if they were part of the melody. This is not how rhythm works in music in most cases. Pro: + Incorporating general musical knowledge into the learned network is a good idea and non-trivial. + Idea to introduce a behavioral measure for the quality of the generated samples is useful, as music is very subjective. + The multitrack idea seems useful and a clear step beyond Magenta as far as I understand. Con: - However, the multitrack part of the architecture does not seem to work properly, rhythm does not seem to behave differently from melodic movement. - The music excerpts sound very simplistic and similar. - The pair-wise evaluation metric with 27 supposedly *random* subjects is not very meaningful and very likely, not significant. - Evaluation of generative models is difficult, but the authors could have done better. - The 4 bins of the random scale variable seem ad-hoc. I have to emphasize that I like the ideas introduced in this paper, but I am not convinced by the way they are presented and evaluated. I would like to suggest this paper for workshop publication.  The paper presents a recurrent neural network (RNN) for generating pop music. The model is trained on 100 hours of user composed pop songs and video game music and the resulting music is evaluated in user studies against songs produced by the Magenta framework. Overall, I find the paper to be well written and clear. I appreciate the review early on of music theory concepts. I think the paper provides a reasonable support for the connection between how pop music is composed and the hierarchical model for generating melody accompanied with chords and drums. With some post-processing, the model appears to generate pleasant sounding music as judged by users and from a personal perspective of listening to the examples available on the web. While the generated examples on the web sound pleasant, they also sound quite similar and make it hard to judge what the model has learned. There are some open questions regarding evaluation of the model. The paper would benefit from improvements in both user and metric evaluations. * The Magenta system serves as a lower baseline for evaluation. The study would benefit from an upper baseline by also evaluating against human composed songs. This would help contextualize the findings for both this and future work. * The user study could be improved by examining other dimensions of appeal, perhaps to gauge diversity through *interestingness* over a collection of samples. * I think a paired/side-by-side design for the user study seems limited (examples on http://www.cs.toronto.edu/songfrompi/eval/eval.html). A simpler design with rating one sample at a time may have been more appropriate because there is no natural way to pair the songs. The examples from each system used in the experiment should be provided with labels or an answer key so that readers can compare the merits of each of the systems* compositions themselves. * The authors propose specific metrics for insight into the diversity (longest subsequence and number of repeats). These would be more meaningful with some context, e.g. by comparison with baseline Magenta samples and the training data (as an upper baseline). * Details of the baseline Magenta system would also benefit the paper. * No guidance is provided on how to judge the applications of neural singing, dancing and karaoke.  The paper describes a recurrent neural network model for generating pop music in the symbolic domain (i.e. MIDI). The layers of the model each generate part of the output, with the first few layers responsible for generating the melody, and further layers generating drums and chords conditioned on the generated melody. The authors argue that this matches how pop music is usually composed. The model is trained on 100+ hours of pop music in MIDI format. The resulting generated music is compared against that produced by another system using human evaluation, which is probably the only way in which such a system can be fairly evaluated. I appreciate that the authors went through the trouble of setting up these experiments. The RNNs generating the different outputs (i.e. key, duration, chord, melody) are trained in sequence, conditioned on the output of the previous step(s).I found the text a bit confusing at times as it initially seems to describe a single end-to-end trained model (even if this is never stated explicitly). It only becomes clear later on that the layers are trained in sequence, with additional supervision provided at each stage. This may simply be a personal bias because recent work on hierarchical RNNs that I*ve read has focused on end-to-end training, but nevertheless it might be useful to mention this more clearly from the get-go. The post-processing of model samples described in the 2nd paragraph of Section 4.5 seems to affect results quite dramatically (based on the results in Table 1). It seems equally applicable to the outputs of the Magenta system, so it might be interesting to compare this version to Magenta as well, to get an idea of how much it contributes to the improvement over the Magenta system. It would be somewhat disappointing if it ends up accounting for most of the gain. I am still unconvinced by the experiment described in the last paragraph of Section 5, where subsequences of generated music fragments are searched for in the training data. While I agree with the authors that minor differences in note choice can have profound effects on how the melody is perceived, I still think this is not particularly convincing, and I think drawing the unambiguous conclusion that *our model is able to generate new music* from this experiment is a bit premature. The additional applications described in Section 6 feel a bit like an afterthought and the datasets used are probably too small for the results to be meaningful. Instead I would have preferred to read about how to reduce the importance of prior knowledge in the design of the model. Considering the venue this work was submitted to, moving towards a more *end-to-end learning* approach (rather than incorporating even more prior knowledge, as the conclusion seems to imply) seems like an interesting direction for future research. Minor remark: giving the formulas for LSTM is probably a bit of a waste of space, especially if you*re not explaining the semantics. A reference is sufficient, and in fact adding a reference to the original LSTM paper is probably a good idea regardless.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",4,3,"Strengths: 1. The authors introduce a novel framework for generating pop music. 2. The model is trained on 100+ hours of pop music in MIDI format. 3. The authors argue that this matches how pop music is usually composed. Weaknesses: 1. The model is trained in sequence, with additional supervision provided at each stage. This may be confusing to the reader. 2. The post-processing of model samples described in the 2nd paragraph of Section 4.5 seems to affect results quite dramatically. It seems equally applicable to the outputs of the Magenta system, so it might be interesting to compare this version to Magenta as well, to get an idea of how much it contributes to the improvement over the Magenta system. 3. The additional applications described in Section 6 feel a bit like an afterthought and the datasets used are probably too small for the results to be meaningful. Rating: 7 Confidence: 4"
Extrapolation and learning equations | OpenReview,"In classical machine learning, regression is treated as a black box process of identifying a suitable function from a hypothesis set without attempting to gain insight into the mechanism connecting inputs and outputs. In the natural sciences, however, finding an interpretable function for a phenomenon is the prime goal as it allows to understand and generalize results. This paper proposes a novel type of function learning network, called equation learner (EQL), that can learn analytical expressions and is able to extrapolate to unseen domains. It is implemented as an end-to-end differentiable feed-forward network and allows for efficient gradient based training. Due to sparsity regularization concise interpretable expressions can be obtained. Often the true underlying source expression is identified.","Thank you for an interesting perspective on the neural approaches to approximate physical phenomenon. This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc. Pros - The approach is rather simple and hence can be applied to existing methods. The major difference is incorporating functions with 2 or more inputs which was done successfully in the paper. - It seems that MLP, even though it is good for interpolation, it fails to extrapolate data to model the correct function. It was a great idea to use basis functions like sine, cosine to make the approach more explicit. Cons - Page 8, the claim that x2 cos(ax1 + b) ~ 1.21(cos(-ax1 + ? + b + 0.41x2) + sin(ax1 + b + 0.41x2)) for y in [-2,2] is not entirely correct. There should be some restrictions on *a* and *b* as well as the approximate equality doesn*t hold for all real values of *a* and *b*. Although, for a=2*pi and b=pi/4, the claim is correct so the model is predicting a correct solution within certain limits. - Most of the experiments involve up to 4 variables. It would be interesting to see how the neural approach models hundreds of variables. - Another way of looking at the model is that the non-linearities like sine, cosine, multiplication act as basis functions. If the data is a linear combination of such functions, the model will be able to learn the weights. As division is not one of the non-linearities, predicting expressions in Equation 13 seems unlikely. Hence, I was wondering, is it possible to make sure that this architecture is a universal approximator. Suggested Edits - Page 8, It seems that there is a typographical error in the expression 1.21(cos(ax1 + ? + b + 0.41x2) + sin(ax1 + b + 0.41x2)). When compared with the predicted formula in Figure 4(b), it should be 1.21(cos(-ax1 + ? + b + 0.41x2) + sin(ax1 + b + 0.41x2)).  The authors attempt to extract analytical equations governing physical systems from observations - an important task. Being able to capture succinct and interpretable rules which a physical system follows is of great importance. However, the authors do this with simple and naive tools which will not scale to complex tasks, offering no new insights or advances to the field. The contribution of the paper (and the first four pages of the submission!) can be summarised in one sentence: *Learn the weights of a small network with cosine, sinusoid, and input elements products activation functions s.t. the weights are sparse (L1)*. The learnt network weights with its fixed structure are then presented as the learnt equation. This research uses tools from literature from the *90s (I haven*t seen the abbreviation ANN (page 3) for a long time) and does not build on modern techniques which have advanced a lot since then. I would encourage the authors to review modern literature and continue working on this important task.  Thank you for an interesting read. To my knowledge, very few papers have looked at transfer learning with **no** target domain data (the authors called this task as *extrapolation*). This paper clearly shows that the knowledge of the underlying system dynamics is crucial in this case. The experiments clearly showed the promising potential of the proposed EQL model. I think EQL is very interesting also from the perspective of interpretability, which is crucial for data analysis in scientific domains. Quesions and comments: 1. Multiplication units. By the universal approximation theorem, multiplication can also be represented by a neural network in the usual sense. I agree with the authors* explanation of interpolation and extrapolation, but I still don*t quite understand why multiplication unit is crucial here. I guess is it because this representation generalises better when training data is not that representative for the future? 2. Fitting an EQL vs. fitting a polynomial. It seems to me that the number of layers in EQL has some connections to the degree of the polynomial. Assume we know the underlying dynamics we want to learn can be represented by a polynomial. Then what*s the difference between fitting a polynomial (with model selection techniques to determine the degree) and fitting an EQL (with model selection techniques to determine the number of layers)? Also your experiments showed that the selection of basis functions (specific to the underlying dynamics you want to learn) is crucial for the performance. This means you need to have some prior knowledge on the form of the equation anyway! 3. Ben-David et al. 2010 has presented some error bounds for the hypothesis that is trained on source data but tested on the target data. I wonder if your EQL model can achieve better error bounds? 4. Can you comment on the comparison of your method to those who modelled the extrapolation data with **uncertainty**?","['Rating:###7: Good paper, accept', 'Rating:###3: Clear rejection', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",7,4,"Strengths: 1. The authors propose a novel type of function learning network, called equation learner (EQL), that can learn analytical expressions and is able to extrapolate to unseen domains. 2. The approach is simple and hence can be applied to existing methods. 3. The authors attempt to extract analytical equations governing physical systems from observations - an important task. 4. The experiments clearly showed the promising potential of the proposed EQL model. Weaknesses: 1. The approach is based on the assumption that the underlying system dynamics is linear. This assumption is not justified in the paper. 2. The authors do not discuss the possibility of using a more complex model, such as a polynomial, to model the underlying dynamics. 3. The authors do not discuss the possibility of using a more complex model, such as a polynomial, to model the underlying dynamics. 4. The experiments involve only a few variables. It would be interesting to see how the neural approach models hundreds of variables. Rating: 6 Confidence: 3"
Combating Deep Reinforcement Learning*s Sisyphean Curse with Intrinsic Fear | OpenReview,"To use deep reinforcement learning in the wild, we might hope for an agent that can avoid catastrophic mistakes. Unfortunately, even in simple environments, the popular deep Q-network (DQN) algorithm is doomed by a Sisyphean curse. Owing to the use of function approximation, these agents eventually forget experiences as they become exceedingly unlikely under a new policy. Consequently, for as long as they continue to train, DQNs may periodically relive catastrophic mistakes. Many real-world environments where people might be injured exhibit a special structure. We know a priori that catastrophes are not only bad, but that agents need not ever get near to a catastrophe state. In this paper, we exploit this structure to learn a reward-shaping that accelerates learning and guards oscillating policies against repeated catastrophes. First, we demonstrate unacceptable performance of DQNs on two toy problems. We then introduce intrinsic fear, a new method that mitigates these problems by avoiding dangerous states. Our approach incorporates a second model trained via supervised learning to predict the probability of catastrophe within a short number of steps. This score then acts to penalize the Q-learning objective, shaping the reward function away from catastrophic states.","This paper presents a heuristic for avoiding large negative rewards which have already been experienced by distilling such events into a *danger model*. The paper is well written including some rather poetic language [*]. The heuristic is evaluated in two toy domains. I would think that in order to properly evaluate this one would use a well known benchmark e.g. Atari. Atari seems particularly apt since those games are full of catastrophes (i.e. sudden death). [*] this reviewer*s favourite quotes: *Imagine a self-driving car that had to periodically hit a few pedestrians in order to remember that it’s undesirable.* *The child can learn to adjust its behaviour without actually having to stab someone.* *... the catastrophe lurking just past the optimal shave.*  - The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. After the pre-review comments, authors do mention that they compared against expected SARSA but I would really like to see these and other extensive baselines before accepting this paper. - There is also an increasing amount of literature of using reward replay buffers in deep RL agents (c.f. Jaderberg, Max, et al. *Reinforcement learning with unsupervised auxiliary tasks.*, Blundell, Charles, et al. *Model-free episodic control.* , Narasimhan et al. *Language understanding for text-based games using deep reinforcement learning*), which could perhaps reinforce the agent to avoid revisiting catastrophic states. - Overall, the approach presented is not very principled. For instance, why isn*t catastrophe directly provided as a signal to the learner instead of a separate model?  This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea). 1) insufficient context of what is known and had been studied before (in shallow RL), for example within the field of “robust RL”. A good place to start might be with the work of Shie Mannor. 2) an ill-defined general problem setup. Does it make sense to do post-hoc labeling of certain actions as “catastrophic” if the agent is not informed about that metric during learning? Training a system to do one thing (maximize reward), but then evaluating it with a different metric is misleading. On the training metric, it could even be that the baseline outperforms the new algorithm? So I’d want to see plots for “average reward” in fig 3 as well. Also, what would the baseline learn if it was given large negative rewards for entering these otherwise invisible “danger states”? 3) a somewhat ad-hoc solution, that introduces new domain-specific hyperparameters (k_r, k_lambda and lambda) a second deep network and and two additional replay memories. In terms of results, I’m also unsure whether I can trust the results, given the long-standing track-record of cart-pole being fully solved by many methods: is DQN an outlier here? Or is the convnet not an appropriate function-approximator? Actually: which exact variant “state-of-the-art” variant of DQN are you using? The good idea that I encourage the authors to pursue further is D_d, this set of rare but dangerous states, that should be kept around in some form. I see it as an ingredient for continual learning that most typical methods lack -- it is also one of the big differences between RL and supervised learning, where such states would generally be discarded as outliers. Given my comment a couple of weeks ago, and the prompt response (“we implemented expected SARSA”), I would have expected that the paper had been revised with the new results by now? In any case, I’m open to discussing all these points and revising my opinion based on an updated version of the paper. Minor comment: the bibliography is done sloppily, with missing years, conference venues and missing/misspelled author lists, e.g. “Sergey et al. Levine”. I also think it is good form to cite the actual conference publications instead of arXiv where applicable.","['Rating:###5: Marginally below acceptance threshold', 'Rating:###4: Ok but not good enough - rejection', 'Rating:###4: Ok but not good enough - rejection']","['Confidence:###2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",5,2,"Strengths: 1. The authors present a heuristic for avoiding large negative rewards which have already been experienced by distilling such events into a *danger model*. 2. The heuristic is evaluated in two toy domains. 3. The authors present a good idea of using a second deep network to predict the probability of catastrophe within a short number of steps. Weaknesses: 1. The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. After the pre-review comments, authors do mention that they compared against expected SARSA but I would really like to see these and other extensive baselines before accepting this paper. 2. There is also an increasing amount of literature of using reward replay buffers in deep RL agents (c.f. Jaderberg, Max, et al. *Reinforcement learning with unsupervised auxiliary tasks.*, Blundell, Charles, et al. *Model-free episodic control.*, Narasimhan et al. *Language understanding for text-based games using deep reinforcement learning*), which could perhaps reinforce the agent to avoid revisiting catastrophic states. 3. Overall, the approach presented is not very principled. For instance, why isn*t catastrophe directly provided as a signal to the learner instead of a separate model?  This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea). 1) insufficient context of what is"
Identity Matters in Deep Learning | OpenReview,"An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as batch normalization, but was also key to the immense success of residual networks. In this work, we put the principle of identity parameterization on a more solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for feed-forward networks in their standard parameterization is substantially more delicate. Second, we show that residual networks with ReLu activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size. Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks.","This paper provides some theoretical guarantees for the identity parameterization by showing that 1) arbitrarily deep linear residual networks have no spurious local optima; and 2) residual networks with ReLu activations have universal finite-sample expressivity. This paper is well written and studied a fundamental problem in deep neural network. I am very positive on this paper overall and feel that this result is quite significant by essentially showing the stability of auto-encoder, given the fact that it is hard to provide concrete theoretical guarantees for deep neural networks. One of key questions is how to extent the result in this paper to the more general nonlinear actuation function case. Minors: one line before Eq. (3.1), U in R ?  imes k  Paper Summary: Authors investigate identity re-parametrization in the linear and the non linear case. Detailed comments: — Linear Residual Network: The paper shows that for a linear residual network any critical point is a global optimum. This problem is non convex it is interesting that this simple re-parametrization leads to such a result. — Non linear Residual Network: Authors propose a construction that maps the points to their labels via a resnet , using an initial random projection, followed by a residual block that clusters the data based on their label, and a last layer that maps the clusters to the label. 1- In Eq 3.4 seems the dimensions are not matching q_j in R^k and e_j in R^r. please clarify 2- The construction seems fine, but what is special about the resnet here in this construction? One can do a similar construction if we did not have the identity? can you discuss this point? In the linear case it is clear from a spectral point of view how the identity is helping the optimization. Please provide some intuition. 3- Existence of a network in the residual class that overfits does it give us any intuition on why residual network outperform other architectures? What does an existence result of such a network tell us about its representation power ? A simple linear model under the assumption that points can not be too close can overfit the data, and get fast convergence rate (see for instance tsybakov noise condition). 4- What does the construction tell us about the number of layers? 5- clustering the activation independently from the label, is an old way to pretrain the network. One could use those centroids as weights for the next layer (this is also related to Nystrom approximation see for instance https://www.cse.ust.hk/~twinsen/nystrom.pdf ). Your clustering is very strongly connected to the label at each residual block. I don*t think this is appealing or useful since no feature extraction is happening. Moreover the number of layers in this construction does not matter. Can you weaken the clustering to be independent to the label at least in the early layers? then one could you use your construction as an initialization in the training. — Experiments : - last layer is not trained means the layer before the linear layer preceding the softmax? Minor comments: Abstract: how the identity mapping motivated batch normalization?  This paper investigates the identity parametrization also known as shortcuts where the output of each layer has the form h(x)+x instead of h(x). This has been shown to perform well in practice (eg. ResNet). The discussions and experiments in the paper are interesting. Here*s a few comments on the paper: -Section 2: Studying the linear networks is interesting by itself. However, it is not clear that how this could translate to any insight about non-linear networks. For example, you have proved that every critical point is global minimum. I think it is helpful to add some discussion about the relationship between linear and non-linear networks. -Section 3: The construction is interesting but the expressive power of residual network is within a constant factor of general feedforward networks and I don*t see why we need a different proof given all the results on finite sample expressivity of feedforward networks. I appreciate if you clarify this. -Section 4: I like the experiments. The choice of random projection on the top layer is brilliant. However, since you have combined this choice with all-convolutional residual networks, it is hard for the reader to separate the affect of each of them. Therefore, I suggest reporting the numbers for all-convolutional residual networks with learned top layer and also ResNet with random projection on the top layer. Minor comments: 1- I don*t agree that Batch Normalization can be reduced to identity transformation and I don*t know if bringing that in the abstract without proper discussion is a good idea. 2- Page 5 above assumption 3.1 : x^(i)=1 ==> ||x^(i)||_2=1","['Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",8,3,Strengths: 1. The paper provides a theoretical analysis of the identity parameterization in deep neural networks. 2. The analysis is well written and easy to follow. 3. The experiments are interesting and provide a good intuition on the importance of identity parameterization. Weaknesses: 1. The paper is not very clear on the importance of identity parameterization. The authors do not discuss the potential reasons why identity parameterization is important. 2. The paper does not discuss the potential limitations of the identity parameterization. 3. The paper does not discuss the potential extensions of the identity parameterization. Rating: 6 Confidence: 3
A recurrent neural network without chaos | OpenReview,"We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.","The authors of the paper set out to answer the question whether chaotic behaviour is a necessary ingredient for RNNs to perform well on some tasks. For that question*s sake, they propose an architecture which is designed to not have chaos. The subsequent experiments validate the claim that chaos is not necessary. This paper is refreshing. Instead of proposing another incremental improvement, the authors start out with a clear hypothesis and test it. This might set the base for future design principles of RNNs. The only downside is that the experiments are only conducted on tasks which are known to be not that demanding from a dynamical systems perspective; it would have been nice if the authors had traversed the set of data sets more to find data where chaos is actually necessary.  I think the authors provide an interesting direction for understanding and maybe constructing recurrent models that are easier to interpret. Is not clear where such direction will lead but I think it could be an interesting starting point for future work, one that worth exploring.  This paper poses an interesting idea: removing chaotic behavior or RNNs. While many other papers on new RNN architecture usually focus too much on the performance improvement and leave the analysis part on their success as a black-box, this paper does a good job on presenting why its method may work well. Although, the paper shows lots of comparison between the chaotic systems (GRUs & LSTMs) and the stable system (proposed CFN model), the reviewer is not fully convinced by the main claim of this paper, the nuance that chaotic behaviour makes dynamic system to have rich representation power but makes the system too unstable. In the paper, the LSTM shows a very sensitive behaviour, even when a very small amount of noise is added to the input. However, it still performs surprisingly well with this chaotic behaviour. Measuring the model complexity is a very difficult task, therefore, many papers manage to use either same number of hidden units or choose approximately close model sizes. In this paper, the experiments were carried by using the same amount of parameters for both the LSTM and CFN. However, I think the CFN may have much more simpler computational graph. Taking the idea of this work, can we develop a stable dynamic system, but which does not only have one attractor? It is also interesting to see that the layers of CFNs are updated in different timescales in a sense that the decaying speed decreases when the layer gets higher. Could you provide more statistics on this? For example, what is the average relaxation time of the whole hidden units at each layer? Batch normalization and layer normalization can be helpful to make the training of RNNs become more stable. How would the behaviour of batch normalized or perhaps layer normalized LSTM look like? Also, it is often not trivial to make batch normalization or layer normalization to work on a new architecture. I think it may be useful to compare batch normalized or layer normalized versions of the LSTM and CFN. The quality of the work is good, explanation is clear enough along with nice analyses and proofs. Overall, the performance is not any better than LSTMs, but it is still interesting when thinking of simplicity of this model. I am a bit concerned if this model might not work that well in more harder task, e.g., translation. Figure 4 of this paper is very interesting, where the proposed architecture shows that the hidden units at the second layer tends to keep its information longer than the first layer ones.","['Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",8,3,Strengths: 1. The paper presents a novel idea of removing chaotic behavior from RNNs. 2. The paper is well-written and easy to understand. 3. The paper is well-structured and easy to follow. 4. The paper is well-motivated and well-justified. 5. The paper is well-organized and easy to read. Weaknesses: 1. The paper does not provide any clear direction for future work. 2. The paper does not provide any clear direction for future work. 3. The paper does not provide any clear direction for future work. 4. The paper does not provide any clear direction for future work. 5. The paper does not provide any clear direction for future work. Rating: 7 Confidence: 4
Towards a Neural Statistician | OpenReview,"An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.","This paper proposes a hierarchical generative model where the lower level consists of points within datasets and the higher level models unordered sets of datasets. The basic idea is to use a *double* variational bound where a higher level latent variable describes datasets and a lower level latent variable describes individual examples. Hierarchical modeling is an important and high impact problem, and I think that it*s under-explored in the Deep Learning literature. Pros: -The few-shot learning results look good, but I*m not an expert in this area. -The idea of using a *double* variational bound in a hierarchical generative model is well presented and seems widely applicable. Questions: -When training the statistic network, are minibatches (i.e. subsets of the examples) used? -If not, does using minibatches actually give you an unbiased estimator of the full gradient (if you had used all examples)? For example, what if the statistic network wants to pull out if *any* example from the dataset has a certain feature and treat that as the characterization. This seems to fit the graphical model on the right side of figure 1. If your statistic network is trained on minibatches, it won*t be able to learn this characterization, because a given minibatch will be missing some of the examples from the dataset. Using minibatches (as opposed to using all examples in the dataset) to train the statistic network seems like it would limit the expressive power of the model. Suggestions: -Hierarchical forecasting (electricity / sales) could be an interesting and practical use case for this type of model.  Sorry for the late review -- I*ve been having technical problems with OpenReview which prevented me from posting. This paper presents a method for learning to predict things from sets of data points. The method is a hierarchical version of the VAE, where the top layer consists of an abstract context unit that summarizes a dataset. Experiments show that the method is able to *learn to learn* by acquiring the ability to learn distributions from small numbers of examples. Overall, this paper is a nice addition to the literature on one- or few-shot learning. The method is conceptually simple and elegant, and seems to perform well. Compared to other recent papers on one-shot learning, the proposed method is simpler, and is based on unsupervised representation learning. The paper is clearly written and a pleasure to read. The name of the paper is overly grandiose relative to what was done; the proposed method doesn’t seem to have much in common with a statistician, unless one means by that *someone who thinks up statistics*. The experiments are well chosen, and the few-shot learning results seem pretty solid given the simplicity of the method. The spatial MNIST dataset is interesting and might make a good toy benchmark. The inputs in Figure 4 seem pretty dense, though; shouldn’t the method be able to recognize the distribution with fewer samples? (Nitpick: the red points in Figure 4 don’t seem to correspond to meaningful points as was claimed in the text.) Will the authors release the code?  The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. Comments: - It*s not clear to me why this should be called a *statistician*. Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like *statistic network* and stuck to the more accurate *approximate posterior*. - The experiments are nice, and I appreciate the response to my question regarding *one shot generation*. I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: (a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? (b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one *proper* way of computing the *one shot generation* performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don*t see a reason not to include that.","['Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",8,2,Strengths: 1. The paper presents a novel approach to learning to summarize data. 2. The approach is well motivated and is clearly explained. 3. The results are well presented and are easy to understand. Weaknesses: 1. The paper is not very clear in its presentation. 2. The paper does not provide a clear motivation for the approach. 3. The results are not very impressive. Rating: 6 Confidence: 3
What does it take to generate natural textures? | OpenReview,"Natural image generation is currently one of the most actively explored fields in Deep Learning. Many approaches, e.g. for state-of-the-art artistic style transfer or natural texture synthesis, rely on the statistics of hierarchical representations in supervisedly trained deep neural networks. It is, however, unclear what aspects of this feature representation are crucial for natural image generation: is it the depth, the pooling or the training of the features on natural images? We here address this question for the task of natural texture synthesis and show that none of the above aspects are indispensable. Instead, we demonstrate that natural textures of high perceptual quality can be generated from networks with only a single layer, no pooling and random filters.","This work proposed a simple but strong baseline for parametric texture synthesis. In empirical experiments, samples generated by the baseline composed by multi-scale and random filters sometime rival the VGG-based model which has multi-layer and pre-trained filters. The authors concluded that texture synthesis does not necessarily depend on deep hierarchical representations or the learned feature maps. This work is indeed interesting and insightful. However, the conclusions are needed to be further testified (especially for deep hierarchical representations). Firstly, all of generated samples by both VGG and single layer model are not perfect and much worse than the results from non-parametric methods. Besides VGG-based model seems to do better in inpainting task in Figure 7. Last but not least, would a hierarchical model (instead of lots of filters with different size) handle multi-scale more efficiently?  This paper provides an interesting analysis of the conditions which enable generation of natural looking textures. The results is quite surprising, and analysis is quite thorough. I do think the evaluation methods require more work, but as other reviewers mentioned this could be an interesting line of work moving forwards and does not take too much from this current paper which, I think, should be accepted.  The framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis. Figure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments. The main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case. The authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work. Overall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material.","['Rating:###7: Good paper, accept', 'Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###8: Top 50% of accepted papers, clear accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",7,4,"Strengths: 1. The paper presents an interesting analysis of the conditions which enable generation of natural looking textures. 2. The results is quite surprising, and analysis is quite thorough. 3. The paper provides a good discussion on the strength and limitations of such methods for texture synthesis. Weaknesses: 1. The evaluation methods require more work. 2. The paper does not systematically compare different methods against a quantifiable objective. 3. The results may be difficult to confirm due to the lack of a clear evaluation metric. Rating: 8 Confidence: 4"
Warped Convolutions: Efficient Invariance to Spatial Transformations | OpenReview,"Convolutional Neural Networks (CNNs) are extremely efficient, since they exploit the inherent translation-invariance of natural images. However, translation is just one of a myriad of useful spatial transformations. Can the same efficiency be attained when considering other spatial invariances? Such generalized convolutions have been considered in the past, but at a high computational cost. We present a construction that is simple and exact, yet has the same computational complexity that standard convolutions enjoy. It consists of a constant image warp followed by a simple convolution, which are standard blocks in deep learning toolboxes. With a carefully crafted warp, the resulting architecture can be made equivariant to a wide range of 2-parameters spatial transformations. We show encouraging results in realistic scenarios, including the estimation of vehicle poses in the Google Earth dataset (rotation and scale), and face poses in Annotated Facial Landmarks in the Wild (3D rotations under perspective).","The paper shows how group convolutions (for two dimensional commutative groups) can be performed by standard CNNs if the input is warped using a fixed warp. The idea is practical and seems to work well. The paper is well written. I agree with reviewer 1 that the ‘theorems’ do not deserve to be labelled as such. Theorem 1 is equivalent to the second equation from this section of the wikipedia page on convolution: https://en.wikipedia.org/wiki/Convolution#Convolutions_on_groups. The existence of Haar measure (from which commutation of Lg and convolution follows immediately) is a well known and elementary theorem in harmonic analysis, which would be treated in the first few pages of any textbook on the subject. It also immediately clear that any commutative group has an additive parameterization (theorem 2). The fact that the paper does not present new deep mathematical results is not a significant weakness in my opinion, but the derivations should not be camouflaged as such. The claim that previous methods that use group convolutions are slow and that the presented approach has better computational complexity is not supported by empirical evidence, and the theoretical analysis is still a bit misleading. For example, the authors write “Unfortunately these approaches do not possess the same memory and speed benefits that CNNs enjoy. The reason is that, ultimately, they have to enumerate all possible transformations”. The presented method also has to enumerate all transformations (in a limited range, on a discretized grid), and this is feasible only because the group is only 2 dimensional (and indeed this is also true for standard CNNs which enumerate translations). As noted in my pre-review question, I believe the computational complexity analysis is not entirely correct, and assume the authors will correct this. Equation 4 is presented as a new invention, but this has been used in previous works and is well known in mathematics, so a citation should be added. The main advantage of the presented method over several earlier methods is that it is very simple to implement, and can re-use highly optimized convolution routines. As I understand it, Dieleman et al. and Cohen & Welling also use standard convolutions (after a fixed filter / feature map warp), but these papers only consider discrete groups. So it seems like this paper occupies a unique place in the space of equivariant convolutional networks: non-commutative (-), low-dimensional (-), continuous (+) groups, simple (+) and efficient (+) algorithm. Given the proximity though, a more thorough and balanced appraisal of the merits and demerits, as well as the novelty, relative to each of the previous works would be useful. Provided that these issues are cleared up, I would recommend the paper for publication.  The paper suggests method to make the convolution invariant to other type of spatial transformations besides translation. It is interesting that the convolution can be invariant to different variation types by simple warping of the input image, which is related to arbitrary data perturbation in image recognition. Experiment section needs more description and technique implementation details. The relationship of theorems to Lie group is not defined properly. It need more detailed comparison of method with available methods in the literature.  This paper deals with convolution along Lie groups. Pros: - Good numerical results - Interesting applications linked to convolutions on Lie groups Cons: - Sometimes, the writing is not clear/poor. - The theorems 1 and 2 are in fact direct applications of Lie group theory, but this is not made explicit. I do not detail more my review, as I believe my questions/answers complete it fairly. However, if the authors require it, I can be more specific. I think the writing should be improved a bit.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",6,5,"Strengths: 1. The paper presents a simple and efficient method to make a convolutional neural network invariant to a wide range of spatial transformations. 2. The idea is interesting and has potential for applications in computer vision. 3. The paper is well written. Weaknesses: 1. The paper does not provide a clear motivation for the choice of the group. 2. The theorems 1 and 2 are in fact direct applications of Lie group theory, but this is not made explicit. 3. The paper does not provide a clear comparison with other methods in the literature. 4. The experimental results are not very clear. 5. The paper does not provide a clear explanation of the warping function used in the experiments. 6. The paper does not provide a clear explanation of the choice of the group. 7. The paper does not provide a clear explanation of the choice of the group. 8. The paper does not provide a clear explanation of the choice of the group. Rating: 6 Confidence: 3"
The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning | OpenReview,"How much can pruning algorithms teach us about the fundamentals of learning representations in neural networks? A lot, it turns out. Neural network model compression has become a topic of great interest in recent years, and many different techniques have been proposed to address this problem. In general, this is motivated by the idea that smaller models typically lead to better generalization. At the same time, the decision of what to prune and when to prune necessarily forces us to confront our assumptions about how neural networks actually learn to represent patterns in data. In this work we set out to test several long-held hypotheses about neural network learning representations and numerical approaches to pruning. To accomplish this we first reviewed the historical literature and derived a novel algorithm to prune whole neurons (as opposed to the traditional method of pruning weights) from optimally trained networks using a second-order Taylor method. We then set about testing the performance of our algorithm and analyzing the quality of the decisions it made. As a baseline for comparison we used a first-order Taylor method based on the Skeletonization algorithm and an exhaustive brute-force serial pruning algorithm. Our proposed algorithm worked well compared to a first-order method, but not nearly as well as the brute-force method. Our error analysis led us to question the validity of many widely-held assumptions behind pruning algorithms in general and the trade-offs we often make in the interest of reducing computational complexity. We discovered that there is a straightforward way, however expensive, to serially prune 40-70\% of the neurons in a trained network with minimal effect on the learning representation and without any re-training.","The authors have put forward a sincere effort to investigate the *fundamental nature of learning representations in neural networks*, a topic of great interest and importance to our field. They propose to do this via a few simplistic pruning algorithms, to essentially monitor performance decay as a function of unit pruning. This is an interesting idea and one that could potentially be instructive, though in total I don*t think that has been achieved here. First, I find the introduction of pruning lengthy and not particularly novel or surprising. For example, Fig 1 is not necessary, nor is most of the preamble section 3.3.0. The pruning algorithms themselves are sensible (though overly simplistic) approaches, which of course would not matter if they were effective in addressing the question. However, in looking for contributions this paper makes, an interesting, pithy, or novel take on pruning is not one of them, in my opinion. Second, and most relevant to my overall rating, Section 4 does not get deeper than scratching the surface. The figures do not offer much beyond the expected decay in performance as a percentage of neurons removed or gain value. The experiments themselves are not particularly deep, covering a toy problem and MNIST, which does not convince me that I can draw lessons to the broader story of neural networks more generally. Third, there is no essential algorithmic, architectural, or mathematical insight, which I expect out of all but the most heavily experimental papers.  I did enjoy reading some of the introductions and background, in particular that of reminding readers of popular papers from the late 1980s and early 1990s. The idea of the proposal is straight forward: remove neurons based on the estimated change in the loss function from the packpropagation estimate with either first or second order backpropagation. The results are as expected that the first order method is worse then the second order method which in turn is worse than the brute force method. However, there are many reasons why I think that this work is not appropriate for ICLR. For one, there is now a much stronger comprehension of weight decay algorithms and their relation to Bayesian priors which has not been mentioned at all. I would think that any work in this regime would require at least some comments about this. Furthermore, there are many statements in the text that are not necessarily true, in particular in light of deep networks with modern regularization methods. For example, the authors state that the most accurate method is what they call brute-force. However, this assumes that the effects of each neurons are independent which might not be the case. So the serial order of removal is not necessarily the best. I also still think that this paper is unnecessarily long and the idea and the results could have been delivered in a much compressed way. I also don’t think just writing a Q&A section is not enough, and the points should be included in the paper.  The paper introduces a new pruning method for neural networks based on the second-order Taylor expansion and compares the results against a first-order method and brute-force pruning. It performs experiments of the three methods on several toy examples - including a two-layer network on MNIST - and shows that the second-order method behaves much worse then the brute-force baseline. In addition, from the success of the brute-force pruning the authors conclude that the hypothesis of Mozer et al - that neurons either contribute to performance or cancel out the effect of other neurons - is probably correct. The authors put in considerable effort to explain all details of the paper clearly and at length, so the content of the paper is accessible even to people novel to pruning methods. Additionally, the authors have very carefully answered all questions that were coming up through the pre-review and have been very responsive. My major criticism is that the paper lacks focus, does not have a concrete conclusion and does not explain what it adds to the literature. To make this apparent, I here summarise each paragraph of the conclusion section: Paragraph 1: We do not benchmark / Pruning methods do not fare well against brute-force baseline / Some evidence for hypothesis of Mozer & Smolensky, but further investigation needed Paragraph 2: Introduced 2nd order Taylor method / Does not fare well against baseline Paragraph 3: Re-training may help but is not fair Paragraph 4: Brute-force can prune 40-70% in shallow networks Paragraph 5: Brute-force less effective in deep networks Paragraph 6: Not all neurons contribute equally to performance of network The title of the paper and answers of the authors to the pre-review questions seemed to strongly suggest that the paper is not about the new second-order method, is not about benchmarking pruning algorithms but is instead about the learnt representations. But only two or three sentences in the conclusion, and no sentence in the part on results in the abstract, even refers to neural representations. In an answer to the pre-review questions the authors stated: > Furthermore, we do not have to accept the conclusion that re-training is a necessary part of pruning because a brute force search reveals that neurons can in fact be > pruned from trained networks in a piecemeal fashion with no retraining and minimal adverse effect on the overall performance of the network. This would be > impossible if neurons did not belong to the distinct classes we describe.* But this can already be concluded from the 2nd order method, which has a similar characteristic and is based on other 2nd order methods (not shown here). What is the motivation to introduce a new 2nd order method here? In addition, some other minor conclusions about representations - in particular the cancellation effect - might be based on side-effects of the greedy serial pruning method. Optimally, one would need to consider all the different ways of pruning (which, of course, scales exponentially with the number of neurons and is computationally infeasible). Notably, the authors do consider this limitation in the context of conventional pruning methods in the conclusions: *Third, we assumed that pruning could be done in a serial fashion [...]. We found that all of these assumptions are deeply flawed in the sense that the true relevance of a neuron can only be partially approximated [...] at certain stages of the pruning process*. But the brute-force pruning process is also serial - why is that not a problem? All in all it is unclear to me what the paper adds: there are little conclusions regarding the learnt representations nor is there sufficient benchmarking against state-of-the-art pruning methods. I would suggest to focus the paper in the following way: first, use a state-of-the-art pruning method from the literature (that works without re-training) or do not use any other pruning methods besides brute-force (depending on whether you want to compare pruning methods against brute-force, or want to learn something about the learnt representations). In this way you need to write little about this second-order tuning methods, and readers are not so easily confused about the purpose of this paper (plus it will be considerably shorter!). Then concentrate on 2-layer MNIST and a deeper CIFAR10 network. Further focus the paper by adding an itemised list of the exact contributions that you make, and streamline the paper accordingly. These measures could strongly boost the impact of your work but will require a major revision. PS: I think the confusion starts with the following sentence in the abstract: *In this work we set out to test several long-held hypothesis about neural network learning representations and numerical approaches to pruning.* Both aspects are pretty orthogonal, but are completely mixed up in the paper.","['Rating:###3: Clear rejection', 'Rating:###3: Clear rejection', 'Rating:###3: Clear rejection']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",3,4,"Strengths: 1. The authors have put in considerable effort to explain all details of the paper clearly and at length, so the content of the paper is accessible even to people novel to pruning methods. 2. The authors have been very responsive to the pre-review questions. Weaknesses: 1. The paper lacks focus, does not have a concrete conclusion and does not explain what it adds to the literature. 2. The paper does not have a clear motivation for introducing a new 2nd order method. 3. The paper does not have a clear motivation for introducing a new 2nd order method. 4. The paper does not have a clear motivation for introducing a new 2nd order method. 5. The paper does not have a clear motivation for introducing a new 2nd order method. 6. The paper does not have a clear motivation for introducing a new 2nd order method. 7. The paper does not have a clear motivation for introducing a new 2nd order method. 8. The paper does not have a clear motivation for introducing a new 2nd order method. 9. The paper does not have a clear motivation for introducing a new 2nd order method. 10. The paper does not have a clear motivation for introducing a new 2nd order method. Rating: 4 Confidence: 3"
Attentive Recurrent Comparators | OpenReview,"Attentive Recurrent Comparators (ARCs) are a novel class of neural networks built with attention and recurrence that learn to estimate the similarity of a set of objects by cycling through them and making observations. The observations made in one object are conditioned on the observations made in all the other objects. This allows ARCs to learn to focus on the salient aspects needed to ascertain similarity. Our simplistic model that does not use any convolutions performs comparably to Deep Convolutional Siamese Networks on various visual tasks. However using ARCs and convolutional feature extractors in conjunction produces a model that is significantly better than any other method and has superior generalization capabilities. On the Omniglot dataset, ARC based models achieve an error rate of 1.5\% in the One-Shot classification task - a 2-3x reduction compared to the previous best models. This is also the first Deep Learning model to outperform humans (4.5\%) and surpass the state of the art accuracy set by the highly specialized Hierarchical Bayesian Program Learning (HBPL) system (3.3\%).","This paper introduces an attention-based recurrent network that learns to compare images by attending iteratively back and forth between a pair of images. Experiments show state-of-the-art results on Omniglot, though a large part of the performance gain comes from when extracted convolutional features are used as input. The paper is significantly improved from the original submission and reflects changes based on pre-review questions. However, while there was an attempt made to include more qualitative results e.g. Fig. 2, it is still relatively weak and could benefit from more examples and analysis. Also, why is the attention in Fig. 2 always attending over the full character? Although it is zooming in, shouldn’t it attend to relevant parts of the character? Attending to the full character on a solid background seems a trivial solution where it is then unclear where the large performance gains are coming from. While the paper is much more polished now, it is still lacking in details in some respects, e.g. details of the convolutional feature extractor used that gives large performance gain.  This paper presents an attention based recurrent approach to one-shot learning. It reports quite strong experimental results (surpassing human performance/HBPL) on the Omniglot dataset, which is somewhat surprising because it seems to make use of very standard neural network machinery. The authors also note that other have helped verify the results (did Soumith Chintala reproduce the results?) and do provide source code. After reading this paper, I*m left a little perplexed as to where the big performance improvements are coming from as it seems to share a lot of the same components of previous work. If the author*s could report result from a broader suite of experiments like in previous work (e.g matching networks), it would much more convincing. An ablation study would also help with understanding why this model does so well.  This paper describes a method that estimates the similarity between a set of images by alternatively attend each image with a recurrent manner. The idea of the paper is interesting, which mimic the human*s behavior. However, there are several cons of the paper: 1. The paper is now well written. There are too many *TODO*, *CITE* in the final version of the paper, which indicates that the paper is submitted in a rush or the authors did not take much care about the paper. I think the paper is not suitable to be published with the current version. 2. The missing of the experimental results. The paper mentioned the LFW dataset. However, the paper did not provide the results on LFW dataset. (At least I did not find it in the version of Dec. 13th) 3. The experiments of Omniglot dataset are not sufficient. I suggest that the paper provides some illustrations about how the model the attend two images (e.g. the trajectory of attend).","['Rating:###4: Ok but not good enough - rejection', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###3: Clear rejection']","['Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",4,5,"Strengths: 1. The paper presents an interesting idea of using attention mechanism in recurrent networks to compare images. 2. The paper is well written. 3. The authors provide a good motivation for the paper. Weaknesses: 1. The paper is missing some experimental results. The authors did not provide the results on LFW dataset. 2. The experiments of Omniglot dataset are not sufficient. The authors should provide more experimental results. 3. The paper is missing some details about the convolutional feature extractor. 4. The paper is missing some details about the architecture of the recurrent network. Rating: 7
Confidence: 4"
Generating Long and Diverse Responses with Neural Conversation Models | OpenReview,"Building general-purpose conversation agents is a very challenging task, but necessary on the road toward intelligent agents that can interact with humans in natural language. Neural conversation models -- purely data-driven systems trained end-to-end on dialogue corpora -- have shown great promise recently, yet they often produce short and generic responses. This work presents new training and decoding methods that improve the quality, coherence, and diversity of long responses generated using sequence-to-sequence models. Our approach adds self-attention to the decoder to maintain coherence in longer responses, and we propose a practical approach, called the glimpse-model, for scaling to large datasets. We introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process. We trained on a combined data set of over 2.3B conversation messages mined from the web. In human evaluation studies, our method produces longer responses overall, with a higher proportion rated as acceptable and excellent as length increases, compared to baseline sequence-to-sequence models with explicit length-promotion. A back-off strategy produces better responses overall, in the full spectrum of lengths.","The paper proposes modification to seq2seq model to help it handle the problems when long responses are needed. Though the technical contributions may be of value, the work in my personal opinion is not in the right direction towards helping dialog systems. Essentially we try to generate long responses that sound ``nice* yet are not grounded to any reality, they just need to be related to the question and not suffers from obvious mistakes. Yet, the architectural innovations proposed may be of merit.  The paper is clearly interesting in that it does address important problems (length and diversity of responses) in sequence-to-sequence models. The two ideas put forward (glimpse model and segment-based stochastic decoding) both seem ideas in the right direction. I was however not so sold on the argument that these are particularly suitable for conversations. The results indicate that the ideas do indeed generate longer and also somewhat more sensible target sequences and as such the paper makes progress w.r.t these important problems. So overall I would suggest accepting the paper even though the flavor of the proposed ideas are somewhat *small steps*.  This paper considers the problem of generating long and diverse responses in dialog systems. Two techniques are proposed to the seq-to-seq framework: (1) glimpse model that trains on fixed-length segments of the target side at a time, and (2) a segment-based stochastic decoding technique which injects diversity earlier in the generated responses. The large scale experiments on 2.3B conversation messages are quite impressive. Experiments on human evaluation should also be encouraged. With all these said, I am still not 100% convinced that machine generated long sequence is the right direction for dialog systems. As shown in Figure 3 (a), human evaluation shows that the proposed system is not significantly better than the baselines. I think more analysis and user preference mining should be done in the future to help us understand the nature of this problem.","['Rating:###5: Marginally below acceptance threshold', 'Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",5,3,Strengths: 1. The paper proposes two techniques to improve the generation of long and diverse responses in dialog systems. 2. The experiments are large scale and the results are quite impressive. Weaknesses: 1. The paper does not provide a clear motivation for why these techniques are particularly suitable for dialog systems. 2. The results of human evaluation are not very encouraging. Rating: 7 Confidence: 3
Neural Data Filter for Bootstrapping Stochastic Gradient Descent | OpenReview,"Mini-batch based Stochastic Gradient Descent(SGD) has been widely used to train deep neural networks efficiently. In this paper, we design a general framework to automatically and adaptively select training data for SGD. The framework is based on neural networks and we call it emph{ extbf{N}eural  extbf{D}ata  extbf{F}ilter} ( extbf{NDF}). In Neural Data Filter, the whole training process of the original neural network is monitored and supervised by a deep reinforcement network, which controls whether to filter some data in sequentially arrived mini-batches so as to maximize future accumulative reward (e.g., validation accuracy). The SGD process accompanied with NDF is able to use less data and converge faster while achieving comparable accuracy as the standard SGD trained on the full dataset. Our experiments show that NDF bootstraps SGD training for different neural network models including Multi Layer Perceptron Network and Recurrent Neural Network trained on various types of tasks including image classification and text understanding.","Final review: The writers were very responsive and I agree the reviewer2 that their experimental setup is not wrong after all and increased the score by one. But I still think there is lack of experiments and the results are not conclusive. As a reader I am interested in two things, either getting a new insight and understanding something better, or learn a method for a better performance. This paper falls in the category two, but fails to prove it with more throughout and rigorous experiments. In summary the paper lacks experiments and results are inconclusive and I do not believe the proposed method would be quite useful and hence not a conference level publication. -- The paper proposes to train a policy network along the main network for selecting subset of data during training for achieving faster convergence with less data. Pros: It*s well written and straightforward to follow The algorithm has been explained clearly. Cons: Section 2 mentions that the validation accuracy is used as one of the feature vectors for training the NDF. This invalidates the experiments, as the training procedure is using some data from the validation set. Only one dataset has been tested on. Papers such as this one that claim faster convergence rate should be tested on multiple datasets and network architectures to show consistency of results. Especially larger datasets as the proposed methods is going to use less training data at each iteration, it has to be shown in much larger scaler datasets such as Imagenet. As discussed more in detail in the pre-reviews question, if the paper is claiming faster convergence then it has to compare the learning curves with other baselines such Adam. Plain SGD is very unfair comparison as it is almost never used in practice. And this is regardless of what is the black box optimizer they use. The case could be that Adam alone as black box optimizer works as well or better than Adam as black box + NDF.  This work proposes to augment normal gradient descent algorithms with a *Data Filter*, that acts as a curriculum teacher by selecting which examples the trained target network should see to learn optimally. Such a filter is learned simultaneously to the target network, and trained via Reinforcement Learning algorithms receiving rewards based on the state of training with respect to some pseudo-validation set. Stylistic comment, please use the more common style of *(Author, year)* rather than *Author (year)* when the Author is *not* referred to or used in the sentence. E.g. *and its variants such as Adagrad Duchi et al. (2011)* should be *such as Adagrad (Duchi et al., 2011)*, and *proposed in Andrychowicz et al. (2016),* should remain so. I think the paragraph containing *What we need to do is, after seeing the mini-batch Dt of M training instances, we dynamically determine which instances in Dt are used for training and which are filtered.* should be clarified. What is *seeing*? That is, you should mention explicitly that you do the forward-pass first, then compute features from that, and then decide for which examples to perform the backwards pass. There are a few choices in this work which I do not understand: Why wait until the end of the episode to update your reinforce policy (algorithm 2), but train your actor critic at each step (algorithm 3)? You say REINFORCE has high variance, which is true, but does not mean it cannot be trained at each step (unless you have some experiments that suggest otherwise, and if so they should be included or mentionned in the paper). Similarly, why not train REINFORCE with the same reward as your Actor-Critic model? And vice-versa? You claim several times that a limitation of REINFORCE is that you need to wait for the episode to be over, but considering your data is i.i.d., you can make your episode be anything from a single training step, one D_t, to the whole multi-epoch training procedure. I have a few qualms with the experimental setting: - is Figure 2 obtained from a single (i.e. one per setup) experiment? From different initial weights? If so, there is no proper way of knowing whether results are chance or not! This is a serious concern for me. - with most state-of-the-art work using optimization methods such as Adam and RMSProp, is it surprising that they were not experimented with. - it is not clear what the learning rates are; how fast should the RL part adapt to the SL part? Its not clear that this was experimented with at all. - the environment, i.e. the target network being trained, is not stationnary at all. It would have been interesting to measure how much the policy changes as a function of time. Figure 3, could both be the result of the policy adapting, or of the policy remaining fixed and the features changing (which could indicate a failure of the policy to adapt). - in fact it is not really adressed in the paper that the environment is non-stationary, given the current setup, the distribution of features will change as the target network progresses. This has an impact on optimization. - how is the *pseudo-validation* data, target to the policy, chosen? It should be a subset of the training data. The second paragraph of section 3.2 suggests something of the sort, but then your algorithms suggest that the same data is used to train both the policies and the networks, so I am unsure of which is what. Overall the idea is novel and interesting, the paper is well written for the most part, but the methodology has some flaws. Clearer explanations and either more justification of the experimental choices or more experiments are needed to make this paper complete. Unless the authors convince me otherwise, I think it would be worth waiting for more experiments and submitting a very strong paper rather than presenting this (potentially powerful!) idea with weak results.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",4,5,"Strengths: 1. The paper proposes a novel idea of using a neural network to filter the training data during training. 2. The paper is well written and easy to follow. Weaknesses: 1. The paper lacks experiments. The authors claim that the proposed method is faster than SGD, but they only compare it to SGD on a single dataset. 2. The authors do not compare the proposed method to other methods such as Adam. 3. The paper does not discuss the learning rate. 4. The environment is not stationary. 5. The pseudo-validation data is not clearly defined. Rating: 6 Confidence: 3"
Fast Adaptation in Generative Models with Generative Matching Networks | OpenReview,"Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples. Both problems may be addressed by conditional generative models that are trained to adapt the generative distribution to additional input data. So far this idea was explored only under certain limitations such as restricting the input data to be a single object or multiple objects representing the same concept. In this work we develop a new class of deep generative model called generative matching networks which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks and the ideas from meta-learning. By conditioning on the additional input dataset, generative matching networks may instantly learn new concepts that were not available during the training but conform to a similar generative process, without explicit limitations on the number of additional input objects or the number of concepts they represent. Our experiments on the Omniglot dataset demonstrate that generative matching networks can significantly improve predictive performance on the fly as more additional data is available to the model and also adapt the latent space which is beneficial in the context of feature extraction.","This paper presents a meta-learning algorithm which learns to learn generative models from a small set of examples. It’s similar in structure to the matching networks of Vinyals et al. (2016), and is trained in a meta-learning framework where the inputs correspond to datasets. Results are shown on Omniglot in terms of log-likelihoods and in terms of generated samples. The proposed idea seems reasonable, but I’m struggling to understand various aspects of the paper. The exposition is hard to follow, partly because existing methods are described using terminology fairly different from that of the original authors. Most importantly, I can’t tell which aspects are meant to be novel, since there are only a few sentences devoted to matching networks, even though this work builds closely upon them. (I brought this up in my Reviewer Question, and the paper has not been revised to make this clearer.) I’m also confused about the meta-learning setup. One natural formulation for meta-learning of generative models would be that the inputs consist of small datasets X, and the task is to predict the distribution from which X was sampled. But this would imply a uniform weighting of data points, which is different from the proposed method. Based on 3.1, it seems like one additionally has some sort of query q, but it’s not clear what this represents. In terms of experimental validation, there aren’t any comparisons against prior work. This seems necessary, since several other methods have already been proposed which are similar in spirit.  This paper proposes an interesting idea for rapidly adapting generative models in the low data regime. The idea is to use similar techniques that are used in one-shot learning, specifically ideas from matching networks. To that end, the authors propose the generative matching networks model, which is effectively a variational auto-encoder that can be conditioned on an input dataset. Given a query point, the model matches the query point to points in the conditioning set using an attention model in an embedding space (this is similar to matching networks). The results on the Omniglot dataset show that this method is successfully able to rapidly adapt to new input distributions given few examples. I think that the method is very interesting, however the major issue for me with this paper is a lack of clarity. I outline more details below, but overall I found the paper somewhat difficult to follow. There are a lot of details that I feel are scattered throughout, and I did not get a sense after reading this paper that I would be able to implement the method and replicate the results. My suggestion is to consolidate the major implementation details into a single section, and be explicit about the functional form of the different embedding functions and their variants. I was a bit disappointed to see that weak supervision in the form of labels had to be used. How does the method perform in a completely unsupervised setting? This could be an interesting baseline. There is a lack of definition of the different functions. Some basic insight into the functional forms of f, g, phi, sim and R would be nice. Otherwise it is very unclear to me what’s going on. Section 3.2: “only state of the recurrent controller was used for matching”, my reading of this section (after several passes) is that the pseudo-input is used in the place of a regular input. Is this correct? Otherwise, this sentence/section needs more clarification. I noticed upon further reading in section 4.2 that there are two versions of the model: one in which a pseudo input is used, and one in which a pseudo input is not used (the conditional version). What is the difference in functional form between these? That is, how do the formulas for the embeddings f and g change between these settings? “since the result was fully contrastive we did not apply any further binarization” what does it mean for a result to be fully contrastive? For clarity, the figures and table refer to the number of shots, but this is never defined. I assume this is T here. This should be made consistent. Figure 2: why is the value of T only 9 in this case? What does it mean for it to be 0? It is stated earlier that T should go up to 20 (I assume #shot corresponds to T). It also looks like the results continue to improve with an increased number of steps, I would like to see the results for 5 and maybe 6 steps as well. Presumably there will come a point where you get diminishing returns. Table 1: is the VAE a fair baseline? You mention that Ctest affects Pd() in the evaluation. The fact that the VAE does not have an associated Ctest implies that the two models are being evaluated with a different metric. Can the authors clarify this? It’s important that the comparison is apples-to-apples. MNIST is much more common than Omniglot for evaluating generative models. Would it be possible to perform similar experiments on this dataset? That way it can be compared with many more models. Further, why are the negative log-likelihood values monotonically decreasing in the number of shots? That is, is there ever a case where increasing the number of shots can hurt things? What happens at T=30? 40? As a minor grammatical issue, the paper is missing determiners in several sentences. At one point, the model is referred to as “she” instead of “it”. “On figure 3” should be changed to “in figure 3” in the experiments section.  The paper explores a VAE architecture and training procedure that allows to generate new samples of a concept based on several exemplars that are shown to the model. The proposed architecture processes the set of exemplars with a recurrent neural network and aggregation procedure similar to the one used in Matching Networks. The resulting *summary* is used to condition a generative model (a VAE) that produces new samples of the same kind as the exemplars shown. The proposed aggregation and conditioning procedure are better suited to sets of exemplars that come from several classes than simple averaging. Perhaps surprisingly the model generalizes from generation conditioned on samples from 2 classes to generation conditioned on samples from 4 classes. The experiments are conducted on the OMNIGLOT dataset and are quite convincing. An explicit comparison to previous works is lacking, but this is explained in the appendices, and a comparison to architectures similar to previous work is presented.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###7: Good paper, accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",4,4,"Strengths:

1. The paper proposes a novel approach to meta-learning for generative models. The idea of using a recurrent neural network to aggregate the features of multiple exemplars is interesting and worth exploring.
2. The authors provide a clear and concise description of the architecture and training procedure of the proposed model.
3. The experiments are well-designed and provide a good evaluation of the proposed approach.

Weaknesses:

1. The paper lacks clarity in its presentation. The authors use a lot of technical terms and concepts without providing a clear explanation of what they mean. This makes it difficult to understand the paper and follow the authors' arguments.
2. The paper does not provide a clear comparison to previous works. The authors mention that the proposed approach is similar to Matching Networks, but they do not provide a clear comparison to other meta-learning approaches.
3. The paper lacks a clear evaluation of the proposed approach. The authors provide a lot of results, but they do not provide a clear evaluation of the performance of the proposed approach.
4. The paper does not provide a clear explanation of the hyperparameters used in the experiments. The authors mention that the number of shots is a hyperparameter, but they do not provide a clear explanation of how it is chosen.
5. The paper lacks a clear discussion of the limitations of the proposed approach. The authors mention that the proposed approach is not suitable for large datasets, but they do not provide a clear explanation of why this is the case.
6. The"
Compositional Kernel Machines | OpenReview,"Convolutional neural networks (convnets) have achieved impressive results on recent computer vision benchmarks. While they benefit from multiple layers that encode nonlinear decision boundaries and a degree of translation invariance, training convnets is a lengthy procedure fraught with local optima. Alternatively, a kernel method that incorporates the compositionality and symmetry of convnets could learn similar nonlinear concepts yet with easier training and architecture selection. We propose compositional kernel machines (CKMs), which effectively create an exponential number of virtual training instances by composing transformed sub-regions of the original ones. Despite this, CKM discriminant functions can be computed efficiently using ideas from sum-product networks. The ability to compose virtual instances in this way gives CKMs invariance to translations and other symmetries, and combats the curse of dimensionality. Just as support vector machines (SVMs) provided a compelling alternative to multilayer perceptrons when they were introduced, CKMs could become an attractive approach for object recognition and other vision problems. In this paper we define CKMs, explore their properties, and present promising results on NORB datasets. Experiments show that CKMs can outperform SVMs and be competitive with convnets in a number of dimensions, by learning symmetries and compositional concepts from fewer samples without data augmentation.","This paper proposes a new learning model *Compositional Kernel Machines (CKMs)* that extends the classic kernel machines by constructing compositional kernel functions using sum-product networks. This paper considers the convnets as nicely learned nonlinear decision functions and resort their success in classification to their compositional nature. This perspective motivates the design of compositional kernel functions and the sum-product implementation is indeed interesting. I agree the composition is important for convnets, but it is not the whole story of convnets* success. One essential difference between convnets and CKMs is that all the kernels in convnets are learned directly from data while CKMs still build on top of feature descriptors. This, I believe, limits the representation power of CKMs. A recent paper *Deep Convolutional Networks are Hierarchical Kernel Machines* by Anselmi, F. et al. seems to be interesting to the authors. Experiments seem to be preliminary in this paper. It*s good to see promising results of CKMs on small NORB, but it is quite important to show competitive results on recent classification standard benchmarks, such as MNIST, CIFAR10/100 and even Imagenet, in order to establish a novel learning model. In NORB compositions, CKMs seem to be better than convnets at classifying images by their dominant objects. I suspect it is because the use of sparse ORB features. It will be great if this paper could show the accuracy of ORB features with matching kernel SVMs. Some details about this experiment need further clarification, such as what are the high and low probabilities of sampling from each collections and how many images are generated. In NORB Symmetries, CKMs show better performance than convnets with small data, but the convnets seem not converged yet. Could it be possible to show results with larger dataset?  The authors propose a method to efficiently augment an SVM variant with many virtual instances, and show promising preliminary results. The paper was an interesting read, with thoughtful methodology, but has partially unsupported and potentially misleading claims. Pros: - Thoughtful methodology with sensible design choices - Potentially useful for smaller (n > 10,000 training instances, it*s unclear whether the predictive accuracy would be competitive with convnets in that domain. Moreover, the idea of doing 10^6 operations simply for creating virtual instances on 10^4 training points and 100 test points is still somewhat daunting. What if we had 10^6 training instances and 10^5 testing instances? Because scalability (in the number of training instances) is one of the biggest drawbacks of using SVMs (e.g. with Gaussian kernels) on modern datasets, the scalability claims in this paper need to be significantly expanded and clarified. On a related note, the suggestion that convnets grow quadratically in computation with additional training instances in the introduction needs to be augmented with more detail, and is potentially misleading. Convnets typically scale linearly with additional training data. In general, the paper suffers greatly from a lack of clarity and issues of presentation. As above, the full story is not presented, with critical details often missing. Moreover, it would strengthen the paper to remove broad claims such as *Just as support vector machines (SVMs) eclipsed multilayer perceptrons in the 1990s, CKMs could become a compelling alternative to convnets with reduced training time and sample complexity*, suggesting that CKMs could eclipse convolutional neural networks, and instead provide more helpful and precise information. Convnets are multilayer perceptrons used in the 1990s (as well as now) and they are not eclipsed by SVMs -- they have different relative advantages. And based on the information presented, broadly advertising scalability over convnets is misleading. Can CKMs scale to datasets with millions of training and test instances? It seems as if the scalability advantages are limited to smaller datasets, and asymptotic scalability could be much worse in general. And even if CKMs could scale to such datasets would they have as good predictive accuracy as convnets on those applications? Being specific and with full disclosure about the precise strengths and limitations of the work would greatly improve this paper. CKMs may be more robust to adversarial examples than standard convnets, due to the virtual instances. But there are many approaches to make deep nets more robust to adversarial examples. It would be useful to consider and compare to these. The ideas behind CKMs also are not inherently specific to kernel methods. Have you considered looking at using virtual instances in a similar way with deep networks? A full exploration might be its own paper, but the idea is worth at least brief discussion in the text. A big advantage of SVMs (with Gaussian kernels) over deep neural nets is that one can achieve quite good performance with very little human intervention (design choices). However, CKMs seem to require extensive intervention, in terms of architecture (as with a neural network), and in insuring that the virtual instances are created in a plausible manner for the particular application at hand. It*s very unclear in general how one would want to create sensible virtual instances and this topic deserves further consideration. Moreover, unlike SVMs (with for example Gaussian or linear kernels) or standard convolutional networks, which are quite general models, CKMs as applied in this paper seem more like SVMs (or kernel methods) which have been highly tailored to a particular application -- in this case, the NORB dataset. There is certainly nothing wrong with the tailored approach, but it would help to be clear and detailed about where the presented ideas can be applied out of the box, or how one would go about making the relevant design choices for a range of different problems. And indeed, it would be good to avoid the potentially misleading suggestions early in the paper that the proposed method is a general alternative to convnets. The experiments give some insights into the advantages of the proposed approach, but are very limited. To get a sense of the properties --the strengths and limitations -- of the proposed method, one needs a greater range of datasets with a much larger range of training and test sizes. The comparisons are also quite limited: why not an SVM with a Gaussian kernel? What about an SVM using convnet features from the dataset at hand (light blue curve in figure 3) -- it should do at least as well as the light blue curve. There are also other works that could be considered which combine some of the advantages of kernel methods with deep networks. Also the claim that the approach helps with the curse of dimensionality is sensible but not particularly explored. It also seems the curse of dimensionality could affect the scalability of creating a useful set of virtual instances. And it*s unclear how CKM would work without any ORB features. Even if the method can (be adapted to) scale to n >> 10000, it*s unclear whether it will be more useful than convnets in that domain. Indeed, in the experiments here, convnets essentially match CKMs in performance after 12,000 examples, and would probably perform better than CKMs on larger datasets. We can only speculate because the experiments don*t consider larger problems. The methodology largely takes inspiration from sum product networks, but its application in the context of a kernel approach is reasonably original, and worthy of exploration. It*s reasonable to expect the approach to be significant, but its significance is not demonstrated. The quality is high in the sense that the methods and insights are thoughtful, but suffers from broad claims and a lack of full and precise detail. In short: I like the paper, but it needs more specific details, and a full disclosure of where the method should be most applicable, and its precise advantages and limitations. Code would be helpful for reproducibility.  Thank you for an interesting read. The ideas presented have a good basis of being true, but the experiments are rather too simple. It would be interesting to see more empirical evidence. Pros - The approach seems to decrease the training time, which is of prime importance in deep learning. Although, that comes at a price of slightly more complex model. - There is a grounded theory for sum-product functions which is basis for the compositional architecture described in the paper. Theoretically, any semiring and kernel could be used for the model which decreases need for handcrafting the structure of the model, which is a big problem in existing convolutional neural networks. Cons - The experiments are on very simple dataset NORB. Although, it is great to understand a model*s dynamics on a simpler dataset, some analysis on complex datasets are important to act as empirical evidence. The compositional kernel approach is compared to convolutional neural networks, hence it is only fair to compare said results on large datasets such as Imagenet. Minor - Section 3.4 claims that CKMs model symmetries of objects. It felt that ample justification was not provided for this claim  This paper proposes a new learning framework called *compositional kernel machines* (CKM). It combines two ideas: kernel methods and sum-product network (SPN). CKM first defines leaf kernels on elements of the query and training examples, then it defines kernel recursively (similar to sum-product network). This paper has shown that the evaluation CKM can be done efficiently using the same tricks in SPN. Positive: I think the idea in this paper is interesting. Instance-based learning methods (such as SVM with kernels) have been successful in the past, but have been replaced by deep learning methods (e.g. convnet) in the past few years. This paper investigate an unexplored area of how to combine the ideas from kernel methods and deep networks (SPN in this case). Negative: Although the idea of this paper is interesting, this paper is clearly very preliminary. In its current form, I simply do not see any advantage of the proposed framework over convnet. I will elaborate below. 1) One of the most important claims of this paper is that CKM is faster to learn than convnet. I am not clear why that is the case. Both CKM and convnet use gradient descent during learning, why would CKM be faster? Also during inference, the running time of convnet only depends on its network structure. But for CKM, in addition to the network structure, it also depends on the size of training set. From this perspective, it does not seem CKM is very scalable when the training size is big. That is probably why this paper has to use all kinds of specialized data structures and tricks (even on a fairly simple dataset like NORB) 2) I am having a hard time understanding what the leaf kernel is capturing. For example, if the *elements* correspond to raw pixel intensities, a leaf kernel essentially compares the intensity value of a pixel in the query image with that in a training image. But in this case, wouldn*t you end up comparing a lot of background pixels across these two images (which does not help with recognition)? I think it probably helps to explain Sec 3.1 a bit better. In its current form, this part is very dense and hard to understand. 3) It is also not entirely clear to me how you would design the architecture of the sum-product function. The example is Sec 3.1 seems to be fairly arbitrary. 4) The experiment section is probably the weakest part. NORB is a very small and toy-ish dataset by today*s standard. Even on this small dataset, the proposed method is only slighly better than SVM (it is not clear whether *SVM* in Table 2 is linear SVM or kernel SVM. If it is linear SVM, I suspect the performance of *SVM* will be even higher when you use kernel SVM), and far worse than convnet. The proposed method only shows improvement over convnet on synthetic datasets (NORB compositions, NORM symmetries) Overall, I think this paper has some interesting ideas. But in its current form, it is a bit too preliminary and more work is needed to show its advantage. Having said that, I acknowledge that in the machine learning history, many important ideas seem pre-mature when they were first proposed, and it took time for these ideas to develop.","['Rating:###5: Marginally below acceptance threshold', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",5,3,"Strengths:
1. The paper proposes a new learning framework called *compositional kernel machines* (CKM). It combines two ideas: kernel methods and sum-product network (SPN). CKM first defines leaf kernels on elements of the query and training examples, then it defines kernel recursively (similar to sum-product network). This paper has shown that the evaluation CKM can be done efficiently using the same tricks in SPN.
2. The authors are able to provide a theoretical basis for the sum-product function, which is a good starting point for the proposed method.

Weaknesses:
1. The experiment section is probably the weakest part. NORB is a very small and toy-ish dataset by today*s standard. Even on this small dataset, the proposed method is only slightly better than SVM (it is not clear whether *SVM* in Table 2 is linear SVM or kernel SVM. If it is linear SVM, I suspect the performance of *SVM* will be even higher when you use kernel SVM), and far worse than convnet. The proposed method only shows improvement over convnet on synthetic datasets (NORB compositions, NORM symmetries)
2. The paper is very dense and hard to understand. The example in Sec 3.1 seems to be fairly arbitrary. It is also not entirely clear to me how you would design the architecture of the sum-product function.
3. The paper has some interesting ideas. But in its current form, it is a"
Two Methods for Wild Variational Inference | OpenReview,"Variational inference provides a powerful tool for approximate probabilistic inference on complex, structured models. Typical variational inference methods, however, require to use inference networks with computationally tractable probability density functions. This largely limits the design and implementation of variational inference methods. We consider wild variational inference methods that do not require tractable density functions on the inference networks, and hence can be applied in more challenging cases. As an example of application, we treat stochastic gradient Langevin dynamics (SGLD) as an inference network, and use our methods to automatically adjust the step sizes of SGLD to maximize its convergence speed, significantly outperforming the hand-designed step size schemes.","The paper proposes two methods for what is called wild variational inference. The goal is to obtain samples from the variational approximate distribution q without requiring to evaluate the density q(z) by which it becomes possible to consider more flexible family of distributions. The authors apply the proposed method to the problem of optimizing the hyperparamter of the SGLD sampler. The experiments are performed on a 1-d mixture of gaussian distribution and Bayesian logistic regression tasks. The key contribution seems to connect the previous findings in SVGD and KSD to the concept of inference networks, and to use them for hyperparameter optimization of SGLD. This can not only be considered as a rather simple connection/extension, but also the toyish experiments are not enough to convince readers on the significance of the proposed model. Particularly, I*m wondering how the particle based methods can deal with the multimodality (not the simple 1d gaussian mixture case) in general. Also, the method seems still to require to evaluate the true gradient of the target distribution (e.g., the posterior distribution) for each z ~ q. This seems to be a computational problem for large dataset settings. In the experiments, the authors compare the methods for the same number of update steps. But, considering the light computation of SGLD per update, I think SGLD can make much more updates per unit time than the proposed methods, particularly for large datasets. The Bayesian logistic regression on 54 dimensions seems also a quite simple experiment, considering that its posterior is close to a Gaussian distribution. Also, including Hamiltonian Monte Carlo (HMC) with automatic hyperparameter tuning mechanism (like, no u-turn sampler) would be interesting. The paper is written very unclearly. Especially, it is not clear what is the exact contributions of the paper compared to the other previous works including the authors* works. The main message is quite simple but most of the pages are spent to explain previous works. Overall, I*d like to suggest to have more significant high-dimension, large scale experiments, and to improve the writing.  The authors propose two variational methods based on the theme of posterior approximations which may not have a tractable density. The first is from another ICLR submission on *amortized SVGD* (Wang and Liu, 2016), where here the innovation is in using SGLD as the inference network. The second is from a NIPS paper (Ranganath et al., 2016) on minimizing the Stein divergence with a parametric approximating family, where here the innovation is in defining their test functions to be an RKHS, obtaining an analytic solution to the inner optimization problem. The methodology is incremental. Everything up to Section 3.2 is essentially motivation, background, or related work. The notion of a *wild variational approximation* was already defined in Ranganath et al. (2016), termed a *variational program*. It would be useful for the authors to comment on the difference, if any. Section 3.2 is at first interesting because it analytically solves the maximum problem that is faced in Ranganath et al. (2016). However, this requires use of a kernel which will certainly not scale in high dimensions, so it is then equivalent in practice to having chosen a very simple test function family. To properly scale to high dimensions would require a deeper kernel and also learning its parameters; this is not any easier than parameterizing the test function family as a neural network to begin with, which Ranganath et al. (2016) do. Section 4 introduces a Langevin inference network, which essentially chooses the variational approximation as an evolving sequence of Markov transition operators as in Salimans et al. (2015). I had trouble understanding this for a while because I could not understand what they mean by inference network. None of it is amortized in the usual inference network sense, which is that the parameters are given by the output of a neural network. Here, the authors simple define global parameters of the SGLD chain which are used across all the latent variables (which is strictly worse?). (What then makes it an *inference network*?) Is this not the variational approximation used in Salimans et al. (2015), but using a different objective to train it? The experiments are limited, on a toy mixture of Gaussians posterior and Bayesian logistic regression. None of this addresses the problems one might suspect on high-dimensional and real data, such as the lack of scalability for the kernel, the comparison to Salimans et al. (2015) for the Langevin variational approximation, and any note of runtime or difficulty of training. Minor comments + It*s not clear if the authors understood previous work on expressive variational families or inference networks. For example, they argue Rezende & Mohamed, 2015b; Tran et al., 2015; Ranganath et al., 2015 require handcrafted inference networks. However, all of them assume use of any neural network for amortized inference. None of them even require an inference network. Perhaps the authors mean handcrafted posterior approximations, which to some extent is true; however, the three mentioned are all algorithmic in nature: in Rezende & Mohamed (2015), the main decision choice is the flow length; Tran et al. (2015), the size of the variational data; Ranganath et al. (2015), the flow length on the auxiliary variable space. Each works well on different problems, but this is also true of variational objectives which admit intractable q (as the latter two consider, as does Salimans et al. (2015)). The paper*s motivation could be better explained, and perhaps the authors could be clearer on what they mean by inference network. + I also recommend the authors not term a variational inference method based on the class of approximating family. While black box variational inference in Ranganath et al. (2014) assumes a mean-field family, the term itself has been used in the literature to mean any variational method that imposes few constraints on the model class.  The authors propose methods for wild variational inference, in which the variational approximating distribution may not have a directly accessible density function. Their approach is based on the Stain*s operator, which acts on a given function and returns a zero mean function with respect to a given density function which may not be normalized. Quality: The derviations seem to be technically sound. However, my impression is that the authors are not very careful and honest at evaluating both the strengths and weaknesses of the proposed work. How does the method perform in cases in which the distribution to be approximated is high dimensional? The logistic regression problem considered only has 54 dimensions. How would this method perform in a neural network in which the number of weights is goint to be way much larger? The logistic regression model is rather simple and its posterior will be likely to be close to Gaussian. How would the method perform in more complicated posteriors such as the ones of Bayesia neural networks? Clarity: The paper is not clearly written. I found it very really hard to follow and not focused. The authors describe way too many methods: 1) Stein*s variational gradient descent (SVGD), 2) Amortized SVGD, 3) Kernelized Stein discrepancy (KSD), 4) Lavengin inference network, not to mention the introduction to Stein*s discrepancy. I found very difficult to indentify the clear contributions of the paper with so many different techniques. Originality: It is not clear how original the proposed contributions are. The first of the proposed methods is also discussed in Wang, Dilin and Liu, Qiang. Learning to draw samples: With application to amortized mle for generative adversarial learning. Submitted to ICLR 2017, 2016 How does this work differ from that one? Significance: It is very hard to evaluate the importance of proposed methods. The authors only report results on a 1d toy problem with a mixture of Gaussians and on a logistic regression model with dimension 54. In both cases the distributions to be approximated are very simple and of low dimension. In the regression case the posterior is also likely to be close to Gaussian and therefore not clear what advances the proposed method would provide with respect to other more simple approaches. The authors do not compare with simple variational approaches based on Gaussian approximations.","['Rating:###3: Clear rejection', 'Rating:###3: Clear rejection', 'Rating:###3: Clear rejection']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",3,4,"Strengths: 1. The paper presents two methods for wild variational inference, which is a novel and interesting topic. 2. The authors present a clear and simple method for training the inference network, which is a good contribution. Weaknesses: 1. The paper is not clearly written. I found it very really hard to follow and not focused. The authors describe way too many methods: 1) Stein*s variational gradient descent (SVGD), 2) Amortized SVGD, 3) Kernelized Stein discrepancy (KSD), 4) Lavengin inference network, not to mention the introduction to Stein*s discrepancy. I found very difficult to indentify the clear contributions of the paper with so many different techniques. 2. The paper only reports results on a 1d toy problem with a mixture of Gaussians and on a logistic regression model with dimension 54. In both cases the distributions to be approximated are very simple and of low dimension. In the regression case the posterior is also likely to be close to Gaussian and therefore not clear what advances the proposed method would provide with respect to other more simple approaches. 3. The authors do not compare with simple variational approaches based on Gaussian approximations. 4. The paper does not discuss the computational complexity of the proposed methods. 5. The authors do not discuss the theoretical properties of the proposed methods. Rating: 6
Confidence: 3"
Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning | OpenReview,"While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (*PredNet*) architecture that is inspired by the concept of *predictive coding* from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. These results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.","An interesting architecture that accumulates and continuously corrects mistakes as you see more and more of a video sequence. Clarity: The video you generated seems very helpful towards understanding the information flow in your network, it would be nice to link to it from the paper. *Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).* It is not clear how different are the train and test sequences at the moment, since standard benchmarks do not really exist for video prediction and each author picks his/her favorite. Underperforming Finn et al 206 at the H3.6m Walking videos is a bit disappointing.  Learning about the physical structure and semantics of the world from video (without supervision) is a very hot area in computer vision and machine learning. In this paper, the authors investigate how the prediction of future image frames (inherently unsupervised) can help to deduce object/s structure and it*s properties (in this case single object pose, category, and steering angle, (after a supervised linear readout step)) I enjoyed reading this paper, it is clear, interesting and proposes an original network architecture (PredNet) for video frame prediction that has produced promising results on both synthetic and natural images. Moreover, the extensive experimental evaluation and analysis the authors provide puts it on solid ground to which others can compare. The weaknesses: - the link to predictive coding should be better explained in the paper if it is to be used as a motivation for the prednet model. - any idea that the proposed method is learning an implicit `model* of the `objects* that make up the `scene* is vague and far fetched, but it sounds great. Minor comment: Next to the number of labeled training examples (Fig.5), it would be interesting to see how much unsupervised training data was used to train your representations.  Paper Summary This paper proposes an unsupervised learning model in which the network predicts what its state would look like at the next time step (at input layer and potentially other layers). When these states are observed, an error signal is computed by comparing the predictions and the observations. This error signal is fed back into the model. The authors show that this model is able to make good predictions on a toy dataset of rotating 3D faces as well as on natural videos. They also show that these features help perform supervised tasks. Strengths - The model is an interesting embodiment of the idea of predictive coding implemented using a end-to-end backpropable recurrent neural network architecture. - The idea of feeding forward an error signal is perhaps not used as widely as it could be, and this work shows a compelling example of using it. - Strong empirical results and relevant comparisons show that the model works well. - The authors present a detailed ablative analysis of the proposed model. Weaknesses - The model (esp. in Fig 1) is presented as a generalized predictive model where next step predictions are made at each layer. However, as discovered by running the experiments, only the predictions at the input layer are the ones that actually matter and the optimal choice seems to be to turn off the error signal from the higher layers. While the authors intend to address this in future work, I think this point merits some more discussion in the current work, given the way this model is presented. - The network currently lacks stochasticity and does not model the future as a multimodal distribution (However, this is mentioned as potential future work). Quality The experiments are well-designed and a detailed analysis is provided in the appendix. Clarity The paper is well-written and easy to follow. Originality Some deep models have previously been proposed that use predictive coding. However, the proposed model is most probably novel in the way it feds back the error signal and implements the entire model as a single differentiable network. Significance This paper will be of wide interest to the growing set of researchers working in unsupervised learning of time series. This helps draw attention to predictive coding as an important learning paradigm. Overall Good paper with detailed and well-designed experiments. The idea of feeding forward the error signal is not being used as much as it could be in our community. This work helps to draw the community*s attention to this idea.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###8: Top 50% of accepted papers, clear accept']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",6,3,Strengths: 1. The paper presents a novel approach to unsupervised learning of video sequences. 2. The authors provide a clear and well-structured presentation of the model and its components. 3. The experiments are well-designed and provide a good evaluation of the model. 4. The paper is well-written and easy to follow. 5. The authors provide a good discussion of the potential applications of the model. Weaknesses: 1. The paper is not very clear about the motivation behind the model. 2. The model is not very well explained. 3. The experiments are not very well explained. 4. The paper does not provide a good discussion of the potential applications of the model. 5. The paper does not provide a good discussion of the potential limitations of the model. Rating: 6 Confidence: 3
Optimization as a Model for Few-Shot Learning | OpenReview,"Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning.","This work presents an LSTM based meta-learning framework to learn the optimization algorithm of a another learning algorithm (here a NN). The paper is globally well written and the presentation of the main material is clear. The crux of the paper: drawing the parallel between Robbins Monroe update rule and the LSTM update rule and exploit it to satisfy the two main desiderata of few shot learning (1- quick acquisition of new knowledge, 2- slower extraction of general transferable knowledge) is intriguing. Several tricks re-used from (Andrychowicz et al. 2016) such as parameter sharing and normalization, and novel design choices (specific implementation of batch normalization) are well motivated. The experiments are convincing. This is a strong paper. My only concerns/questions are the following: 1. Can it be redundant to use the loss, gradient and parameters as input to the meta-learner? Did you do ablative studies to make sure simpler combinations are not enough. 2. It would be great if other architectural components of the network can be learned in a similar fashion (number of neurons, type of units, etc.). Do you have an opinion about this? 3. The related work section (mainly focused on meta learning) is a bit shallow. Meta-learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using LSTMs: - Samy Bengio PhD thesis (1989) is all about this ;-) - Use of genetic programming for the search of a new learning rule for neural networks (S. Bengio, Y. Bengio, and J. Cloutier. 1994) - I am convince Schmidhuber has done something, make sure you find it and update related work section. Overall, I like the paper. I believe the discussed material is relevant to a wide audience at ICLR.  This paper describes a new approach to meta learning by interpreting the SGD update rule as gated recurrent model with trainable parameters. The idea is original and important for research related to transfer learning. The paper has a clear structure, but clarity could be improved at some points. Pros: - An interesting and feasible approach to meta-learning - Competitive results and proper comparison to state-of-the-art - Good recommendations for practical systems Cons: - The analogy would be closer to GRUs than LSTMs - The description of the data separation in meta sets is hard to follow and could be visualized - The experimental evaluation is only partly satisfying, especially the effect of the parameters of i_t and f_t would be of interest - Fig 2 doesn*t have much value Remarks: - Small typo in 3.2: *This means each coordinate has it* -> its > We plan on releasing the code used in our evaluation experiments. This would certainly be a major plus.  In light of the authors* responsiveness and the updates to the manuscript -- in particular to clarify the meta-learning task -- I am updating my score to an 8. ----- This manuscript proposes to tackle few-shot learning with neural networks by leveraging meta-learning, a classic idea that has seen a renaissance in the last 12 months. The authors formulate few-shot learning as a sequential meta-learning problem: each *example* includes a sequence of batches of *training* pairs, followed by a final *test* batch. The inputs at each *step* include the outputs of a *base learner* (e.g., training loss and gradients), as well as the base learner*s current state (parameters). The paper applies an LSTM to this meta-learning problem, using the inner memory cells in the *second* layer to directly model the updated parameters of the base learner. In doing this, they note similarities between the respective update rules of LSTM memory cells and gradient descent. Updates to the LSTM meta-learner are computed based on the base learner*s prediction loss for the final *test* batch. The authors make several simplifying assumptions, such as sharing weights across all second layer cells (analogous to using the same learning rate for all parameters). The paper recreates the Mini-ImageNet data set proposed in Vinyals et al 2016, and shows that the meta-learner LSTM is competitive with the current state-of-the-art (Matchin Networks, Vinyals 2016) on 1- and 5-shot learning. Strengths: - It is intriguing -- and in hindsight, natural -- to cast the few-shot learning problem as a sequential (meta-)learning problem. While the authors did not originate the general idea of persisting learning across a series of learning problems, I think it is fair to say that they have advanced the state of the art, though I cannot confidently assert its novelty as I am not deeply familiar with recent work on meta-learning. - The proposed approach is competitive with and outperforms Vinyals 2016 in 1-shot and 5-shot Mini-ImageNet experiments. - The base learner in this setting (simple ConvNet classifier) is quite different from the nearest-neighbor-on-top-of-learned-embedding approach used in Vinyals 2016. It is always exciting when state-of-the-art results can be reported using very different approaches, rather than incremental follow-up work. - As far as I know, the insight about the relationship between the memory cell and gradient descent updates is novel here. It is interesting regardless. - The paper offers several practical insights about how to design and train an LSTM meta-learner, which should make it easier for others to replicate this work and apply these ideas to new problems. These include proper initialization, weight sharing across coordinates, and the importance of normalizing/rescaling the loss, gradient, and parameter inputs. Some of the insights have been previously described (the importance of simulating test conditions during meta-training; assuming independence between meta-learner and base learner parameters when taking gradients with respect to the meta-learner parameters), but the discussion here is useful nonetheless. Weaknesses: - The writing is at times quite opaque. While it describes very interesting work, I would not call the paper an enjoyable read. It took me multiple passes (as well as consulting related work) to understand the general learning problem. The task description in Section 2 (Page 2) is very abstract and uses notation and language that is not common outside of this sub-area. The paper could benefit from a brief concrete example (based on MNIST is fine), perhaps paired with a diagram illustrating a sequence of few-shot learning tasks. This would definitely make it accessible to a wider audience. - Following up on that note, the precise nature of the N-class, few-shot learning problem here is unclear to me. Specifically, the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing. Does this mean that only 64/100 classes are observed through meta-training? Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training? If it*s the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training? 64 (only those observed in training) or 100 (of which 36 are never observed)? Many other details like these are unclear (see question). - The plots in Figure 2 are pretty uninformative in and of themselves, and the discussion section offers very little insight around them. This is an interesting paper with convincing results. It seems like a fairly clear accept, but the presentation of the ideas and work therein could be improved. I will definitely raise my score if the writing is improved.","['Rating:###9: Top 15% of accepted papers, strong accept', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###8: Top 50% of accepted papers, clear accept']","['Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",9,5,"Strengths: 
1. The paper presents a novel approach to few-shot learning by casting the problem as a sequential meta-learning problem. The idea is original and important for research related to transfer learning. 
2. The paper has a clear structure, and the main idea is well presented. 
3. The authors provide a clear and well-motivated explanation of the meta-learning task. 
4. The paper is well-written and easy to follow. 
5. The authors provide a clear and well-motivated explanation of the meta-learning task. 

Weaknesses: 
1. The paper does not provide a clear and well-motivated explanation of the meta-learning task. 
2. The paper does not provide a clear and well-motivated explanation of the meta-learning task. 
3. The paper does not provide a clear and well-motivated explanation of the meta-learning task. 
4. The paper does not provide a clear and well-motivated explanation of the meta-learning task. 
5. The paper does not provide a clear and well-motivated explanation of the meta-learning task. 

Rating: 7
Confidence: 3"
Exploring LOTS in Deep Neural Networks | OpenReview,"Deep neural networks have recently demonstrated excellent performance on various tasks. Despite recent advances, our understanding of these learning models is still incomplete, at least, as their unexpected vulnerability to imperceptibly small, non-random perturbations revealed. The existence of these so-called adversarial examples presents a serious problem of the application of vulnerable machine learning models. In this paper, we introduce the layerwise origin-target synthesis (LOTS) that can serve multiple purposes. First, we can use it as a visualization technique that gives us insights into the function of any intermediate feature layer by showing the notion of a particular input in deep neural networks. Second, our approach can be applied to assess the invariance of the learned features captured at any layer with respect to the class of the particular input. Finally, we can also utilize LOTS as a general way of producing a vast amount of diverse adversarial examples that can be used for training to further improve the robustness of machine learning models and their performance as well.","This paper proposes the Layerwise Origin Target Synthesis (LOTS) method, which entails computing a difference in representation at a given layer in a neural network and then projecting that difference back to input space using backprop. Two types of differences are explored: linear scalings of a single input’s representation and difference vectors between representations of two inputs, where the inputs are of different classes. In the former case, the LOTS method is used as a visualization of the representation of a specific input example, showing what it would mean, in input space, for the feature representation to be supressed or magnified. While it’s an interesting computation to perform, the value of the visualizations is not very clear. In the latter case, LOTS is used to generate adversarial examples, moving from an origin image just far enough toward a target image to cause the classification to flip. As expected, the changes required are smaller when LOTS targets a higher layer (in the limit of targetting the last layer, results similar to the original adversarial image results would be obtained). The paper is an interesting basic exploration and would probably be a great workshop paper. However, the results are probably not quite compelling enough to warrant a full ICLR paper. A few suggestions for improvement: - Several times it is claimed that LOTS can be used as a method for mining for diverse adversarial examples that could be used in training classifiers more robust to adversarial perturbation. But this simple experiment of training on LOTS generated examples isn’t tried. Showing whether the LOTS method outperforms, say, FGS would go a long way toward making a strong paper. - How many layers are in the networks used in the paper, and what is their internal structure? This isn’t stated anywhere. I was left wondering whether, say, in Fig 2 the CONV2_1 layer was immediately after the CONV1_1 layer and whether the FC8 layer was the last layer in the network. - In Fig 1, 2, 3, and 4, results of the application of LOTS are shown for many intermediate layers but miss for some reason applying it to the input (data) layer and the output/classification (softmax) layer. Showing the full range of possible results would reinforce the interpreatation (for example, in Fig 3, are even larger perturbations necessary in pixel space vs CONV1 space? And does operating directly in softmax space result in smaller perturbations than IP2?) - The PASS score is mentioned a couple times but never explained at all. E.g. Fig 1 makes use of it but does not specify such basics as whether higher or lower PASS scores are associated with more or less severe perturbations. A basic explanation would be great. - 4.2 states “In summary, the visualized internal feature representations of the origin suggest that lower convolutional layers of the VGG Face model have managed to learn and capture features that provide semantically meaningful and interpretable representations to human observers.” I don’t see that this follows from any results. If this is an important claim to the paper, it should be backed up by additional arguments or results. 1/19/17 UPDATE AFTER REBUTTAL: Given that experiments were added to the latest version of the paper, I*m increasing my review from 5 -> 6. I think the paper is now just on the accept side of the threshold.  The paper presents a new exciting layerwise origin-target synthesis method both for generating a large number of diverse adversarials as well as for understanding the robustness of various layers. The methodology is then used to visualize the amount of perturbation necessary for producing a change for higher level features. The approach to match the features of another unrelated image is interesting and it goes beyond producing adversarials for classification. It can also generate adversarials for face-recognition and other models where the result is matched with some instance from a database. Pro: The presented approach is definitely sound, interesting and original. Con: The analyses presented in this paper are relatively shallow and don*t touch the most obvious questions. There is not much experimental quantitative evidence for the efficacy of this method compared with other approaches to produce adversarials. The visualization is not very exciting and it is hard to any draw any meaningful conclusions from them. It would definitely improve the paper if it would present some interesting conclusions based on the new ideas.  This paper presents a relatively novel way to visualize the features / hidden units of a neural network and generate adversarial examples. The idea is to do gradient descent in the pixel space, from a given hidden unit in any layer. This can either be done by choosing a pair of images and using the difference in activations of the unit as the thing to do gradient descent over or just the activation itself of the unit for a given image. In general this method seems intriguing, here are some comments: It’s not clear that some of the statements at the beginning of Sec 4.1 are actually true, re: positive/negative signs and how that changes (or does not change) the class. Mathematically, I don’t see why that would be the case? Moreover the contradictory evidence from MNIST vs. faces supports my intuition. The authors use the PASS score through the paper, but only given an intuition + citation for it. I think it’s worth explaining what it actually does, in a sentence or two. The PASS score seems to have some, but not complete, correlation with L_2, L_{infty} or visual estimation of how “good” the adversarial examples are. I am not sure what the take-home message from all these numbers is. “In general, LOTS cannot produce high quality adversarial examples at the lower layers” (sec 5.2) seems false for MNIST, no? I would have liked this work to include more quantitative results (e.g., extract adversarial examples at different layers, add them to the training set, train networks, compare on test set), in addition to the visualizations present. That to me is the main drawback of the paper, in addition to basically no comparisons with other methods (it’s hard to judge the merits of this work in vacuum). ----- EDIT after rebuttal: thanks to the authors for addressing the experimental validation concerns. I think this makes the paper more interesting, so revising my score accordingly.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",6,4,"Strengths:
1. The paper presents a novel approach to visualize the internal representations of a neural network. The method is based on gradient descent in the pixel space from a given hidden unit in any layer. This is a good starting point for understanding the behavior of the network.
2. The authors also propose a method to generate adversarial examples by perturbing the input to a hidden unit. This is an interesting application of the visualization method.
Weaknesses:
1. The paper lacks quantitative results. The authors only present visualizations of the internal representations and do not provide any quantitative analysis of the results. This makes it difficult to understand the significance of the findings.
2. The paper does not provide any comparison with other methods for generating adversarial examples. This makes it difficult to evaluate the effectiveness of the proposed method.
3. The paper lacks a clear explanation of the PASS score. The authors mention it in the paper but do not provide any explanation of what it represents or how it is calculated.
4. The paper does not provide any experimental results for the adversarial example generation method. This makes it difficult to evaluate the effectiveness of the proposed method.
Rating: 6
Confidence: 3"
Why Deep Neural Networks for Function Approximation? | OpenReview,"Recently there has been much interest in understanding why deep neural networks are preferred to shallow networks. We show that, for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. First, we consider univariate functions on a bounded interval and require a neural network to achieve an approximation error of uniformly over the interval. We show that shallow networks (i.e., networks whose depth does not depend on ) require neurons while deep networks (i.e., networks whose depth grows with ) require neurons. We then extend these results to certain classes of important multivariate functions. Our results are derived for neural networks which use a combination of rectifier linear units (ReLUs) and binary step units, two of the most popular type of activation functions. Our analysis builds on a simple observation: the multiplication of two bits can be represented by a ReLU.","SUMMARY This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible with much less units when using a network with one more hidden layer. PROS The paper presents an interesting combination of tools and arrives at a nice result on the exponential superiority of depth. CONS The main result appears to address only strongly convex univariate functions. SPECIFIC COMMENTS - Thanks for the comments on L. Still it would be a good idea to clarify this point as far as possible in the main part. Also, I would suggest to advertise the main result more prominently. I still have not read the revision and maybe you have already addressed some of these points there. - The problem statement is close to that from [Montufar, Pascanu, Cho, Bengio NIPS 2014], which specifically arrives at exponential gaps between deep and shallow ReLU networks, albeit from a different angle. I would suggest to include that paper it in the overview. - In Lemma 3, there is an i that should be x - In Theorem 4, `` ilde f** is missing the (x). - Theorem 11, the lower bound always increases with L ? - In Theorem 11,  f xin [0,1]^d?  The main contribution of this paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network that uses O(log(1/eps)) layers and O(poly log(1/eps)) hidden units where the activation functions can be either ReLU or binary step or any combination of them. The paper is well written and clear. The arguments and proofs are easy to follow. I only have two questions: 1- It would be great to have similar results without binary step units. To what extent do you find the binary step unit central to the proof? 2- Is there an example of piecewise smooth function that requires at least poly(1/eps) hidden units with a shallow network?  This paper shows: 1. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1/e) layers and O(log 1/e) ReLU units. 2. Extensions of the previous results to more general function classes, such as smooth or vector-valued functions. 3. Lower bounds on the neural network size, as a function of its number of layers. The lower bound reveals the need of exponentially many more units to approximate functions using shallow architectures. The paper is well written and easy to follow. The technical content, including the proofs in the Appendix, look correct. Although the proof techniques are simple (and are sometimes modifications of arguments by Gil, Telgarsky, or Dasgupta), they are brought together in a coherent manner to produce sharp results. Therefore, I am leaning toward acceptance.","['Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",7,4,"Strengths: 1. The paper is well written and clear. The arguments and proofs are easy to follow. 2. The paper is well structured and easy to read. 3. The results are interesting and relevant. Weaknesses: 1. The paper is not very original. The main result is a consequence of previous work by Montufar, Pascanu, Cho, Bengio, and Telgarsky. 2. The paper does not provide a clear motivation for the choice of the functions. 3. The paper does not provide a clear motivation for the choice of the functions. 4. The paper does not provide a clear motivation for the choice of the functions. 5. The paper does not provide a clear motivation for the choice of the functions. Rating: 6 Confidence: 3"
The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables | OpenReview,"The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables -- continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.","The authors of the paper present a novel distribution for discrete variables called the *concrete distribution*. The distribution can be seen as a continuous relaxation for a distribution over discrete random variables. The main motivation for introduction of the concrete distribution is the possibility to compute the gradient of discrete stochastic nodes in Stochastic Computational Graphs. I think the paper is well written and sound, definitely of interest for the conference program. As to the experimental part, the authors have results which support some kind of consistent superior performance for VIMCO for linear models and for concrete relaxations for non-linear models. Any explanation for that? Is this confirmed over different models and maybe datasets? Similarly, it looks like VIMCO outperforms (in Figure 4) Concrete for large m, on the test NLL. I would encourage to try with other values of m to see if this dependence on large m is confirmed or not. I believe the paper should be accepted to the conference, however please consider that I*m not an expert in this field. Some minor observations/comments/issues: -Section 2.1: there is a repetition *be be* in the first paragraph. -Section 2.4: I would add a reference for the *multi-sample variational objective* -Section 3.1, just before Section 3.2: *the Gumbel is a crucial 1*. Why 1 and not *one*? -Section 3.3, last paragraph: *Thus, in addition to relaxing the sampling pass of a SCG the log...* I would add a comma after *SCG*. More in general, the second part of the paragraph is very dense and not easy to *absorb*. I don*t think it*s an issue with the presentation: the concepts themselves are just dense. However, maybe the authors could find a way to make the paragraph easier to assimilate for a less experienced reader. -Section 5.1, second paragraph: *All our models are neural networks with layers of n-ary discrete stochastic nodes with log_2(n)-dimensional states on the corners of the hypercube {-1,1}^log_2(n). The distribution of the nodes are parametrized by n real values log alpha_k*. It is not clear to me, where does the log_2(n) come from. Similarly for the {-1,1}. -Section 5.2: After *this distribution.* and *We will* there is an extra space. -If a compare the last formula in Section 5.3 with Eq. 8, I don*t see exactly why the former is a special case of the latter. Is it because q(Z^i | x) is always one?  The authors describe the concrete distribution, a continuous approximation to discrete distributions parameterized by a vector of continuous positive numbers proportional to the probability of each discrete result. The concrete distribution is obtained by using the softmax function to approximate the argmax operator. The paper is clearly written, original and significant. The experiments clearly illustrate the advantages of the proposed method. Some minor questions: *for the general n-ary case the Gumbel is a crucial 1 and the Gumbel-Max trick cannot be generalized for other additive noise distributions* What do you mean by this? Can you be more specific? What is the temperature values used to obtain Table 1 and the table in Figure 4.  Thank you for an interesting read. I think this paper has proposed a very useful method, which significantly simplifies the implementation of gradients for discrete random variables. Using this trick quite a lot of discrete variable-based methods will be significantly easier to implement, e.g. a GAN-style generator for text (see the recent arxiv preprint arXiv:1611.04051). I*ve got one suggestion to make the paper even better, but maybe the authors want to leave it to future work. I think compared to lots of variance reduction techniques such as NVIL and VIMCO, this relaxation trick has smaller variance (from empirical observation of the reparameterisation trick), but in the price of introducing biases. It would be fantastic if the authors can discuss the bias-variance trade-off, either in theoretical or experimental way. My bet will be that here the variance dominates the stochastic estimation error of the gradient estimation, but it would be great if the authors can confirm this. **to area chair: concurrent paper by Jang et al. 2016** It seems there*s a concurrent submission by Jang et al. I havent* read that paper in detail, but maybe the conference should accept or reject both?","['Rating:###7: Good paper, accept', 'Rating:###9: Top 15% of accepted papers, strong accept', 'Rating:###8: Top 50% of accepted papers, clear accept']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",7,3,Strengths: 1. The paper is well written and original. 2. The authors introduce a new distribution that can be used to approximate discrete distributions. 3. The authors show that the proposed distribution can be used to approximate discrete distributions in a way that is similar to the Gumbel-Max trick. 4. The authors provide a clear and concise explanation of the proposed distribution and its properties. 5. The authors provide a clear and concise explanation of the proposed method and its advantages. Weaknesses: 1. The paper is not very clear about the motivation for introducing the new distribution. 2. The authors do not provide a clear explanation of the advantages of the proposed distribution over other methods for approximating discrete distributions. 3. The authors do not provide a clear explanation of the bias-variance trade-off of the proposed method. 4. The authors do not provide a clear explanation of the limitations of the proposed method. 5. The paper does not provide a clear explanation of how the proposed method can be used in practice. Rating: 8 Confidence: 4
Chess Game Concepts Emerge under Weak Supervision: A Case Study of Tic-tac-toe | OpenReview,"This paper explores the possibility of learning chess game concepts under weak supervision with convolutional neural networks, which is a topic that has not been visited to the best of our knowledge. We put this task in three different backgrounds: (1) deep reinforcement learning has shown an amazing capability to learn a mapping from visual inputs to most rewarding actions, without knowing the concepts of a video game. But how could we confirm that the network understands these concepts or it just does not? (2) cross-modal supervision for visual representation learning draws much attention recently. Is this methodology still applicable when it comes to the domain of game concepts and actions? (3) class activation mapping is widely recognized as a visualization technique to help us understand what a network has learnt. Is it possible for it to activate at non-salient regions? With the simplest chess game tic-tac-toe, we report interesting results as answers to those three questions mentioned above. All codes, pre-processed datasets and pre-trained models will be released.","Game of tic-tac-toe is considered. 1029 tic-tac-toe board combinations are chosen so that a single move will result into victory of either the black or the white player. There are 18 possible moves - 2 players x 9 locations. A CNN is trained from a visual rendering of the game board to these 18 possible outputs. CAM technique is used to visualize the salient regions in the inputs responsible for the prediction that CNN makes. Authors find that predictions correspond to the winning board locations. Authors claim that this: 1. is a very interesting finding. 2. CNN has figured out game rules. 3. Cross modal supervision is applicable to higher-level semantics. I don*t think (2) be can be claimed because the knowledge of game rules is not tested by any experiment. There is only *one* stage of a game - i.e. last move that is considered. Further, the results are on the training set itself - the bare minimum requirement of any implicit or explicit representation of game rules is the ability to act in previously unseen states (i.e. generalization). Even if the CNN did generalize, I would avoid making any claims about knowledge of game rules. For (3), author*s definition of cross-modal seems to be training from images to games moves. In image-classification we go from images --> labels (i.e. between two different domains). We already know CNNs can perform such mappings. CNNs have been used to map images to actions such as in DQN my Mnih et al., or DDPG by Lillicrap et al. and a lot of other classical work such as ALVIN. It*s unclear what points authors are trying to make. For (1): how interesting is an implicit attention mechanism is a subjective matter. The authors claim a difference between the concepts of *what do do* and *what will happen*. They claim by supervising for *what will happen*, the CNN can automatically learn about *what to do*. This is extensively studied in the model predictive control literature. Where model is *what will happen next*, and the model is used to infer a control law - *what to do*. However, in the experimental setup presented in the paper what will happen and what to do seem to be the exact same things. For further analysis of what the CNN has learnt I would recommend: (a) Visualizing CAM with respect to incorrect classes. For eg, visualize the CAM with respect to player would lose (instead of winning). (b) Split the data into train/val and use the predictions on the val-set for visualization. These would be much more informative about what kind of *generalizable* features the CNN pays attention to. In summary, understanding why CNN*s make what decisions they make is a very interesting area of research. While the emergence of an implicit attention mechanism may be considered to be an interesting finding by some, many claims made by the authors are not supported by experiments (see comments above).  Summary === This paper presents tic-tac-toe as toy problem for investigating CNNs. A dataset is created containing tic-tac-toe boards where one player is one move away from winning and a CNN is trained to label boards according to (1) the player who can win (2 choices) and (2) the position they may move to win (9 choices), resulting in 18 labels. The CNN evaluated in this paper performs perfectly at the task and the paper*s goal is to inspect how the CNN works. The fundamental mechanism for this inspection is Class Activation Mapping (CAM) (Zhou et. al. 2016), which identifies regions of implicit attention in the CNN. These implicit attention maps (localization heat maps) are used to derive actions (which square each player should move). The attention maps (1) attend to squares in the tic-tac-toe board rather than arbitrary blobs, despite the fact that one square in a board has uniform color, and (2) they can be used to pick correct (winning) actions. This experiment are used to support assertions that the network understands (1) chess (tic-tac-toe) boards (2) a rule for winning tic-tac-toe (3) that there are two players. Some follow up experiments indicate similar results under various renderings of the tic-tac-toe boards and an incomplete training regime. More Clarifying Questions === * I am not quite sure precisely how CAM is implemented here. In the original CAM one must identify a class of interest to visualize (e.g., cat or dog). I don*t think this paper identifies such a choice. How is one of the 18 possible classes chosen for creating the CAM visualization and through that visualization choosing an action? * How was the test set for this dataset for the table 1 results created? How many of the final 1029 states were used for test and was the distribution of labels the same in train and test? * How is RCO computed? Is rank correlation or Pearson correlation used? If Pearson correlation is used then it may be good to consider rank correlation, as argued in *Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?* by Das et. al. in EMNLP 2016. In table 1, what does the 10^3 next to RCO mean? Pros === * The proposed method, deriving an action to take from the result of a visualization technique, is very novel. * This paper provides an experiment that clearly shows a CNN relying on context to make accurate predictions. * The use of a toy tic-tac-toe domain to study attention in CNNs (implicit or otherwise) is a potentially fruitful setting that may lead to better understanding of implicit and maybe explicit attention mechanisms. Cons === * This work distinguishes between predictions about *what will happen* (will the white player win?) and *what to do* (where should the white player move to win?). The central idea is generalization from *what will happen* to *what to do* indicates concept learning (sec. 2.1). Why should an ability to act be any more indicative of a learned concept than an ability to predict future states. I see a further issue with the presentation of this approach and a potential correctness problem: 1. (correctness) In the specific setting proposed I see no difference between *what to do* and *what will happen.* Suppose one created labels dictating *what to do* for each example in the proposed dataset. How would these differ from the labels of *what will happen* in the proposed dataset? In this case *what will happen* labels include both player identity (who wins) and board position (which position they move to win). Wouldn*t the *what to do* labels need to indicate board position? They could also chosen to indicate player identity, which would make them identical to the *what will happen* labels (both 18-way softmaxes). 2. (presentation) I think this distinction would usually be handled by the Reinforcement Learning framework, but the proposed method is not presented in that framework or related to an RL based approach. In RL *what will happen* is the reward an agent will receive for making a particular action and *what to do* is the action an agent should take. From this point of view, generalization from *what will happen* to *what to do* is not a novel thing to study. Alternate models include: * A deep Q network (Mnih. et. al. 2015) could predict the value of every possible action where an action is a (player, board position) tuple. * The argmax of the current model*s softmax could be used as an action prediction. The deep Q network approach need not be implemented, but differences between methods should be explained because of the uniqueness of the proposed approach. * Comparison to work that uses visualization to investigate deep RL networks is missing. In particular, other work in RL has used Simonyan et. al. (arXiv 2013) style saliency maps to investigate network behavior. For example, *Dueling Network Architectures for Deep Reinforcement Learning* by Wang et. al. in (ICML 2016) uses saliency maps to identify differences between their state-value and advantage networks. In *Graying the black box: Understanding DQNs* by Zahavy et. al. (ICML 2016) these saliency maps are also used to analyze network behavior. * In section 2.3, saliency maps of Simonyan et. al. are said to not be able to activate on grid squares because they have constant intensity, yet no empirical or theoretical evidence is provided for this claim. On a related note, what precisely is the notion of information referenced in section 2.3 and why is it relevant? Is it entropy of the distribution of pixel intensities in a patch? To me it seems that any measure which depends only on one patch is irrelevant because the methods discussed (e.g., saliency maps) depend on context as well as the intensities within a patch. * The presentation in the paper would be improved if the results in section 7 were presented along with relevant discussion in preceding sections. Overall Evaluation === The experiments presented here are novel, but I am not sure they are very significant or offer clear conclusions. The methods and goals are not presented clearly and lack the broader relevant context mentioned above. Furthermore, I find the lines of thought mentioned in the Cons section possibly incorrect or incomplete. As detailed with further clarifying questions, upon closer inspection I do not see how some aspects of the proposed approach were implemented, so my opinion may change with further details.  1029 tic-tac-toe boards are rendered (in various ways). These 1029 boards are legal boards where the next legal play can end the game. There are 18 categories of such boards -- 9 for the different locations of the next play, and 2 for the color of the next play. The supervision is basically saying *If you place a black square in the middle right, black will win* or *if you place a white square in the upper left, white will win*. A CNN is trained to predict these 18 categories and can do so with 100% accuracy. The focus of the paper is using Zhou et al*s Class Activation Mapping to show where the CNN focuses when making it*s decision. As I understand it, an input to CAM is the class of interest. So let*s say it is class 1 (black wins with a play to the bottom right square, if I*ve deciphered figure 2 correctly. Figure 2 should really be more clear about what each class is). So we ask CAM to determine the area of focus of the CNN for deciding whether class 1 is exhibited. The focus ends up being on the empty bottom right square (because certainly you can*t exhibit class 1 if the bottom right square is occupied). The CNN also needs to condition its decision on other parts of the board -- it needs to know whether there will be 3 in a row from some direction. But maybe that conditioning is weaker? That*s kind of interesting but I*m not sure about the deeper statements about discovering game rules that the paper hints at. I*m also not sure about the connection of this work to weakly supervised learning or multi-modal learning. The paper is pretty well written, overall, with some grammatical mistakes, but I simply don*t see the surprising discovery of this work. I also have some concerns about how contrived this scenario is -- using a big, expressive CNN for such a simple game domain and using a particular CNN visualization method. I am not an expert in reinforcement learning (which isn*t happening in this paper, but is in related works on CNN game playing), so maybe I*m not appreciating the paper appropriately.","['Rating:###3: Clear rejection', 'Rating:###3: Clear rejection', 'Rating:###3: Clear rejection']","['Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']",3,5,"Strengths:
1. The paper presents a novel approach to understanding the behavior of deep neural networks. The authors use a simple game (tic-tac-toe) to demonstrate the effectiveness of their approach. The paper is well-written and easy to follow.
2. The authors provide a clear and concise presentation of their approach and results. The paper is well-organized and easy to understand.

Weaknesses:
1. The paper does not provide a clear connection to the broader field of deep learning. The authors do not discuss the relevance of their approach to other areas of research, such as reinforcement learning or multi-modal learning.
2. The paper does not provide a clear explanation of the CNN architecture used in the experiments. The authors do not discuss the specific architecture of the CNN, which makes it difficult to understand the results.
3. The paper does not provide a clear discussion of the limitations of the approach. The authors do not discuss the potential biases of the data or the CNN, which makes it difficult to generalize the results to other domains.
4. The paper does not provide a clear comparison to other approaches. The authors do not discuss the results of other approaches to understanding the behavior of deep neural networks, which makes it difficult to evaluate the effectiveness of their approach.
5. The paper does not provide a clear explanation of the results. The authors do not provide a clear discussion of the implications of the results, which makes it difficult to understand the significance of the findings.

Rating: 6"
Neuro-Symbolic Program Synthesis | OpenReview,"Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these architectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross correlation I/O network, given a set of input-output examples, produces a continuous representation of the set of I/O examples. The second module, the Recursive-Reverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Experiments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training.","The paper presents a method to synthesize string manipulation programs based on a set of input output pairs. The paper focuses on a restricted class of programs based on a simple context free grammar sufficient to solve string manipulation tasks from the FlashFill benchmark. A probabilistic generative model called Recursive-Reverse-Recursive Neural Network (R3NN) is presented that assigns a probability to each program*s parse tree after a bottom-up and a top-down pass. Results are presented on a synthetic dataset and a Microsoft Excel benchmark called FlashFill. The problem of program synthesis is important with a lot of recent interest from the deep learning community. The approach taken in the paper based on parse trees and recursive neural networks seems interesting and promising. However, the model seems too complicated and unclear at several places (details below). On the negative side, the experiments are particularly weak, and the paper does not seem ready for publication based on its experimental results. I was positive about the paper until I realized that the method obtains an accuracy of 38% on FlashFill benchmark when presented with only 5 input-output examples but the performance degrades to 29% when 10 input-output examples are used. This was surprising to the authors too, and they came up with some hypothesis to explain this phenomenon. To me, this is a big problem indicating either a bug in the code or a severe shortcoming of the model. Any model useful for program synthesis needs to be applicable to many input-output examples because most complicated programs require many examples to disambiguate the details of the program. Given the shortcoming of the experiments, I am not convinced that the paper is ready for publication. Thus, I recommend weak reject. I encourage the authors to address the comments below and resubmit as the general idea seems promising. More comments: I am unclear about the model at several places: - How is the probability distribution normalized? Given the nature of bottom-up top-down evaluation of the potentials, should one enumerate over different completions of a program and the compare their exponentiated potentials? If so, does this restrict the applicability of the model to long programs as the enumeration of the completions gets prohibitively slow? - What if you only use 1 input-output pair for each program instead of 5? Do the results get better? - Section 5.1.2 is not clear to me. Can you elaborate by potentially including some examples? Does your input-output representation pre-supposes a fixed number of input-output examples across tasks (e.g. 5 or 10 for all of the tasks)? Regarding the experiments, - Could you present some baseline results on FlashFill benchmark based on previous work? - Is your method only applicable to short programs? (based on the choice of 13 for the number of instructions) - Does a program considered correct when it is identical to a test program, or is it considered correct when it succeeds on a set of held-out input-output pairs? - When using 100 or more program samples, do you report the accuracy of the best program out of 100 (i.e. recall) or do you first filter the programs based on training input-output pairs and then evaluate a program that is selected? Your paper is well beyond the recommended limit of 8 pages. please consider making it shorter.  This paper sets out to tackle the program synthesis problem: given a set of input/output pairs discover the program that generated them. The authors propose a bipartite model, with one component that is a generative model of tree-structured programs and the other component an input/output pair encoder for conditioning. They consider applying many variants of this basic model to a FlashFill DSL. The experiments explore a practical dataset and achieve fine numbers. The range of models considered, carefulness of the exposition, and basic experimental setup make this a valuable paper for an important area of research. I have a few questions, which I think would strengthen the paper, but think it*s worth accepting as is. Questions/Comments: - The dataset is a good choice, because it is simple and easy to understand. What is the effect of the *rule based strategy* for computing well formed input strings? - Clarify what *backtracking search* is? I assume it is the same as trying to generate the latent function? - In general describing the accuracy as you increase the sample size could be summarize simply by reporting the log-probability of the latent function. Perhaps it*s worth reporting that? Not sure if I missed something.  This paper proposes a model that is able to infer a program from input/output example pairs, focusing on a restricted domain-specific language that captures a fairly wide variety of string transformations, similar to that used by Flash Fill in Excel. The approach is to model successive “extensions” of a program tree conditioned on some embedding of the input/output pairs. Extension probabilities are computed as a function of leaf and production rule embeddings — one of the main contributions is the so-called “Recursive-Reverse-Recursive Neural Net” which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree (but training this operation in an end-to-end differentiable way). There are many strong points about this paper. In contrast with some of the related work in the deep learning community, I can imagine this being used in an actual application in the near future. The R3NN idea is a good one and the authors motivate it quite well. Moreover, the authors have explored many variants of this model to understand what works well and what does not. Finally, the exposition is clear (even if it is a long paper), which made this paper a pleasure to read. Some weaknesses of this paper: the results are still not super accurate, perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer. And it’s unclear why the authors did not simply train on longer programs… It also seems that the number of I/O pairs is fixed? So if I had more I/O pairs, the model might not be able to use those additional pairs (and based on the experiments, more pairs can hurt…). Overall however, I would certainly like to see this paper accepted at ICLR. Other miscellaneous comments: * Too many e’s in the expansion probability expression — might be better just to write “Softmax”. * There is a comment about adding a bidirectional LSTM to process the global leaf representations before calculating scores, but no details are given on how this is done (as far as I can see). * The authors claim that using hyperbolic tangent activation functions is important — I’d be interested in some more discussion on this and why something like ReLU would not be good. * It’s unclear to me how batching was done in this setting since each program has a different tree topology. More discussion on this would be appreciated. Related to this, it would be good to add details on optimization algorithm (SGD? Adagrad? Adam?), learning rate schedules and how weights were initialized. At the moment, the results are not particularly reproducible. * In Figure 6 (unsolved benchmarks), it would be great to add the program sizes for these harder examples (i.e., did the approach fail because these benchmarks require long programs? Or was it some other reason?) * There is a missing related work by Piech et al (Learning Program Embeddings…) where the authors trained a recursive neural network (that matched abstract syntax trees for programs submitted to an online course) to predict program output (but did not synthesize programs).","['Rating:###5: Marginally below acceptance threshold', 'Rating:###7: Good paper, accept', 'Rating:###8: Top 50% of accepted papers, clear accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",5,4,"Strengths: 1. The paper presents a novel approach to program synthesis, which is an important problem in the field of computer science. 2. The authors provide a clear and well-structured exposition of their approach. 3. The paper is well-written and easy to follow. 4. The authors provide a good motivation for their approach and discuss the related work in the field. 5. The paper is well-structured and easy to follow. Weaknesses: 1. The paper is quite long and could be shortened. 2. The results are not very impressive and the authors could have done more to evaluate their approach. 3. The paper could benefit from more discussion on the limitations of the approach. 4. The authors could have provided more details on the training procedure and the hyperparameters used. 5. The paper could benefit from more discussion on the related work and how their approach compares to it. Rating: 6 Confidence: 3"
Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer | OpenReview,"Attention plays a critical role in human visual experience. Furthermore, it has recently been demonstrated that attention can also play an important role in the context of applying artificial neural networks to a variety of tasks from fields such as computer vision and NLP. In this work we show that, by properly defining attention for convolutional neural networks, we can actually use this type of information in order to significantly improve the performance of a student CNN network by forcing it to mimic the attention maps of a powerful teacher network. To that end, we propose several novel methods of transferring attention, showing consistent improvement across a variety of datasets and convolutional neural network architectures.","The paper proposes a new way of transferring knowledge. I like the idea of transferring attention maps instead of activations. However, the experiments don’t show a big improvement compared with knowledge distillation alone and I think more experiments are required in IMAGENET section. I would consider updating the score if the authors extend the last section 4.2.2.  The paper presented a modified knowledge distillation framework that minimizes the difference of the sum of statistics across the a feature map between the teacher and the student network. The authors empirically demonstrated the proposed methods outperform the fitnet style distillation baseline. Pros: + The author evaluated the proposed methods on various computer vision dataset + The paper is in general well-written Cons: - The method seems to be limited to the convolutional architecture - The attention terminology is misleading in the paper. The proposed method really just try to distill the summed squared(or other statistics e.g. summed lp norm) of activations in a hidden feature map. - The gradient-based attention transfer seems out-of-place. The proposed gradient-based methods are never compared directly to nor are used jointly with the *attention-based* transfer. It seems like a parallel idea added to the paper that does not seem to add much value. - It is also not clear how the induced 2-norms in eq.(2) is computed. Q is a matrix in mathbb{R}^{H  imes W} whose induced 2-norm is its largest singular value. It seems computationally expensive to compute such cost function. Is it possible the authors really mean the Frobenius norm? Overall, the proposed distillation method works well in practice but the paper has some organization issues and unclear notation.  This paper proposes to investigate attention transfers between a teacher and a student network. Attention transfer is performed by minimising the l2 distance between the teacher/student attention maps at different layers, in addition to minimising the classification loss and optionally a knowledge distillation term. Authors define several activation based attentions (sum of absolute feature values raise at the power p or max of values raised at the power p). They also propose a gradient based attention (derivative of the Loss w.r.t. inputs). They evaluate their approaches on several datasets (CIFAR, Cub/Scene, Imagenet) showing that attention transfers does help improving the student network test performance. However, the student networks performs worst than the teacher, even with attention. Few remarks/questions: - in section 3 authors claim that networks with higher accuracy have a higher spatial correlation between the object and the attention map. While Figure 4 is compelling, it would be nice to have quantitative results showing that as well. - how did you choose the hyperparameter values, it would be nice to see what is the impact of . - it would be nice to report teacher train and validation loss in Figure 7 b) - from the experiments, it is not clear what at the pros/cons of the different attention maps - AT does not lead to better result than the teacher. However, the student networks have less parameters. It would be interesting to characterise the corresponding speed-up. If you keep the same architecture between the student and the teacher, is there any benefit to the attention transfer? In summary: Pros: - Clearly written and well motivated. - Consistent improvement of the student with attention compared to the student alone. Cons: - Students have worst performances than the teacher models. - It is not clear which attention to use in which case? - Somewhat incremental novelty relatively to Fitnet","['Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",6,4,Strengths: 1. The paper presents a novel method for transferring knowledge from a teacher network to a student network. 2. The authors provide a clear and well-motivated explanation of the proposed method. 3. The paper is well-written and easy to follow. Weaknesses: 1. The paper does not provide a clear motivation for the proposed method. 2. The authors do not provide a clear explanation of the attention transfer mechanism. 3. The paper does not provide a clear comparison with other methods. Rating: 6 Confidence: 4
Low-rank passthrough neural networks | OpenReview,"Deep learning consists in training neural networks to perform computations that sequentially unfold in many steps over a time dimension or an intrinsic depth dimension. For large depths, this is usually accomplished by specialized network architectures that are designed to mitigate the vanishing gradient problem, e.g. LSTMs, GRUs, Highway Networks and Deep Residual Networks, which are based on a single structural principle: the state passthrough. We observe that these *Passthrough Networks* architectures enable the decoupling of the network state size from the number of parameters of the network, a possibility that is exploited in some recent works but not thoroughly explored. In this work we propose simple, yet effective, low-rank and low-rank plus diagonal matrix parametrizations for Passthrough Networks which exploit this decoupling property, reducing the data complexity and memory requirements of the network while preserving its memory capacity. We present competitive experimental results on several tasks, including a near state of the art result on sequential randomly-permuted MNIST classification, a hard task on natural data.","The author proposes the use of low-rank matrix in feedfoward and RNNs. In particular, they try their approach in a GRU and a feedforward highway network. Author also presents as a contribution the passthrough framework, which can describe feedforward and recurrent networks. However, this framework seems hardly novel, relatively to the formalism introduced by LSTM or highway networks. An empirical evaluation is performed on different datasets (MNIST, memory/addition tasks, sequential permuted MNIST and character level penntreebank). However, there are few problems with the evaluation: - In the highway network experiment, the author does not compare with a baseline. We can not assess what it the impact of the low-rank parameterization. Also, it would be interesting to compare the result with a highway network that have this capacity bottleneck across layer (first layer of size , second layer of size , third layer of size ) and not in the gate functions. Also, how did you select the hyperparameter values?. - It is unfortunate that the character level penntreebank does not use the same experimental setting than previous works as it prevents from direct comparison. Also the overall bpc perplexity seems relatively high for this dataset. It is therefore not clear how low-rank decomposition would perform on this task applied on a stronger baseline. -Author claims state-of-art in the memory task. However, their approach uses more parameters than the uRNN (41K against 6.5K for the memory) which makes the comparison a little bit unfair toward uRNN. It would be informative to see how low-rank RNN performs using overall 6.5K parameters. Generally, it would be good to see what is the impact of the matrix rank given a fix state size. - It would be informative as well to have the baseline and the uRNN curve in Figure 2 for the memory/addition task. - it is not clear when to use low-rank or low-rank + diagonal from the experiments. Overall, the evaluation in its current form in not really convincing, except for the sequential MNIST dataset.  The authors study the use of low-rank approximation to the matrix-multiply in RNNs. This reduces the number of parameters by a large factor, and with a diagonal addition (called low-rank plus diagonal) it is shown to work as well as a fully-parametrized network on a number of tasks. The paper is solid, the only weakness being some claims about conceptual unification (e.g., the first line of the conclusion -- *We presented a framework that unifies the description various types of recurrent and feed-forward neural networks as passthrough neural networks.* -- claiming this framework as a contribution of this paper is untrue, the general framework is well known in the community and RNNs have been presented in this way before.) Aside from the above small point, the true contribution is in making low-rank RNNs work, the results are generally as good as fully-parametrized networks. They are hardly better though, which makes it unclear why low-rank networks should be used. The contribution is thus not very strong in terms of results, but even achieving the same results with fewer parameters is not easy and the studies were well-executed and explained.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",4,4,"Strengths: 
1. The paper is well-written and easy to follow. 
2. The authors are able to show that low-rank matrix-multiplication can be used to reduce the number of parameters in RNNs. 
3. The experimental results are clear and easy to understand. 

Weaknesses: 
1. The paper does not present a new contribution. The general framework of passthrough networks is well-known in the community and RNNs have been presented in this way before. 
2. The experimental results are not very impressive. The results are not significantly better than those of a fully-parametrized network. 
3. The experimental results are not very well explained. The authors do not explain why low-rank matrix-multiplication is better than a fully-parametrized network. 
4. The experimental results are not very well compared to a baseline. The baseline is not specified and the results are not compared to a baseline. 
5. The paper does not discuss the computational complexity of the proposed method. 

Rating: 6
Confidence: 3"
Deep Learning with Sets and Point Clouds | OpenReview,"We introduce a simple permutation equivariant layer for deep learning with set structure. This type of layer, obtained by parameter-sharing, has a simple implementation and linear-time complexity in the size of each set. We use deep permutation-invariant networks to perform point-could classification and MNIST digit summation, where in both cases the output is invariant to permutations of the input. In a semi-supervised setting, where the goal is make predictions for each instance within a set, we demonstrate the usefulness of this type of layer in set-outlier detection as well as semi-supervised learning with clustering side-information.","Pros: * Part of the paper addresses an industrially important topic, namely how to make deep networks work properly on point clouds, i.e. in many (most?) potential applications they should be invariant to permutations of the points within the cloud, as well as rigid transformations of the cloud (depends on the application). * The authors propose a formalism for dealing with compositions of different kinds of invariance. Cons: * For me the explanation of the generalization is really hard to follow. For me, the paper would be stronger if were less broad, but went into more depth for the permutation-invariance case. * It is very easy to sit down and come up with network structures that are permutation invariant. It seems the author tried a few networks in the family (a few different point cloud sizes, a couple options for the number of parameters, averaging vs. max in the set, dropout vs. no dropout), but unless the space is more completely and systematically explored, there*s not much reason for a practitioner to use the proposed structure vs. some other random structure they cook up that is also permutation invariant. i.e. what about just using a FC layer that is shared between the points instead of your three *set invariant* layers? Seems simpler, more general, and also permutation invariant... * It is not clear to me how valuable the author*s definition of *minimally invariant* is. Is a sufficiently large composition of *set invariant* layers a universal approximator for permutation invariant functions? * I*m concerned that proposed *set invariant layer* might be strongly variant to spatial transformations, as well as vulnerable to large outliers. In particular there is a term that subtracts a corner of the clouds bounding box (i.e. the max over set operator inside the first layer), before the cloud goes through a learned affine transform and pixelwise nonlinearity. Seems like that could saturate the whole network... I*m reviewing with low confidence, because there*s a chance the formalism in the first part of the paper is more valuable than I realize; I haven*t fully understood it.  Pros : - New and clear formalism for invariance on signals with known structure - Good numerical results Cons : - The structure must be specified. - The set structure dataset is too simple - There is a gap between the large (and sometimes complex) theory introduced and the numerical experiments ; consequently a new reader could be lost since examples might be missing Besides, from a personal point of view, I think the topic of the paper and its content could be suitable for a big conference as the author improves its content. Thus, if rejected, I think you should not consider the workshop option for your paper if you wish to publish it later in a conference, because big conferences might consider the workshop papers of ICLR as publications. (that*s an issue I had to deal with at some points)  This paper discusses ways to enforce invariance in neural networks using weight sharing. The authors formalize a way for feature functions to be invariant to a collection of relations and the main invariance studied is a “set-invariant” function, which is used in an anomaly detection setting and a point cloud classification problem. “Invariance” is, at a high level, an important issue of course, since we don’t want to spend parameters to model spurious ordering relationships, which may potentially be quite wasteful and I like the formalization of invariance presented in this paper. However, there are a few weaknesses that I feel prevent this from being a strong submission. First, the exposition is too abstract and this paper could really use a running and *concrete* example starting from the very beginning. Second, “set invariance”, which is the main type of invariance studied in the paper is defined via the author’s formalization of invariance, but is never explicitly related to what I might think of as “set invariance” — e.g. to permutations of input or output dimensions. Explicitly defining set invariance in some other way, then relating it to the “structural invariance” formulation may be a better way to explain things. It is never made clear, for example, why Figure 1(b) is *the* set data-structure. I like the discussion of compositionality of structures (one question I have here is: are the resulting compositional structures are still valid as structures?). But the authors have ignored the other kind of compositionality that is important to neural networks — specifically that relating the proposed notion of invariance to function composition seems important — i.e. under what conditions do compositions of invariant functions remain invariant? And It is clear to me that just by having one layer of invariance in a network doesn’t make the entire network invariant, for example. So if we look at the anomaly detection network at the end for example, is it clear that the final predictor is “set invariant” in some sense? Regarding experiments, there are no baselines presented for anomaly detection. Baselines *are* presented in the point cloud classification problem, but the results of the proposed model are not the best, and this should be addressed. (I should say that I don’t know enough about the dataset to say whether these are exactly fair comparisons or not). It is also never really made clear why set invariance is a desirable property for a point cloud classification setting. As a suggestion: try a network that uses a fully connected layer at the end, but uses data augmentation to enforce set invariance. Also, what about classical set kernels? Other random things: * Example 2.2: Shouldn’t |S|=5 in the case of left-right and up-down symmetry? * “Parameters shared within a relation” is vague and undefined. * Why is “set convolution” called “set convolution” in the appendix? What is convolutional about it? * Is there a relationship to symmetric function theory?  This review is only an informed guess - unfortunately I cannot assess the paper due to my lack of understanding of the paper. I have spent several hours trying to read this paper - but it has not been possible for me to follow - partially due to my own limitations, but also I think due to an overly abstract level of presentation. The paper is clearly written, but in the same way that a N. Bourbaki book is clearly written. I would prefer to leave the accept/reject decision to the other reviewers who may have a better understanding - even if the authors had made a serious mistake, I would not be able to tell. My proposal is positive because the paper is apparently clearly written and the empirical evaluation is quite promising. But some effort will be needed in order to address the broader audience that could potentially be interested in the topic. I therefore would like to provide feedback only at the level of presentation. My main source of problems is that the authors do not try to ground their abstract formalism with concrete examples; when the examples show up it is by *revelation* rather than by explaining how they connect to the previous concepts. The one example that could unlock most people*s understanding is how convolution, or inner product operations connect with the setting described here. For what I know convolution is tied with space (or time) and is understood as an equivariant operation - shifting the signal shifts the output. It is not explained how the *(x, x*)* pairs used by the authors in order to build relations, structures and then to define invariance relate to this setting. Going from sets, to relations, to functions, to operators, and then to shift-invariant operators (convolutions) involves many steps, and some hand-holding is needed. Why is the 3x3 convolution associated to 9 relations? Are these relations referring to the input at a given coordinate and its contribution to the output? (w_{offset} x_{i-offset})? In that case, why is there a backward arrow from the center node to the other nodes? And why are there arrows across nodes? What is a Cardinal and what is a Cartesian convolution in signal processing terms? (clearly these are not standard terms). Are we talking about separable filters? What are the X and Square symbols in Figure 2? And what are the horizontal and vertical sub-graphs standing for? What is x_1 and what is x_{11},x_{1,2},x_{1,3} and what is the relationship between them? I realize that to the authors these questions may seem to be trivial and left as homework for the reader. But I think part of publishing a paper is doing a big part of the homework for the readers so that it becomes easy to get the idea. Clearly the authors target the more general case - but spending some time to explain how the particular case is an instance of the the general case would be a good use of space. I would propose that the authors explain what are x, x_{I}, and x_{S} for the simplest possible example, e.g. convolving a 1x5 signal with a 1x3 filter, how the convolution filter parameters show up in the function f, as well as how the spatial invariance (or, equivariance) of convolution is reflected here.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###7: Good paper, accept']","['Confidence:###2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###1: The reviewer*s evaluation is an educated guess']",6,2,Strengths: 1. The paper presents a new formalism for invariance in neural networks. 2. The authors provide a clear and simple way to define invariance for a set of points. 3. The paper is well-written and easy to follow. Weaknesses: 1. The paper is too abstract. 2. The authors do not provide enough concrete examples. 3. The paper does not clearly explain how the proposed formalism relates to the concept of invariance in neural networks. 4. The authors do not provide enough details about the experiments. 5. The paper does not clearly explain how the proposed formalism relates to the concept of invariance in neural networks. 6. The paper does not clearly explain how the proposed formalism relates to the concept of invariance in neural networks. 7. The paper does not clearly explain how the proposed formalism relates to the concept of invariance in neural networks. Rating: 6 Confidence: 3
Learning to Understand: Incorporating Local Contexts with Global Attention for Sentiment Classification | OpenReview,"Recurrent neural networks have shown their ability to construct sentence or paragraph representations. Variants such as LSTM overcome the problem of vanishing gradients to some degree, thus being able to model long-time dependency. Still, these recurrent based models lack the ability of capturing complex semantic compositions. To address this problem, we propose a model which can incorporate local contexts with the guide of global context attention. Both the local and global contexts are obtained through LSTM networks. The working procedure of this model is just like how we human beings read a text and then answer a related question. Empirical studies show that the proposed model can achieve state of the art on some benchmark datasets. Attention visualization also verifies our intuition. Meanwhile, this model does not need pretrained embeddings to get good results.","The authors did not bother responding or fixing any of the pre-review comments. Hence I repeat here: Please do not make incredibly unscientific statements like this one: *The working procedure of this model is just like how we human beings read a text and then answer a related question. * Really, *humans beings* have an LSTM like model to read a text? Can you cite an actual neuroscience paper for such a claim? The answer is no, so please delete such statements from future drafts. Generally, your experiments are about simple classification and the methods you*re competing against are simple models like NB-SVM. So I would change the title, abstract ad introduction accordingly and not attempt hyperbole like *Learning to Understand* in the title. Lastly, your attention level approach seems similar to dynamic memory networks by Kumar et al. they also have experiments for sentiment and it would be interesting to understand the differences to your model and compare to them. Other reviewers included further missing related work and fitting this paper into the context of current literature. Given that no efforts were made to fix the pre-review questions and feedback, I doubt this will become ready in time for publication.  The paper proposes to enhance the attention mechanism for sentiment classification by using global context computed by a Bi-LSTM. The proposed models outperform many existing models in the literature on 3 sentiment analysis datasets. The key idea of using Bi-LSTM to compute global context for attention is actually not novel, as proposed several times in the literature, e.g., Luong et al (2015) and Shen & Lee (2016). Especially, Luong et al (2015) already proposed to combine global context with local context for attention. Regarding to the experiments, of course it would be nice if the model can work well without the need of tricks like dropout or pre-trained word embeddings. However, it would be even better if the model can work well using those tricks. The authors should show results of the models using those tricks and compare them to the results in the literature. Ref: Luong et al. Effective Approaches to Attention-based Neural Machine Translation. EMNLP 2015  This paper presents a hierarchical attention-based method for document classification. The main idea is to first run a bidirectional LSTM to get global context vector, and then run another attention-based bidirectional LSTM that uses the final hidden state from the first pass to weight local context vectors (TS-ATT). A simpler architecture that removes the first LSTM and uses the output of the second LSTM as the global context vector is also proposed (SS-ATT). Experiments on three datasets are presented, however the results are mostly not state-of-the-art. I think the idea is nice, but the experiment results are not convincing enough to justify this new model architecture. Why is your Yelp 2013 dataset smaller than the original Tang et al, 2015 paper that has ~300k documents? I noticed your other datasets are also quite small. Is it because your model is difficult to scale to large datasets? You should also include results from Tang et al., 2015 in Table 2 that achieves 65.1% accuracy on Yelp 2013 (why is your number so much lower?) I also suggest removing phrases such as *Learning to Understand* when presenting their model. Overall, I think that this submission is a better fit for the workshop. Minor comments: - gloal -> global - Not needing a pretrained embeddings, while of course nice, is not that big of a deal. Various models will work just fine without pretrained embeddings.","['Rating:###3: Clear rejection', 'Rating:###3: Clear rejection', 'Rating:###4: Ok but not good enough - rejection']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",3,4,Strengths: 1. The authors propose a novel approach to incorporating local context with global context attention for sentiment classification. 2. The authors provide a clear and concise description of their approach. Weaknesses: 1. The authors do not provide a clear motivation for their approach. What problem do they try to solve? 2. The authors do not provide a clear description of the global context vector. How is it computed? What is its role in the model? 3. The authors do not provide a clear description of the local context vector. How is it computed? What is its role in the model? 4. The authors do not provide a clear description of the attention mechanism. How is it computed? What is its role in the model? 5. The authors do not provide a clear description of the experimental setup. What are the hyperparameters? What is the evaluation metric? 6. The authors do not provide a clear description of the baseline models. What are the baseline models? How do they compare to the proposed model? 7. The authors do not provide a clear description of the results. What are the results? How do they compare to the baseline models? 8. The authors do not provide a clear description of the attention visualization. What does it show? What does it imply? 9. The authors do not provide a clear description of the dynamic memory network. What is it? How does it compare to the proposed model?
Sequence to Sequence Transduction with Hard Monotonic Attention | OpenReview,"We present a supervised sequence to sequence transduction model with a hard attention mechanism which combines the more traditional statistical alignment methods with the power of recurrent neural networks. We evaluate the model on the task of morphological inflection generation and show that it provides state of the art results in various setups compared to the previous neural and non-neural approaches. Eventually we present an analysis of the learned representations for both hard and soft attention models, shedding light on the features such models extract in order to solve the task.","The paper describes a recurrent transducer that uses hard monotonic alignments: at each step a discrete decision is taken either to emit the next symbol or to consume the next input token. The model is moderately novel - similar architecture was proposed for speech recognition (https://arxiv.org/pdf/1608.01281v1.pdf). Soft monotonic alignemts are also enforced by A. Graves in https://arxiv.org/abs/1308.0850. The difficult part in training the proposed model is backpropagation through the discrete decisions. Typically, reinforcement learning techniques are used. In this contribution, the authors side-step the issue by using a problem-dependent aligner to generate optimal decisions for which they train the model. The results indicate that such specially supervised model is better than the generic soft-attention model that doesn*t require any problem-dependent external supervision. However the authors did not attempt to work on regularizing the soft-attention model, which is not fair - the extra supervision by using the ground-truth alignment is a form of regularization and it could be used as e.g. an extra signal to the soft-attention model for a better comparison. That being said the authors reash state-of-the-art results against other domain specific methods. I believe the paper would more suit a NLP venue - it sound and properly written, but its applicability is limited to the considered NLP problem.  This paper proposes a sequence transduction model that first uses a traditional statistical alignment methods to provide alignments for an encoder-decoder type model. The paper provides experiments on a number of morphological inflection generation datasets. They shows an improvement over other models, although they have much smaller improvements over a soft attention model on some of their tasks. I found this paper to be well-written and to have very thorough experiments/analysis, but I have concerns that this work isn*t particularly different from previous approaches and thus has a more focused contribution that is limited to its application on this type of shorter input (the authors *suggest* that their approach is sufficient for shorter sequences, but don*t compare against the approach of Chorowski et al. 2015 or Jailty el at 2016). In summary, I found this paper to be well-executed/well-written, but it*s novelty and scope too small. That said, I feel this work would make a very good short paper.  The paper proposes an approach to sequence transduction for the case when a monotonic alignment between the input and the output is plausible. It is assumed that the alignment can be provided as a part of training data, with Chinese Restaurant process being used in the actual experiments. The idea makes sense, although its applicability is limited to the domains where a monotonic alignment is available. But as discussed during the pre-review period, there has been a lot of strongly overlapping related work, such as probabilistic models with hard-alignment (Sequence Transduction With Recurrent Neural Network, Graves et al, 2012) and also attempts to use external alignments in end-to-end models (A Neural Transducer, Jaitly et al, 2015). That said, I do not think the approach is sufficiently novel. I also have a concern regarding the evaluation. I do not think it is fair to compare the proposed model that depends on external alignment with the vanilla soft-attention model that learns alignments from scratch. In a control experiment soft-attention could be trained to match the external alignment. Such a pretraining could reduce overfitting on the small dataset, the one on which the proposed approach brings the most improvement. On a larger dataset, especially SIGMORPHON, the improvements are not very big and are only obtained for a certain class of languages. To sum up, two main issues are (a) lack of novelty (b) the comparison of a model trained with external alignment and one without it.","['Rating:###5: Marginally below acceptance threshold', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###4: Ok but not good enough - rejection']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",5,4,Strengths: 1. The paper is well-written and well-structured. 2. The authors provide a clear and well-justified motivation for their approach. 3. The experiments are well-designed and well-justified. Weaknesses: 1. The novelty of the approach is limited. 2. The comparison with other models is limited. 3. The paper does not provide a clear direction for future work. Rating: 6 Confidence: 3
Visualizing Deep Neural Network Decisions: Prediction Difference Analysis | OpenReview,"This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).","The paper presents a theoretically well motivated for visualizing what parts of the input feature map are responsible for the output decision. The key insight is that features that maximally change the output and are simultaneously more unpredictable from other features are the most important ones. Most previous work has focused on finding features that maximally change the output without accounting for their predictability from other features. Authors build upon ideas presented in the work of Robnik-Šikonja & Kononenko (2008). The results indicate that the proposed visualization mechanism based on modeling conditional distribution identifies more salient regions as compared to a mechanism based on modeling marginal distribution. I like that authors have presented visualization results for a single image across multiple networks and multiple classes. There results show that the proposed method indeed picks up on class-discriminative features. Authors have provided a link to visualizations for a random sample of images in a comment – I encourage the authors to include this in the appendix of the paper. My one concern with the paper is – Zeiler et al., proposed a visualization method by greying small square regions in the image. This is similar to computing the visualization using the marginal distribution. Authors compute the marginal visualization using 10 samples, however in the limit of infinite samples the image region would be gray. The conditional distribution is computed using a normal distribution that provides some regularization and therefore estimating the conditional and marginal distributions using 10 samples each is not justified. I would like to see the comparison when grey image patches (akin to Zeiler et al.) are used for visualization against the approach based on the conditional distribution.  The authors propose a way to visualize which areas of an image provide mostly influence a certain DNN response mostly. They apply some very elegant and convincing improvements to the basic method by Robnik-Sikonja and Konononko from 2008 to DNNs, thus improving it*s analysis and making it usable for images and DNNs. The authors provide a very thorough analysis of their methods and show very convincing examples (which they however handpicked. It would be very nice to have maybe at least one figure showing the analysis on e.g. 24 random picks from ImageNet). One thing I would like to see is how their method compares to some other methods they mention in the introduction (like gradient-based ones or deconvolution based ones). They paper is very clearly written, all necessary details are given and the paper is very nice to read. Alltogether: The problem of understanding how DNNs function and how they draw their conclusions is discussed a lot. The author*s method provides a clear contribution that can lead to further progress in this field (E.g. I like figure 8 showing how AlexNet, GoogLeNet and VGG differ in where they collect evidence from). I can think of several potential applications of the method and therefore consider it of high significance. Update: The authors did a great job of adopting all of my suggestions. Therefore I improve the rating from 8 to 9.  The authors of this work propose an interesting approach to visualizing the predictions made by a deep neural network. The manuscript is well written is provides good insight into the problem. I also appreciate the application to medical images, as simply illustrating the point on ImageNet isn*t interesting enough. I do have some questions and comments. 1. As the authors correctly point out in 3.1, approximating the conditional probability of a feature x_i by the marginal distribution p(x_i) is not realistic. They advocate for translation invariance, i.e. the position of the pixel in the image shouldn*t affect the probability, and suggest that the pixels appearance depends on the small neighborhood around it. However, it is well known that global context makes an big impact on the semantics of pixels. In *Objects in Contexts*, authors show that a given neighborhood of pixels can take different semantic meanings based on the global context in the image. In the context of deep neural networks, works such as *ParseNet* also illustrate the importance of global context on the spatial label distribution. This does not necessarily invalidate this approach, but is a significant limitation. It would be great if the authors provided a modification to (4) and empirically verified the change. 2. Figure 7 shows the distribution over top 3 predictions before and after softmax. It is expected that even fairly uniform distributions will transform toward delta functions after softmax normalization. Is there an additional insight here? 4. Finally, in 4.1, the authors state that it takes 30 minutes to analyze a single image with GooLeNet on a GPU? Why is this so computationally expensive? Such complexity seems to make the algorithm impractical and analyzing datasets of statistical relevance seems prohibitive.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###9: Top 15% of accepted papers, strong accept', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",6,4,Strengths: 1. The paper presents a theoretically well motivated method for visualizing the response of a deep neural network to a specific input. 2. The method is based on the idea that features that maximally change the output and are simultaneously more unpredictable from other features are the most important ones. 3. The results indicate that the proposed visualization mechanism based on modeling conditional distribution identifies more salient regions as compared to a mechanism based on modeling marginal distribution. 4. The authors have provided a link to visualizations for a random sample of images in a comment – I encourage the authors to include this in the appendix of the paper. 5. The authors have provided a link to visualizations for a random sample of images in a comment – I encourage the authors to include this in the appendix of the paper. 6. The authors have provided a link to visualizations for a random sample of images in a comment – I encourage the authors to include this in the appendix of the paper. 7. The authors have provided a link to visualizations for a random sample of images in a comment – I encourage the authors to include this in the appendix of the paper. 8. The authors have provided a link to visualizations for a random sample of images in a comment – I encourage the authors to include this in the appendix of the paper. 9. The authors have provided a link to visualizations for a random sample of images in a comment – I encourage the authors to
Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity | OpenReview,"In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free networks in the form of , where is ReLU nonlinearity. We assume that the input follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters using loss. We first show that when , the nonlinear dynamics can be written in close form, and converges to with at least probability, if random weight initializations of proper standard derivation ( ) is used, verifying empirical practice. For networks with many ReLU nodes ( ), we apply our close form dynamics and prove that when the teacher parameters forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with loss. Simulations verify our theoretical analysis.","The paper proposes a convergence analysis of some two-layer NNs with ReLUs. It is not the first such analysis, but maybe it is novel on the assumptions used in the analysis, and the focus on ReLU nonlinearity that is pretty popular in practice. The paper is quite hard to read, with many English mistakes and typos. Nevertheless, the analysis seems to be generally correct. The novelty and the key insights are however not always well motivated or presented. And the argument that the work uses realistic assumptions (Gaussian inputs for example) as opposed to other works, is quite debatable actually. Overall, the paper looks like a correct analysis work, but its form is really suboptimal in terms of writing/presentation, and the novelty and relevance of the results are not always very clear, unfortunately. The main results and intuition should be more clearly presented, and details could be moved to appendices for example - that could only help to improve the visibility and impact of these interesting results.  This work analyzes the continuous-time dynamics of gradient descent when training two-layer ReLU networks (one input, one output, thus only one layer of ReLU units). The work is interesting in the sense that it does not involve some unrealistic assumptions used by previous works with similar goal. Most importantly, this work does not assume independence between input and activations, and it does not rely on noise injection (which can simplify the analysis). Nonetheless, removing these simplifying assumptions comes at the expense of limiting the analysis to: 1. Only one layer of nonlinear units 2. Discarding the bias term in ReLU while keeping the input Gaussian (thus constant input trick cannot be used to simulate the bias term). 3. Imposing strong assumption on the representation on the input/output via (bias-less) ReLU networks: existence of orthonormal bases to represent this relationships. Having that said, as far as I can tell, the paper presents original analysis in this new setting, which is interesting and valuable. For example, by exploiting the symmetry in the problem under the assumption 3 I listed above, the authors are able to reduce the high-dimensional dynamics of the gradient descent to a bivariate dynamics (instead of dealing with original size of the parameters). Such reduction to 2D allows the author to rigorously analyze the behavior of the dynamics (e.g. convergence to a saddle point in symmetric case, or to the optimum in non-symmetric case). Clarification Needed: first paragraph of page 2. Near the end of the paragraph you say *Initialization can be arbitrarily close to origin*, but at the beginning of the same paragraph you state *initialized randomly with standard deviation of order 1/sqrt(d)*. Aren*t these inconsistent? Some minor comments about the draft: 1. In section 1, 2nd paragraph: *We assume x is Gaussian and thus the network is bias free*. Do you mean *zero-mean* Gaussian then? 2. *standard deviation* is spelled *standard derivation* multiple times in the paper. 3. Page 6, last paragraph, first line: Corollary 4.1 should be Corollary 4.2  In this paper, the author analyzes the convergence dynamics of a single layer non-linear network under Gaussian iid input assumptions. The first half of the paper, dealing with a single hidden node, was somewhat clear, although I have some specific questions below. The second half, dealing with multiple hidden nodes, was very difficult for me to understand, and the final *punchline* is quite unclear. I think the author should focus on intuition and hide detailed derivations and symbols in an appendix. In terms of significance, it is very hard for me to be sure how generalizable these results are: the Gaussian assumption is a very strong one, and so is the assumption of iid inputs. Real-world feature inputs are highly correlated and are probably not Gaussian. Such assumptions are not made (as far as I can tell) in recent papers analyzing the convergence of deep networks e.g. Kawaguchi, NIPS 2016. Although the author says the no assumption is made on the independence of activations, this assumption is shifted to the input instead. I think this means that the activations are combinations of iid random variables, and are probably Gaussian like, right? So I*m not sure where this leaves us. Specific comments: 1. Please use D_w instead of D to show that D is a function of w, and not a constant. This gets particularly confusing when switching to D(w) and D(e) in Section 3. In general, notation in the paper is hard to follow and should be clearly introduced. 2. Section 3, statement that says *when the neuron is cut off at sample l, then (D^(t))_u* what is the relationship between l and u? Also, this is another example of notational inconsistency that causes problems to the reader. 3. Section 3.1, what is F(e, w) and why is D(e) introduced? This was unclear to me. 4. Theorem 3.3 suggests that (if epsilon is > 0), then to have the maximal probability of convergence, epsilon should be very close to 0, which means that the ball B_r has radius r -> 0? This seems contradictory from Figure 2. 5. Section 4 was really unclear and I still do not understand what the symmetry group really represents. Is there an intuitive explanation why this is important? 6. Figure 5: what is a_j ? I encourage the author to rewrite this paper for clarity. In it*s present form, it would be very difficult to understand the takeaways from the paper.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###4: Ok but not good enough - rejection']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",4,3,"Strengths: 1. The paper is well-written and clear. 2. The authors are able to provide a clear and intuitive explanation of the main results. Weaknesses: 1. The paper is not very original. The main result is a proof that the gradient descent converges to a saddle point under certain conditions. This is a well-known result and has been proven many times before. 2. The paper is very hard to read. The notation is not consistent and the text is not well-organized. 3. The paper is very short. It is hard to believe that the authors were able to derive such a clear and intuitive result in such a short amount of time. Rating: 4
Confidence: 2"
Delving into Transferable Adversarial Examples and Black-box Attacks | OpenReview,"An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.","The paper presents an interesting and very detailed study of targeted and non-targeted adversarial examples in CNNs. I’m on the fence about this paper but am leaning towards acceptance. Such detailed empirical explorations are difficult and time-consuming to construct yet can serve as important stepping stones for future work. I see the length of the paper as a strength since it allows for a very in-depth look into the effectiveness and transferability of different kinds of adversarial examples. There are, however, some concerns: 1) While the length of the paper is a strength in my mind, the key contributions should be made much more clear. As evidenced by my comment earlier, I got confused at some point between the ensemble/non-ensemble method, and about the contribution of the Clarifai evaluation and what I should be focusing on where. I’d strongly suggest a radical revision which more clearly focuses the story: - First, we demonstrate that non-targeted attacks are easy while targeted attacks are hard (evidenced by a key experiment comparing the two; we refer to appendix or later sections for the extensive exploration of e.g., current Section 3) - Thus, we propose an ensemble method that is able to handle targeted attacks much better (evidenced by experiments focusing on the comparison between ensemble and non-ensemble method, both in a controlled setting and on Clarifai) - Also, here are all the other details and explorations. 2) Instead of using ResNet-152, Res-Net-101 and ResNet-50 as three of the five models, it would*ve been better to use one ResNet architecture and the other two, say, AlexNet and Network-in-Network. This would make the ensemble results a lot more compelling.  This paper present an experimental study of the robustness of state-of-the-art CNNs to different types of *attacks* in the context of image classication. Specifically, an attack aims to fool the classification system with a specially corrupted image, i.e. making it misclassify the image as (1) any wrong class (non-targeted attack) or (2) a target class, chosen in advance by the attacker (targeted attack). For instance, the attacker could corrupt an image of an ostrich in such a way that it would be classified as a megalith. Even though the attacker*s agenda is not so clear in this example, it is still interesting to study the weaknesses of current systems in view of (1) improving them in general and (2) actual risks with e.g. autonomous vehicles. The paper is mostly experimental. In short, it compares different strategies (already published in previous papers) for all popular networks (VGG, GoogLeNet, ResNet-50/101/152) and the two aforementionned types of attacks. The experiments are well conducted and clearly exposed. A convincing point is that attacks are also conducted on *clarifai.com* which is a black-box classification system. Some analysis and insightful explanations are also provided to help understanding why CNNs are prone to such attacks (Section 6). To sum up, the main findings are that non-targeted attacks are easy to perform, even on a black-box system. Non-targeted attacks are more difficult to realize with existing schemes, but the authors propose a new approach for that that vastly improves over existing attacks (even though it*s still far from perfect: ~20% success rate on clarifai.com versus 2% with previous schemes). Arguably, The paper still has some weaknesses: - The authors are treating the 3 ResNet-based networks as different, yet they are obviously clearly correlated. See Table 7 for instance. This is naturally expected because their architecture is similar (only their depth varies). Hence, it does not sound very fair to state that *One interesting finding is that [...] the first misclassified label (non-targeted) is the same for all models except VGG-16 and GoogLeNet.*, i.e., the three ResNet-based networks. - A subjective measure is employed to evaluate the effectiveness of the attacks on the black box system. While this is for a good reason (clarifai.com returns image labels that are different from ImageNet), it is not certain that the reported numbers are fair (even though the qualitative results look convincing). - The novelty of the proposed approach (optimizing an ensemble of network instead of a single network) is limited. However, this was not really the point of the paper, and it is effective, so it seems ok overall. - The paper is quite long. This is expected because it is an extensive evaluation study, but still. I suggest the authors prune some near-duplicate content (e.g. Section 2.3 has a high overlap with Section 1, etc.). - The paper would benefit from additional discussions with the recent and related work of Fawzi et al (NIPS*16) in Section 6. Indeed the work of Fawzi et al. is mostly theoretical and well aligned with the experimental findings and observations (in particular in Section 6). To conclude, I think that this paper is somewhat useful for the community and could help to further improve existing architectures, as well as better assess their flaws and weaknesses.  I reviewed the manuscript as of December 7th. Summary: The authors investigate the transferability of adversarial examples in deep networks. The authors confirm that transferability exists even in large models but demonstrate that it is difficult to manipulate the network to adversarially perturb an image into a specifically desired label. The authors additionally demonstrate real world attacks on a vision web service and explore the geometric properties of adversarial examples. Major Comments: 1. The paper contains a list of many results and it is not clear what single message this paper provides. As mentioned in the comments, this paper is effectively 15 pages and 9 page of results in the Appendix heavily discussed throughout the main body of the paper. Although there is no strict page limit for this conference, I do feel this pushes the spirit of a conference publication. I do not rule out this paper for acceptance based on the length but I do hold it as a negative because clarity of presentation is an important quality. If this paper is ultimately accepted, I would suggest that the authors make some effort to cut down the length even further beyond the 13 pages posted elsewhere. I have marked some sections to highlight areas that may be trimmed. 2. The section of geometric understanding is similar to results of *Adversarial Perturbations of Deep Neural Networks* in Warde-Farley and Goodfellow (2015). See Figure 1.2. I am not clear what the authors show above-and-beyond these results. If there are additional findings, the authors should emphasize them. 3. The authors expand on observations by Goodfellow et al (2014) and Szegedy et al (2013) demonstrating that large-scale models are susceptible to adversarial perturbations (see also Kurakin et al (2016)). The authors additionally demonstrate that attempting to perform adversarial manipulation to convert an image to a particular, desired label is more difficult. 4. The authors demonstrate that they can target a real-world vision API. These results are compelling but it is not clear what these results demonstrate above-and-beyond Papernot et al (2016). As far I can understand, I think that the most interesting result from this paper not previously described in the literature is to note about the unique difficulty about performing adversarial manipulation to convert an image to a particular, desired label. The rest of the results appear to expand on other results that have already appeared in the literature and the authors need to better explain what these makes these results unique above-and-beyond previous work. Areas to Trim the Paper: - Table 1 is not necessary. Just cite other results or write the Top-1 numbers in the text. - Condense Section 2.2.1 and cite heavily. - Figure 2 panels may be overlaid to highlight a comparison.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",6,3,"Strengths: 1. The paper presents an interesting and very detailed study of targeted and non-targeted adversarial examples in CNNs. I’m on the fence about this paper but am leaning towards acceptance. Such detailed empirical explorations are difficult and time-consuming to construct yet can serve as important stepping stones for future work. I see the length of the paper as a strength since it allows for a very in-depth look into the effectiveness and transferability of different kinds of adversarial examples. There are, however, some concerns: 2. While the length of the paper is a strength in my mind, the key contributions should be made much more clear. As evidenced by my comment earlier, I got confused at some point between the ensemble/non-ensemble method, and about the contribution of the Clarifai evaluation and what I should be focusing on where. I’d strongly suggest a radical revision which more clearly focuses the story: - First, we demonstrate that non-targeted attacks are easy while targeted attacks are hard (evidenced by a key experiment comparing the two; we refer to appendix or later sections for the extensive exploration of e.g., current Section 3) - Thus, we propose an ensemble method that is able to handle targeted attacks much better (evidenced by experiments focusing on the comparison between ensemble and non-ensemble method, both in a controlled setting and on Clarifai) - Also, here are all the other details and explorations. 3. Instead of using"
"ParMAC: distributed optimisation of nested functions, with application to binary autoencoders | OpenReview","Many powerful machine learning models are based on the composition of multiple processing layers, such as deep nets, which gives rise to nonconvex objective functions. A general, recent approach to optimise such *nested* functions is the *method of auxiliary coordinates (MAC)*. MAC introduces an auxiliary coordinate for each data point in order to decouple the nested model into independent submodels. This decomposes the optimisation into steps that alternate between training single layers and updating the coordinates. It has the advantage that it reuses existing single-layer algorithms, introduces parallelism, and does not need to use chain-rule gradients, so it works with nondifferentiable layers. We describe ParMAC, a distributed-computation model for MAC. This trains on a dataset distributed across machines while limiting the amount of communication so it does not obliterate the benefit of parallelism. ParMAC works on a cluster of machines with a circular topology and alternates two steps until convergence: one step trains the submodels in parallel using stochastic updates, and the other trains the coordinates in parallel. Only submodel parameters, no data or coordinates, are ever communicated between machines. ParMAC exhibits high parallelism, low communication overhead, and facilitates data shuffling, load balancing, fault tolerance and streaming data processing. We study the convergence of ParMAC and its parallel speedup, and implement ParMAC using MPI to learn binary autoencoders for fast image retrieval, achieving nearly perfect speedups in a 128-processor cluster with a training set of 100 million high-dimensional points.","The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) (Carreira-Perpinan and Wang, 2012). This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between machines. Relatively good speedup factors are reported especially on larger datasets and a theoretical model of performance is presented that matches with the experiments. My main concern is that even though the method is presented as a general framework for nested functions, experiments focus on a restricted family of models (i.e. binary autoencoders with linear or kernel encoders and linear decoders) with only two components. While the speedup factors are encouraging, it is hard to get a sense of their importance as the binary autoencoder model considered is not well studied by other researchers and is not widely used. I encourage the authors to apply this framework to more generic architectures and problems. Questions: 1- Does this framework apply to some form of generic multi-layer neural network? If so, some experimental results are useful. 2- What is the implication of applying this framework to more than two components (an encoder and a decoder) and non-linear components? 3- It is desired to see a plot of performance as a function of time for different setups to demonstrate the speedup after convergence. It seems the paper only focuses on the speedup factors per iteration. For example, increasing the mini-batch size may improve the speed per iteration but may hurt the convergence speed. 4- Did you consider a scenario where the dataset is too big that storing the data and auxiliary variables on multiple machines simultaneously is not possible? The paper cites an ArXiv manuscript with the same title by the authors multiple times. Please make the paper self-contained and include any supplementary material in the appendix. I believe without applying this framework to a more generic architecture beyond binary autoencoders, this paper does not appeal to a wide audience at ICLR, hence weak reject.  UPDATE: I looked at the arxiv version of the paper. It is much longer and appears more rigorous. Fig 3 there is indeed more insightful. However, I am reviewing the submission and my overall assessment does not change. This is not a minor incremental contribution, and if you want to compress it into a conference submission of this type, I would recommend choosing message you want to convey, and focus on that. As you say, *...ICLR submission focus on the ParMAC algorithm...*, I would focus on this properly - and remove or move to appendix all extensions and theoretical remarks, and have an extra page on explaining the algorithm. Additionally, make sure to clearly explain the relation of the arxiv paper, in particular that the submission was a compressed version. ORIGINAL REVIEW: The submission proposes ParMAC, based on MAC (Method of Auxiliary Coordinates), formulating a distributed variant of the idea. Related Work: In the part on convex ERM and methods, I would recommend citing general communication efficient frameworks, COCOA (Ma et al.) and AIDE (Reddi et al.). I believe these works are most related to the practical objectives authors of this paper set, while number of the papers cited are less relevant. Section 2, explaining MAC, is quite clearly written, but I do not find part on MAC and EM particularly useful. Section 3 is much less clearly written. I have trouble following notation, particularly in the speedups part, as different symbols were introduced at different places. Perhaps a quick summary or paragraph on notation in the introduction would be helpful. In paragraph 2, you write as if reader knew how data/anything is distributed, but this was not mentioned yet; it is specified later. It is not clear what is meant by *submodel*. Perhaps a more precise example pointing back to eqs (1) & (2) would be useful. As far as I understand from what is written, there are P independent sets of submodels, that traverse the machines in circular fashion. I don*t understand how are they initialized (identically?), and more importantly I don*t understand what would be a single output of the algorithm (averaging? does not seem to make sense). Since this is not addressed, I suppose I get it wrong, leaving me to guess what was actually meant. The fact that I am not able to understand what is actually happening, I see as major issue. I don*t like the later paragraphs on extensions, model for speedup, convergence and topologies. I don*t understand whether these are novel contributions or not, as the authors refer to other work for details. If these are novel, the explanation is not sufficient, particularly speedup part, which contains undefined quantities, e.g. T(P) (or I can*t find it). If this is not novel, It does not provide enough explanation to understand anything more, compared with a its version compressed to 1/4 of its size and referring to the other work. The statement that we can recover the original convergence guarantees seems strong and I don*t see why it should be trivial to show (but author point to other work which I did not look at). In topologies part, claiming that something does *true SGD*, without explaining what is *true SGD* seems very strange. Other statements in this section seem also very vague and unjustified/unexplained. Experimental section seems to suggest that the method is interesting for binary autoencoders, but I don*t see how would I conclude anything about any other models. ParMAC is also not compared to alternative methods, only with itself, focusing on scaling properties. Conclusion contains statements that are too strong or misleading based on what I saw. In particular, *we analysed its parallel speedup and convergence* seems ungrounded. Further, the claim *The convergence properties of MAC remain essentially unaltered in ParMAC* is unsupported, regardless of the meaning of *essentially unchanged*. In summary, the method seems relevant for particular model class, binary autoencoders, but clarity of presentation is insufficient - I wouldn*t be able to recreate the algorithm used in experiments - and the paper contains a number of questionable claims.  This paper proposes an extension of the MAC method in which subproblems are trained on a distributed cluster arranged in a circular configuration. The basic idea of MAC is to decouple the optimization between parameters and the outputs of sub-pieces of the model (auxiliary coordinates); optimization alternates between updating the coordinates given the parameters and optimizing the parameters given the outputs. In the circular configuration. Because each update is independent, they can be massively parallelized. This paper would greatly benefit from more concrete examples of the sub-problems and how they decompose. For instance, can this be applied effectively for deep convolutional networks, recurrent models, etc? From a practical perspective, there*s not much impact for this paper beyond showing that this particular decoupling scheme works better than others. There also seem to be a few ideas worth comparing, at least: - Circular vs. parameter server configurations - Decoupled sub-problems vs. parallel SGD Parallel SGD also has the benefit that it*s extremely easy to implement on top of NN toolboxes, so this has to work a lot better to be practically useful. Also, it*s a bit hard to understand what exactly is being passed around from round to round, and what the trade-offs would be in a deep feed-forward network. Assuming you have one sub-problem for every hidden unit, then it seems like: 1. In the W step, different bits of the NN walk their way around the cluster, taking SGD steps w.r.t. the coordinates stored on each machine. This means passing around the parameter vector for each hidden unit. 2. Then there*s a synchronization step to gather the parameters from each submodel, requiring a traversal of the circular structure. 3. Then each machine updates it*s coordinates based on the complete model for a slice of the data. This would mean, for a feed-forward network, producing the intermediate activations of each layer for each data point. So for something comparable to parallel SGD, you could do the following: put a mini-batch of size B on each machine with ParMAC, compared to running such mini-batches in parallel. Completing steps 1-2-3 above would then be roughly equivalent to one synchronized PS type implementation step (distribute model to workers, get P gradients back, update model.) It would be really helpful to see how this compares in practice. It*s hard for me to understand intuitively why the proposed method is theoretically any better than parallel SGD (except for the issue of non-smooth function optimization); the decoupling also can fundamentally change the problem since you*re not doing back-propagation directly anymore, so that seems like it would conflate things as well and it*s not necessarily going to just work for other types of architectures.  This paper proposes a novel approach ParMAC, a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models which is based on the composition of multiple processing layers (i.e., deep nets). The basic idea of MAC to optimise the nested objective function, which is traditionally learned using methods based on the chain-rule gradients but inconvenient and is hard to parallelise, is to break nested functional relationships judiciously by introducing new variables ( the auxiliary coordinates) as equality constraints, and then to optimise a penalised function using alternating optimisation over the original parameters (W step) and over the coordinates (Z step). The minimisation (W step) updates the parameters by splitting the nested model into independent submodels and training them using existing algorithms, and the coordination (Z step) ensures that corresponding inputs and outputs of submodels eventually match. In this paper, the basic assumptions of ParMAC are that with large datasets in distributed systems, it is imperative to minimise data movement over the network because of the communication time generally far exceeds the computation time in modern architectures. Thus, the authors propose the ParMAC to translate the parallelism inherent in MAC into a distributed system by data parallelism and model parallelism. They also analyse its parallel speedup and convergence, and demonstrated it with MPI-based implementation to optimise binary autoencoders. The proposed ParMAC is tested on 3 colour image retrieval datasets. The organization of the paper is well written, and the presentation is clear. My questions are included in the following: - The MAC framework solves the original problem approximately. If people use the sigmoid function to smooth the stepwise function, the naive optimization methods can be easier applied. What is the difference between these two? Or why do we want to use a new approach to solve it? - The authors do not compare their ParMAC model with other distributed approaches for the same nested function optimization problem.","['Rating:###5: Marginally below acceptance threshold', 'Rating:###4: Ok but not good enough - rejection', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",5,4,"Strengths:
1. The paper presents a novel approach to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) (Carreira-Perpinan and Wang, 2012). 
2. The authors propose a distributed variant of the MAC method, which is based on the composition of multiple processing layers (i.e., deep nets). 
3. The paper is well-written and easy to follow. 

Weaknesses:
1. The paper focuses on a restricted family of models (i.e. binary autoencoders with linear or kernel encoders and linear decoders) with only two components. While the speedup factors are encouraging, it is hard to get a sense of their importance as the binary autoencoder model considered is not well studied by other researchers and is not widely used. 
2. The paper does not provide enough theoretical analysis of the convergence properties of the proposed method. 
3. The paper does not compare the proposed method with other distributed approaches for the same nested function optimization problem. 
4. The paper does not provide enough experimental results to support the claims made in the paper. 
5. The paper does not provide enough theoretical analysis of the convergence properties of the proposed method. 
6. The paper does not provide enough experimental results to support the claims made in the paper. 
7. The paper does not provide enough theoretical analysis of the convergence properties of the proposed method. 
8. The paper does not provide enough experimental results to"
Unrolled Generative Adversarial Networks | OpenReview,"We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator*s objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.","This work introduces a novel method for training GANs by displacing simultaneous SGD, and unrolling the inner optimization in the minmax game as a computational graph. The paper is very clearly written, and explains the justification very well. The problem being attacked is very significant and important. The approach is novel, however, similar ideas have been tried to solve problems unrelated to GANs. The first quantitative experiment is section 3.3.1, where the authors attempt to find the best z which can generate training examples. This is done by using L-BFGS on |G(z) - x|. The claim is that if we*re able to find such a z, then the generator can generate this particular training example. It*s demonstrated that 0-step GANs are not able to generate many training examples, while unrolled GANs do. However, I find this experiment unreasonable. Being able to find a certain z, which generates a certain sample does not guarantee that this particular mode is high probability. In fact, an identity function can potentially beat all the GAN models in the proposed metric. And due to Cantor*s proof of equivalence between all powers of real spaces, this applies to smaller dimension of z as well. More realistically, it should be possible to generate *any* image from a generator by finding a very specific z. That a certain z exists which can generate a sample does not prove that the generator is not missing modes. It just proves that the generator is similar enough to an identity function to be able to generate any possible image. This metric is thus measuring something potentially tangential to diversity or mode-dropping. Another problem with this metric is that that showing that the optimization is not able to find a z for a specific training examples does not prove that such a z does not exist, only that it*s harder to find. So, this comparison might just be showing that unrolled GANs have a smoother function than 0-step GANs, and thus easier to optimize for z. The second quantitative experiment considers mean pairwise distance between generated samples, and between data samples. The first number is likely to be small in the case of a mode-dropping GAN. The authors argue that the two numbers being closer to each other is an indication of the generated samples being as diverse as the data. Once again, this metric is not convincing. 1. The distances are being measured in pixel-space. 2. A GAN model could be generating garbage, and yet still perform very well in this metric. There are no other quantitative results in the paper. Even though the method is optimizing diversity, for a sanity check, scores for quality such as Inception scores or SSL performance would have been useful. Another metric that the authors can consider is training GAN using this approach on the tri-MNIST dataset (concatenation of 3 MNIST digits), which results in 1000 easily-identifiable modes. Then, demonstrate that the GAN is able to generate all the 1000 modes with equal probability. This is not a perfect metric either, but arguably much better than the metrics in this paper. This metric is used in this ICLR submission: https://openreview.net/pdf?id=HJKkY35le Whether this paper is accepted or not, I encourage the authors to investigate this approach further, since the method is promising and interesting. # Post-rebuttal review The authors have incorporated changed in the paper by adding more experiments. These experiments now demonstrate the claims of the paper better. The paper was already well-written and introduced a novel idea and addressed an important problem. The only thing holding this paper back was unconvincing experiments, which now has been corrected. Thus, I would increase my score by 2 points, and recommend accepting the paper.  The paper presents an approach for tackling the instability problem that is present in generative adversarial networks. The general idea is to allow the generator to *peek ahead* at how the discriminator will evolve its decision boundary over-time with the premise that this information should prevent the generator from collapsing to produce only samples from a single mode of the data distribution. This is a very well written paper that clearly motivates its attack on an important open issue. The experiments are well carried out and strongly support the presented idea. The pursued approach is substantially more elegant than current existing *hacks* that are commonly used to make GANs work in practice. I however have three main issues that let me partly doubt the success of the method. If these can be resolved this paper is a clear candidate for acceptance. 1) I am not entirely convinced that the same effect cannot be obtained by the following procedure: simply train the discriminator for an extended number of K steps when updating the generator (say a number equivalent to the unrolling steps used in the current experiments) then, after the generator was updated undo the K updates to the discriminator and do 1 new update step instead. I only briefly glanced at your response to Reviewer2 which seems to imply you now tried something similar to this setup by stopping gradient flow at an appropriate point (although I think this is not exactly equivalent). 2) I tried to reproduce the simple MNIST example but using a fully connected network instead of an RNN generator without much success. Even when unrolling the discriminator for 30-40 steps the generator still engages in mode seeking behavior or does not train at all. This could either be because of a bug in my implementation or because of some peculiarities of the RNN generator or because I did not use batch normalization anywhere. If it is one of the latter two this would entail a dependence of the proposed approach on specific forms of the discriminator and generator and should be discussed. My code can be found here https://github.com/iclrreproducer/unrolled_gan -- see the comments in the file for switching on unrolling and batch normalization in the generator. This issue could also be addressed simply by opening up the code and setting up a fully connected network example on your side. 3) For the more complicated data distributions the method is only used in combination with many of the existing tricks for training GANs. As a result the experiments are much less convincing. I personally cannot see a strong difference between the samples generated by any of the CIFAR-10 models. Why did you not try to remove batch-normalization etc. for these examples ? Does training then again become unstable ? ---- UPDATE ----- I am leaving the initial review intact to preserve the conversation. After the authors response all points have been resolved, including a fix to my re-implementation of the ideas presented in the paper. As a result I have increased my score and believe the paper should be accepted. Please also see my response to the rebuttal below.  The paper introduces a technique for stabilizing the training of Generative Adversrial Networks by unrolling the inner (discriminator) optimization in the GAN loss function several steps and optimizing the generator with respect to the final state of this optimization process. The experimental evidence that this actually helps is very compelling: the 2d example shows a toy problem where this technique helps substantially, the LSTM MNIST generator example shows that the procedure helps with stabilizing the training of an unusual architecture of generator, and the image generation experiment, while not being definitive, is very convincing. For future work it would be interesting to see whether a method with smaller memory requirements could be devised based on similar principles. I strongly recommend to accept this paper.","['Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept', 'Rating:###9: Top 15% of accepted papers, strong accept']","['Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",7,5,"Strengths:
1. The paper presents a novel approach to training GANs by unrolling the inner optimization in the GAN loss function several steps and optimizing the generator with respect to the final state of this optimization process. The experimental evidence that this actually helps is very compelling: the 2d example shows a toy problem where this technique helps substantially, the LSTM MNIST generator example shows that the procedure helps with stabilizing the training of an unusual architecture of generator, and the image generation experiment, while not being definitive, is very convincing.
2. The paper is well-written and easy to follow. The authors clearly motivate their attack on an important open issue and provide a clear explanation of the method.
Weaknesses:
1. I am not entirely convinced that the same effect cannot be obtained by the following procedure: simply train the discriminator for an extended number of K steps when updating the generator (say a number equivalent to the unrolling steps used in the current experiments) then, after the generator was updated undo the K updates to the discriminator and do 1 new update step instead. I only briefly glanced at your response to Reviewer2 which seems to imply you now tried something similar to this setup by stopping gradient flow at an appropriate point (although I think this is not exactly equivalent).
2. I tried to reproduce the simple MNIST example but using a fully connected network instead of an RNN generator without much success. Even when unrolling the discriminator for 30-40 steps the generator"
Unsupervised Pretraining for Sequence to Sequence Learning | OpenReview,"This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. Our main result is that the pretraining accelerates training and improves generalization of seq2seq models, achieving state-of-the-art results on the WMT English->German task, surpassing a range of methods using both phrase-based machine translation and neural machine translation. Our method achieves an improvement of 1.3 BLEU from the previous best models on both WMT*14 and WMT*15 English->German. On summarization, our method beats the supervised learning baseline.","In this paper, the authors propose to pretrain the encoder/decoder of seq2seq models on a large amount of unlabeled data using a LM objective. They obtain improvements using this technique on machine translation and abstractive summarization. While the effectiveness of pretraining seq2seq models has been known among researchers and explored in a few papers (e.g. Zoph et al. 2016, Dai and Le 2015), I believe this is the first paper to pretrain using a LM for both the encoder/decoder. The technique is simple, but the gains are large (e.g. +2.7 BLEU on NMT). In addition, the authors perform extensive ablation studies to analyze where the performance is coming from. Hence, I think this paper should be accepted.  strengths: A method is proposed in this paper to initialize the encoder and decoder of the seq2seq model using the trained weights of language models with no parallel data. After such pretraining, all weights are jointly fine-tuned with parallel labeled data with an additional language modeling loss. It is shown that pretraining accelerates training and improves generalization of seq2seq models. The main value of the proposed method is to leverage separate source and target corpora, contrasting the common methods of using large amounts of parallel training corpora. weaknesses: The objective function shown in the middle of pg 3 is highly empirical, not directly linked to how non-parallel data helps to improve the final prediction results. The paper should compare with and discuss the objective function based on expectation of cross entropy which is directly linked to improving prediction results as proposed in arXiv:1606.04646, Chen et al.: Unsupervised Learning of Predictors from Unpaired Input-Output Samples, 2016. The pre-training procedure proposed in this paper is also closely connected with the DNN pretraining method presented in Dahl et al. 2011, 2012. Comparisons should be made in the paper, highlighting why the proposed one is conceptually superior if the authors believe so.  Authors propose the use of layer-wise language model-like pretraining for encoder-decoder models. This allows to leverage separate source and target corpora (in unsupervised manner) without necessity of large amounts of parallel training corpora. The idea is in principle fairly simple, and rely on initial optimising both encoder and decoder with LSTM tasked to perform language modelling. The ideas are not new, and the paper is more like a successful compilation of several approaches that have been around for some time. The experimental validation, though, offers some interesting insights into importance of initialization, and the effectiveness of different initialisations approaches in enc-dec setting. The regulariser you propose to use on page 3, looks like typical multi-task objective function, especially it is used in an alternating manner would be interesting to see whether similar performance might have been obtained starting with this objective, from random initialisation. You should probably give credit for encoder-decoder like-RNN models published in 1990s. Minors: Pg. 2, Sec 2.1 2nd paragraph: can be different sizes -> can be of different sizes","['Rating:###7: Good paper, accept', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",7,5,"Strengths: The authors propose a simple and effective method to pretrain the encoder and decoder of a sequence-to-sequence model. The method is based on the idea of using a language model to pretrain the model, and the authors show that this can improve the performance of the model on a variety of tasks. The paper also includes a good discussion of the theoretical background of the method, and the authors provide a good analysis of the results. The experimental results are also well presented and clearly explained. The paper is well written and easy to follow. The authors also provide a good discussion of the limitations of the method and the potential for future work. 

Weaknesses: The paper could benefit from a more detailed discussion of the theoretical background of the method. The authors mention that the method is based on the idea of using a language model to pretrain the model, but they do not provide a clear explanation of why this is effective. Additionally, the paper could benefit from a more detailed discussion of the potential limitations of the method. For example, the authors mention that the method may not be effective for tasks that require a large amount of parallel data, but they do not provide a clear explanation of why this is the case. The paper could also benefit from a more detailed discussion of the potential for future work. For example, the authors mention that the method could be extended to other sequence-to-sequence models, but they do not provide a clear explanation of how this would work. 

Rating:"
DSD: Dense-Sparse-Dense Training for Deep Neural Networks | OpenReview,"Modern deep neural networks have a large number of parameters, making them very hard to train. We propose DSD, a dense-sparse-dense training flow, for regularizing deep neural networks and achieving better optimization performance. In the first D (Dense) step, we train a dense network to learn connection weights and importance. In the S (Sparse) step, we regularize the network by pruning the unimportant connections with small weights and retraining the network given the sparsity constraint. In the final D (re-Dense) step, we increase the model capacity by removing the sparsity constraint, re-initialize the pruned parameters from zero and retrain the whole dense network. Experiments show that DSD training can improve the performance for a wide range of CNNs, RNNs and LSTMs on the tasks of image classification, caption generation and speech recognition. On ImageNet, DSD improved the Top1 accuracy of GoogLeNet by 1.1%, VGG-16 by 4.3%, ResNet-18 by 1.2% and ResNet-50 by 1.1%, respectively. On the WSJ’93 dataset, DSD improved DeepSpeech and DeepSpeech2 WER by 2.0% and 1.1%. On the Flickr-8K dataset, DSD improved the NeuralTalk BLEU score by over 1.7. DSD is easy to use in practice: at training time, DSD incurs only one extra hyper-parameter: the sparsity ratio in the S step. At testing time, DSD doesn’t change the network architecture or incur any inference overhead. The consistent and significant performance gain of DSD experiments shows the inadequacy of the current training methods for finding the best local optimum, while DSD effectively achieves superior optimization performance for finding a better solution. DSD models are available to download at https://songhan.github.io/DSD.","Training highly non-convex deep neural networks is a very important practical problem, and this paper provides a great exploration of an interesting new idea for more effective training. The empirical evaluation both in the paper itself and in the authors’ comments during discussion convincingly demonstrates that the method achieves consistent improvements in accuracy across multiple architectures, tasks and datasets. The algorithm is very simple (alternating between training the full dense network and a sparse version of it), which is actually a positive since that means it may get adapted in practice by the research community. The paper should be revised to incorporate the additional experiments and comments from the discussion, particularly the accuracy comparisons with the same number of epochs.  Summary: The paper proposes a model training strategy to achieve higher accuracy. The issue is train a too large model and you going to over-fit and your model will capture noise. Prune models or make it too small then it will miss important connections and under-fit. Thus, the proposed method involves various training steps: first they train a dense network, then prune it making it sparse then train a sparse network and finally they add connections back and train the model as dense again (DSD). The DSD method is generic method that can be used in CNN/RNN/LSTM. The reasons why models have better accuracy after DSD are: escape of saddle point, sparsity makes model more robust to noise and symmetry break allowing richer representations. Pro: The main point that this paper wants to show is that a model has the capacity to achieve higher accuracy, because it was shown that it is possible to compress a model without losing accuracy. And lossless compression means that there’s significant redundancy in the models that were trained using current training methods. This is an important observation that large models can get better accuracies as better training schemes are used. Cons & Questions: The issue is that the accuracy is slightly increased (2 or 3%) for most models. And the question is what is the price paid for this improvement? Resource and performance concerns arises because training a large model is computationally expensive (hours or even days using high performance GPUs). Second question, can I keep adding Dense, Sparse and Dense training iterations to get higher and higher accuracy improvement? Are there limitations to this DSDSD… approach?  This paper presents a training strategy for deep networks. First, the network is trained in a standard fashion. Second, small magnitude weights are clamped to 0; the rest of the weights continue to be trained. Finally, all the weights are again jointly trained. Experiments on a variety of image, text, and speech datasets demonstrate the approach can obtain high-quality results. The proposed idea is novel and interesting. In a sense it is close to Dropout, though as noted in the paper the deterministic weight clamping method is different. The main advantage of the proposed method is its simplicity. Three hyper-parameters are needed: the number of weights to clamp to 0, and the numbers of epochs of training used in the first dense phase and the sparse phase. Given these, it can be plugged in to training a range of networks, as shown in the experiments. The concern I have is regarding the current empirical evaluation. As noted in the question phase, it seems the baseline methods are not trained for as many epochs as the proposed method. Standard tricks, such as dropping the learning rate upon *convergence* and continuing to learn, can be employed. The response seems to indicate that these approaches can be effective. I think a more thorough empirical analysis of performance over epochs, learning rates, etc. would strengthen the paper. An exploration regarding the sparsity hyper-parameter would also be interesting.","['Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",8,3,Strengths: 1. The paper proposes a novel training strategy for deep neural networks. 2. The idea is simple and easy to implement. 3. The paper provides a good empirical evaluation of the proposed method. Weaknesses: 1. The paper does not provide a clear theoretical explanation of the proposed method. 2. The paper does not provide a clear theoretical explanation of the proposed method. 3. The paper does not provide a clear theoretical explanation of the proposed method. Rating: 6 Confidence: 3
NEWSQA: A MACHINE COMPREHENSION DATASET | OpenReview,"We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four- stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.","It would seem that the shelf life of a dataset has decreased rapidly in recent literature. SQuAD dataset has been heavily pursued as soon as it hit online couple months ago, the best performance on their leaderboard now reaching to 82%. This is rather surprising when taking into account the fact that the formal conference presentation of the dataset took place only a month ago at EMNLP’16, and that the reported machine performance (at the time of paper submission) was only at 51%. One reasonable speculation is that the dataset may have not been hard enough. NewsQA, the paper in submission, aims to address this concern by presenting a dataset of a comparable scale created through different QA collection strategies. Most notably, the authors solicit questions without requiring answers from the same turkers, in order to promote more diverse and hard-to-answer questions. Another notable difference is that the questions are gathered without showing the content of the news articles, and the dataset makes use of a bigger subset of CNN/Daily corpus (12K / 90K), as opposed to a much smaller subset (500 / 90K) used by SQuAD. In sum, I think NewsQA dataset presents an effort to construct a harder, large-scale reading comprehension challenge, a recently hot research topic for which we don’t yet have satisfying datasets. While not without its own weaknesses, I think this dataset presents potential values compared to what are available out there today. That said, the paper does read like it was prepared in a hurry, as there are numerous small things that the authors could have done better. As a result, I do wonder about the quality of the dataset. For one, human performance of SQuAD measured by the authors (70.5 - 82%) is lower than that reported by SQuAD (80.3 - 90.5%). I think this sort of difference can easily happen depending on the level of carefulness the annotators can maintain. After all, not all humans have the same level of carefulness or even the same level of reading comprehension. I think it’d be the best if the authors can try to explain the reason behind these differences, and if possible, perform a more careful measurement of human performance. If anything, I don’t think it looks favorable for NewsQA if the human performance is only at the level of 74.9%, as it looks as if the difficulty of the dataset comes mainly from the potential noise from the QA collection process, which implies that the low model performance could result from not necessarily because of the difficulty of the comprehension and reasoning, but because of incorrect answers given by human annotators. I’m also not sure whether the design choice of not presenting the news article when soliciting the questions was a good one. I can imagine that people might end up asking similar generic questions when not enough context has been presented. Perhaps taking a hybrid, what I would like to suggest is to present news articles where some sentences or phrases are randomly redacted, so that the question generators can have a bit more context while not having the full material in front of them. Yet another way of encouraging the turkers from asking too trivial questions is to engage an automatic QA system on the fly — turkers must construct a QA pair for which an existing state-of-the-art system cannot answer correctly.  Paper Summary: This paper presents a new comprehension dataset called NewsQA dataset, containing 100,000 question-answer pairs from over 10,000 news articles from CNN. The dataset is collected through a four-stage process -- article filtering, question collection, answer collection and answer validation. Examples from the dataset are divided into different types based on answer types and reasoning required to answer questions. Human and machine performances on NewsQA are reported and compared with SQuAD. Paper Strengths: -- I agree that models can benefit from diverse set of datasets. This dataset is collected from news articles, hence might pose different sets of problems from current popular datasets such as SQuAD. -- The proposed dataset is sufficiently large for data hungry deep learning models to train. -- The inclusion of questions with null answers is a nice property to have. -- A good amount of thought has gone into formulating the four-stage data collection process. -- The proposed BARB model is performing as good as a published state-of-the-art model, while being much faster. Paper Weaknesses: -- Human evaluation is weak. Two near-native English speakers* performance on 100 examples each can hardly be a representative of the complete dataset. Also, what is the model performance on these 200 examples? -- Not that it is necessary for this paper, but to clearly demonstrate that this dataset is harder than SQuAD, the authors should either calculate the human performance the same way as SQuAD or calculate human performances on both NewsQA and SQuAD in some other consistent manner on large enough subsets which are good representatives of the complete datasets. Dataset from other communities such as VQA dataset (Antol et al., ICCV 2015) also use the same method as SQuAD to compute human performance. -- Section 3.5 says that 86% of questions have answers agreed upon by atleast 2 workers. Why is this number inconsistent with the 4.5% of questions which have answers without agreement after validation (last line in Section 4.1)? -- Is the same article shown to multiple Questioners? If yes, is it ensured that the Questioners asking questions about the same article are not asking the same/similar questions? -- Authors mention that they keep the same hyperparameters as SQuAD. What are the accuracies if the hyperparameters are tuned using a validation set from NewsQA? -- 500 examples which are labeled for reasoning types do not seem enough to represent the complete dataset. Also, what is the model performance on these 500 examples? -- Which model*s performance has been shown in Figure 1? -- Are the two *students* graduate/undergraduate students or researchers? -- Test set seems to be very small. -- Suggestion: Answer validation step is nice, but maybe the dataset can be released in 2 versions -- one with all the answers collected in 3rd stage (without the validation step), and one in the current format with the validation step. Preliminary Evaluation: The proposed dataset is a large scale machine comprehension dataset collected from news articles, which in my suggestion, is diverse enough from existing datasets that state-of-the-art models can definitely benefit from it. With a better human evaluation, I think this paper will make a good poster.  Summary: The paper proposes a novel machine comprehension dataset called NEWSQA. The dataset consists of over 100,000 question answer pairs based on over 10,000 news articles from CNN. The paper analyzes the different types of answers and the different types of reasoning required to answer questions in the dataset. The paper evaluates human performance and the performance of two baselines on the dataset and compares them with the performance on SQuAD dataset. Strengths: 1. The paper presents a large scale dataset for machine comprehension. 2. The question collection method seems reasonable to collect exploratory questions. Having an answer validation step is desirable. 3. The paper proposes a novel (computationally more efficient) implementation of the match-LSTM model. Weaknesses: 1. The human evaluation presented in the paper is not satisfactory because the human performance is reported on a very small subset (200 questions). So, it seems unlikely that these 200 questions will provide a reliable measure of the human performance on the entire dataset (which consists of thousands of questions). 2. NEWSQA dataset is very similar to SQuAD dataset in terms of the size of the dataset, the type of dataset -- natural language questions posed by crowdworkers, answers comprising of spans of text from related paragraphs. The paper presents two empirical ways to show that NEWSQA is more challenging than SQuAD -- 1) the gap between human and machine performance in NEWSQA is larger than that in SQuAD. However, since the human performance numbers are reported on very small subset, these trends might not carry over when human performance is computed on all of the dataset. 2) the sentence-level accuracy on SQuAD is higher than that in NEWSQA. However, as the paper mentions, the differences in accuracies could likely be due to different lengths of documents in the two datasets. So, even this measure does not truly reflect that SQuAD is less challenging than NEWSQA. So, it is not clear if NEWSQA is truly more challenging than SQuAD. 3. Authors mention that BARB is computationally more efficient and faster compared to match-LSTM. However, the paper does not report how much faster BARB is compared to match-LSTM. 4. On page 7, under *Boundary pointing* paragraph, the paper should clarify what *s* in *n_s* refers to. Review summary: While the dataset collection method seems interesting and promising, I would be more convinced after I see the following -- 1. Human performance on all (or significant percentage of the dataset). 2. An empirical study that fairly shows that NEWSQA is more challenging (or better in some other way) than SQuAD.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",6,4,"Strengths: 1. The paper presents a large scale dataset for machine comprehension. 2. The question collection method seems reasonable to collect exploratory questions. Having an answer validation step is desirable. 3. The paper proposes a novel (computationally more efficient) implementation of the match-LSTM model. Weaknesses: 1. The human evaluation presented in the paper is not satisfactory because the human performance is reported on a very small subset (200 questions). So, it seems unlikely that these 200 questions will provide a reliable measure of the human performance on the entire dataset (which consists of thousands of questions). 2. NEWSQA dataset is very similar to SQuAD dataset in terms of the size of the dataset, the type of dataset -- natural language questions posed by crowdworkers, answers comprising of spans of text from related paragraphs. The paper presents two empirical ways to show that NEWSQA is more challenging than SQuAD -- 1) the gap between human and machine performance in NEWSQA is larger than that in SQuAD. However, since the human performance numbers are reported on very small subset, these trends might not carry over when human performance is computed on all of the dataset. 2) the sentence-level accuracy on SQuAD is higher than that in NEWSQA. However, as the paper mentions, the differences in accuracies could likely be due to different lengths of documents in the two datasets. So, even this measure does not truly reflect that S"
Normalizing the Normalizers: Comparing and Extending Network Normalization Schemes | OpenReview,"Normalization techniques have only recently begun to be exploited in supervised learning tasks. Batch normalization exploits mini-batch statistics to normalize the activations. This was shown to speed up training and result in better models. However its success has been very limited when dealing with recurrent neural networks. On the other hand, layer normalization normalizes the activations across all activities within a layer. This was shown to work well in the recurrent setting. In this paper we propose a unified view of normalization techniques, as forms of divisive normalization, which includes layer and batch normalization as special cases. Our second contribution is the finding that a small modification to these normalization schemes, in conjunction with a sparse regularizer on the activations, leads to significant benefits over standard normalization techniques. We demonstrate the effectiveness of our unified divisive normalization framework in the context of convolutional neural nets and recurrent neural networks, showing improvements over baselines in image classification, language modeling as well as super-resolution.","The authors present a unified framework for various divisive normalization schemes, and then show that a somewhat novel version of normalization does somewhat better on several tasks than some mid-strength baselines. Pros: * It has seemed for a while that there are a bunch of different normalization methods out there, of varying importance in varying applications, so having a standardized framework for them all, and evaluating them carefully and systematically, is a very useful contribution. * The paper is clearly written. * From an architectural standpoint, the actual comparisons seem well motivated. (For instance, I*m glad they tried DN* and BN* -- if they hadn*t tried those, I would have wanted them too.) Cons: * I*m not really sure what the difference is between their new DN method and standard cross-channel local contrast normalization. (Oh, actually -- looking at the other reviews, everyone else seems to have noticed this too. I*ll not beat a dead horse about this any further.) * I*m nervous that the conclusions that they state might not hold on larger, stronger tasks, like ImageNet, and with larger deeper models. I myself have found that while with smaller models on simpler tasks (e.g. Caltech 101), contrast normalization was really useful, that it became much less useful for larger architectures on larger tasks. In fact, if I recall correctly, the original AlexNet model had a type of cross-unit normalization in it, but this was dispensed with in more recent models (I think after Zeiler and Fergus 2013) largely because it didn*t contribute that much to performance but was somewhat expensive computationally. Of course, batch normalization methods have definitely been shown to contribute performance on large problems with large models, but I think it would be really important to show the same with the DN methods here, before any definite conclusion could be reached.  This paper empirically studies multiple combinations of various tricks to improve the performance of deep neural networks on various tasks. Authors investigate various combinations of normalization techniques together with additional regularizations. The paper makes few interesting empirical observations, such that the L1 regularizer on top of the activations is relatively useful for most of the tasks. In general, it seems that this work can be significantly improved by providing more precise study of existing normalization techniques. Also, studying more closely the overall volumes of the summation and suppression fields (e.g. how many samples one needs to collect for a robust enough normalization) would be useful. In more detail, the work seems to have the following issues: * Divisive normalization, is used extensively in Krizhevsky12 (LRN). It is almost exactly the same definition as in equation 1, however with slightly different constants. Therefore claiming that it is less explored is questionable. * It is not clear whether the Divisive normalization does subtract the mean from the activation as there is a contradiction in its definition in equation 1 and 3. This questions whether the *General Formulation of Normalization* is correct. * In seems that Divisive normalization is used also in Jarrett09, called Contrast Normalization, with a definition more similar to equation 3 (subtracting the mean). * In case of the RNN experiments, it would be more clear to provide the absolute size of the summation and suppression field as BN may be inferior to DN due to a small batch size. * It is unclear what and how are measured the results shown in Table 10. Also it is unclear what are the sizes of the suppression/summation fields for the CIFAR and Super Resolution experiments. Minor, relatively irrelevant issues: * It is usually better to pick a stronger baseline for the tasks. The selected CIFAR model from Caffe seems to be quite far from the state of the art on the CIFAR dataset. A stronger baseline (e.g. the widely available ResNet) would allow to see whether the proposed techniques are useful for the more recent models as well. * Double caption for Table 7/8.  *** Paper Summary *** This paper proposes a unified view on normalization. The framework encompases layer normalization, batch normalization and local contrast normalization. It also suggests decorrelating the inputs through L1 regularization of the activations. Results are reported on three tasks: CIFAR classification, PTB Language models and super resolution on Berkeley dataset. *** Review Summary *** Overall, I feel it is good to refresh the community about local normalization schemes and other mechanism to favor unit competitions. The paper reads well and reports results on various setups, with sufficient discussion. *** Detailed Review *** The paper is clear and reads well. It lacks a few reference to prior research. Also I am surprised that *Local Contrast Normalization* is not said anywhere, as it is a common terminology in the neural network and vision literature. It is unclear to me why you chose to pair L1 regularization of the activation and normalization. They seem complementary. Would it make sense to apply L1 regularization to the baseline to highlight it is helpful on its own. Overall, it seems the only thing that brings a consistent improvement across all setups. On related work, maybe it would be worthwhile to insist that Local Contrast Normalization (LCN) used to be very popular [Pinto et al, 2008, Jarret et al 2009, Sermanet et al 2012; Quoc Le 2013] and effective. It is great to connect this litterature to current work on layer normalization and batch normalization. Similarly, sparsity or group sparsity of the activation has shown effective in the past [Rozell et al 08, Kavukcuoglu et al 09] and need more exposure today. Finally, since dropout is so popular but interact poorly with normalizer estimates, I feel it would be worthwhile to report results with dropout beyond the baseline and discuss how the different normalization scheme interact with it. Overall, I feel it is good to refresh the community about local normalization schemes and other mechanism to favor unit competitions. The paper reads well and reports results on various setups, with sufficent discussion. *** References *** Jarrett, Kevin, Koray Kavukcuoglu, and Yann Lecun. *What is the best multi-stage architecture for object recognition?.* 2009 IEEE 12th International Conference on Computer Vision. IEEE, 2009. Pinto, N., Cox, D., DiCarlo, J.: Why is real-world visual object recognition hard? PLoS Comput Biol 4 (2008) Le, Quoc V. *Building high-level features using large scale unsupervised learning.* 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neural networks applied to house numbers digit classification. In ICPR, 2012. C. Rozell, D. Johnson, and B. Olshausen. Sparse coding via thresholding and local competition in neural circuits.Neural Computation, 2008. K. Kavukcuoglu, M. Ranzato, R. Fergus, and Y. LeCun. Learning invariant features through topographic filter maps. In CVPR, 2009.","['Rating:###7: Good paper, accept', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###9: Top 15% of accepted papers, strong accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",7,4,"Strengths: 1. The paper presents a unified framework for various normalization schemes, which is a good contribution. 2. The paper is clear and well-written. Weaknesses: 1. The paper seems to be somewhat unclear about the difference between the proposed normalization schemes. 2. The paper does not provide a clear explanation of the results in Table 10. 3. The paper does not provide a clear explanation of the results in Table 7/8. 4. The paper does not provide a clear explanation of the results in Table 8. 5. The paper does not provide a clear explanation of the results in Table 7. 6. The paper does not provide a clear explanation of the results in Table 9. 7. The paper does not provide a clear explanation of the results in Table 10. 8. The paper does not provide a clear explanation of the results in Table 8. 9. The paper does not provide a clear explanation of the results in Table 7. 10. The paper does not provide a clear explanation of the results in Table 9. Rating: 6 Confidence: 3"
Decomposing Motion and Content for Natural Video Sequence Prediction | OpenReview,"We propose a deep neural network for the prediction of future frames in natural video sequences. To effectively handle complex evolution of pixels in videos, we propose to decompose the motion and content, two key components generating dynamics in videos. Our model is built upon the Encoder-Decoder Convolutional Neural Network and Convolutional LSTM for pixel-level prediction, which independently capture the spatial layout of an image and the corresponding temporal dynamics. By independently modeling motion and content, predicting the next frame reduces to converting the extracted content features into the next frame content by the identified motion features, which simplifies the task of prediction. Our model is end-to-end trainable over multiple time steps, and naturally learns to decompose motion and content without separate training. We evaluate the pro- posed network architecture on human activity videos using KTH, Weizmann action, and UCF-101 datasets. We show state-of-the-art performance in comparison to recent approaches. To the best of our knowledge, this is the first end-to-end trainable network architecture with motion and content separation to model the spatio-temporal dynamics for pixel-level future prediction in natural videos.","This paper introduces an approach for future frame prediction in videos by decoupling motion and content to be encoded separately, and additionally using multi-scale residual connections. Qualitative and quantitative results are shown on KTH, Weizmann, and UCF-101 datasets. The idea of decoupling motion and content is interesting, and seems to work well for this task. However, the novelty is relatively incremental given previous cited work on multi-stream networks, and it is not clear that this particular decoupling works well or is of broader interest beyond the specific task of future frame prediction. While results on KTH and Weizmann are convincing and significantly outperform baselines, the results are less impressive on less constrained UCF-101 dataset. The qualitative examples for UCF-101 are not convincing, as discussed in the pre-review question. Overall this is a well-executed work with an interesting though not extremely novel idea. Given the limited novelty of decoupling motion and content and impact beyond the specific application, the paper would be strengthened if this could be shown to be of broader interest e.g. for other video tasks.  The paper presents a method for predicting video sequences in the lines of Mathieu et al. The contribution is the separation of the predictor into two different networks, picking up motion and content, respectively. The paper is very interesting, but the novelty is low compared to the referenced work. As also pointed out by AnonReviewer1, there is a similarity with two-stream networks (and also a whole body of work building on this seminal paper). Separating motion and content has also been proposed for other applications, e.g. pose estimation. Details : The paper can be clearly understood if the basic frameworks (like GANs) are known, but the presentation is not general and good enough for a broad public. Example : Losses (7) to (9) are well known from the Matthieu et al. paper. However, to make the paper self-contained, they should be properly explained, and it should be mentioned that they are *additional* losses. The main target is the GAN loss. The adversarial part of the paper is not properly enough introduced. I do agree, that adversarial training is now well enough known in the community, but it should still be properly introduced. This also involves the explanation that L_Disc is the loss for a second network, the discriminator and explaining the role of both etc. Equation (1) : c is not explained (are these motion vectors)? c is also overloaded with the feature dimension c*. The residual nature of the layer should be made more apparent in equation (3). There are several typos, absence of articles and prepositions (*of* etc.). The paper should be reread carefully.  1) Summary This paper investigates the usefulness of decoupling appearance and motion information for the problem of future frame prediction in natural videos. The method introduces a novel two-stream encoder-decoder architecture, MCNet, consisting of two separate encoders -- a convnet on single frames and a convnet+LSTM on sequences of temporal differences -- followed by combination layers (stacking + convolutions) and a deconvolutional network decoder leveraging also residual connections from the two encoders. The architecture is trained end-to-end using the objective and adversarial training strategy of Mathieu et al. 2) Contributions + The architecture seems novel and is well motivated. It is also somewhat related to the two-stream networks of Simonyan & Zisserman, which are very effective for real-world action recognition. + The qualitative results are numerous, insightful, and very convincing (including quantitatively) on KTH & Weizmann, showing the benefits of decoupling content and motion for simple scenes with periodic motions, as well as the need for residual connections. 3) Suggestions for improvement Static dataset bias: In response to the pre-review concerns about the observed static nature of the qualitative results, the authors added a simple baseline consisting in copying the pixels of the last observed frame. On the one hand, the updated experiments on KTH confirm the good results of the method in these conditions. On the other hand, the fact that this baseline is better than all other methods (not just the authors*s) on UCF101 casts some doubts on whether reporting average statistics on UCF101 is insightful enough. Although the authors provide some qualitative analysis pertaining to the quantity of motion, further quantitative analysis seems necessary to validate the performance of this and other methods on future frame prediction. At least, the results on UCF101 should be disambiguated with respect to the type of scene, for instance by measuring the overall quantity of motion (e.g., l2 norm of time differences) and reporting PSNR and SSIM per quartile / decile. Ideally, other realistic datasets than UCF101 should be considered in complement. For instance, the Hollywood 2 dataset of Marszalek et al would be a good candidate, as it focuses on movies and often contains complex actor, camera, and background motions that would make the *pixel-copying* baseline very poor. Experiments on video datasets beyond actions, like the KITTI tracking benchmark, would also greatly improve the paper. Additional recognition experiments: As mentioned in pre-review questions, further UCF-101 experiments on action recognition tasks by fine-tuning would also greatly improve the paper. Classifying videos indeed requires learning both appearance and motion features, and the two-stream encoder + combination layers of the MCNet+Res architecture seem particularly adapted, if they indeed allowed for unsupervised pre-trainining of content and motion representations, as postulated by the authors. These experiments would also contribute to dispelling the aforementioned concerns about the static nature of the learned representations. 4) Conclusion Overall, this paper proposes an interesting architecture for an important problem, but requires additional experiments to substantiate the claims made by the authors. If the authors make the aforementioned additional experiments and the results are convincing, then this paper would be clearly relevant for ICLR. 5) Post-rebuttal final decision The authors did a significant amount of additional work, following the suggestions made by the reviewers, and providing additional compelling experimental evidence. This makes this one of the most experimentally thorough ones for this problem. I, therefore, increase my rating, and suggest to accept this paper. Good job!","['Rating:###7: Good paper, accept', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept']","['Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",7,5,"Strengths: 1) The paper proposes a novel architecture for future frame prediction in videos by decoupling appearance and motion information. 2) The qualitative results are numerous, insightful, and very convincing (including quantitatively) on KTH & Weizmann, showing the benefits of decoupling content and motion for simple scenes with periodic motions, as well as the need for residual connections. 3) The authors provide a clear and well-structured paper with a good introduction, a clear and well-structured presentation of the architecture, and a clear and well-structured presentation of the results. 4) The authors provide a good discussion of the related work and the contributions of the paper. 5) The authors provide a good conclusion. Weaknesses: 1) The paper is not very original. The idea of decoupling appearance and motion information is not new, and the authors do not provide a clear motivation for this idea. 2) The paper is not very well motivated. The authors do not provide a clear explanation of the benefits of decoupling appearance and motion information. 3) The paper is not very well written. The writing is clear, but the presentation is not very clear. The authors do not provide a clear explanation of the architecture, and the results are not very well explained. 4) The paper is not very well supported by experiments. The authors only provide results on two datasets, and the results are not very convincing."
Efficient Summarization with Read-Again and Copy Mechanism | OpenReview,"Encoder-decoder models have been widely used to solve sequence to sequence prediction tasks. However current approaches suffer from two shortcomings. First, the encoders compute a representation of each word taking into account only the history of the words it has read so far, yielding suboptimal representations. Second, current models utilize large vocabularies in order to minimize the problem of unknown words, resulting in slow decoding times and large storage costs. In this paper we address both shortcomings. Towards this goal, we first introduce a simple mechanism that first reads the input sequence before committing to a representation of each word. Furthermore, we propose a simple copy mechanism that is able to exploit very small vocabularies and handle out-of-vocabulary words. We demonstrate the effectiveness of our approach on the Gigaword dataset and DUC competition outperforming the state-of-the-art.","Summary: This paper proposes a read-again attention-based representation of the document with the copy mechanism for the summarization task. The model reads each sentence in the input document twice and creates a hierarchical representation of it instead of a bidirectional RNN. During the decoding, it uses the representation of the document obtained via the read-again mechanism and points the words that are OOV in the source document. The model does abstractive summarization. The authors show improvements on DUC 2004 dataset and provide an analysis of their model with different configurations. Contributions: The main contribution of this paper is the read-again attention mechanism where the model reads the same sentence twice and obtains a better representation of the document. Writing: The text of this paper needs more work. There are several typos and the explanations of the model/architecture are not really clear, some parts of the paper feel somewhat bloated. Pros: - The proposed model is a simple extension to the model to the model proposed in [2] for summarization. - The results are better than the baselines. Cons: - The improvements are not that large. - Justifications are not strong enough. - The paper needs a better writeup. Several parts of the text are not using a clear/precise language and the paper needs a better reorganization. Some parts of the text is somewhat informal. - The paper is very application oriented. Question: - How does the training speed when compared to the regular LSTM? Some Criticisms: A similar approach to the read again mechanism which is proposed in this paper has already been explored in [1] in the context of algorithmic learning and I wouldn’t consider the application of that on the summarization task a significant contribution. The justification behind the read-again mechanism proposed in this paper is very weak. It is not really clear why additional gating alpha_i is needed for the read again stage. As authors also suggest, pointer mechanism for the unknown/rare words [2] and it is adopted for the read-again attention mechanism. However, in the paper, it is not clear where the real is the gain coming from, whether from “read-again” mechanism or the use of “pointing”. The paper is very application focused, the contributions of the paper in terms of ML point of view is very weak. It is possible to try this read-again mechanism on more tasks other than summarization, such as NMT, in order to see whether if those improvements are The writing of this paper needs more work. In general, it is not very well-written. Minor comments: Some of the corrections that I would recommend fixing, On page 4: “… better than a single value … ” —> “… scalar gating …” On page 4: “… single value lacks the ability to model the variances among these dimensions.” —> “… scalar gating couldn’t capture the ….” On page 6: “ … where h_0^2 and h_0^*2 are initial zero vectors … “ —> “… h_0^2 and h_0^*2 are initialized to a zero vector in the beginning of each sequence …* There are some inconsistencies for example parts of the paper refer to Tab. 1 and some parts of the paper refer to Table 2. Better naming of the models in Table 1 is needed. The location of Table 1 is a bit off. [1] Zaremba, Wojciech, and Ilya Sutskever. *Reinforcement learning neural Turing machines.* arXiv preprint arXiv:1505.00521 362 (2015). [2] Gulcehre, Caglar, et al. *Pointing the Unknown Words.* arXiv preprint arXiv:1603.08148 (2016).  This paper proposed two incremental ideas to extend the current state-of-the-art summarization work based on seq2seq models with attention and copy/pointer mechanisms. 1. This paper introduces 2-pass reading where the representations from the 1st-pass is used to re-wight the contribution of each word to the sequential representation of the 2nd-pass. The authors described how such a so-called read-again process applies to both GRU and LSTM. 2. On the decoder side, the authors also use the softmax to choose between generating from decoder vocabulary and copying a source position, with a new twist of representing the previous decoded word Y_{t-1} differently. This allows the author to explore a smaller decoder vocabulary hence led to faster inference time without losing summarization performance. This paper claims the new state-of-the-art on DUC2004 but the comparison on Gigaword seems to be incomplete (missing more recent results after Rush 2015 etc). While the overall work is solid, there are also other things missing scientifically. For example, - how much computational costs does the 2nd pass reading add to the end-to-end system? - How does the decoder small vocabulary trick work without 2nd-pass reading on the encoder side for both summarization performance and runtime speed? - There are other ways to improve the embedding of a sentence. How does the 2nd-pass reading compare to recent work from multiple authors on self-attention and/or LSTMN? For example, Cheng et al. 2016, Long Short-Term Memory-Networks for Machine Reading; Parikh et al. 2016, A Decomposable Attention Model for Natural Language Inference?  This work explores the neural models for sentence summarisation by using a read-again attention model and a copy mechanism which grants the ability of direct copying word representations from the source sentences. The experiments demonstrate the model achieved better results on DUC dataset. Overall, this paper is not well-written. There are confusing points, some of the claims are lack of evidence and the experimental results are incomplete. Detailed comments: -Read-again attention. How does it work better than a vanilla attention? What would happen if you read the same sentences multiple times? Have you compared it with staked LSTM (with same number of parameters)? There is no model ablation in the experiment section. -Why do you need reading two sentences? The Gigaword dataset is a source-to-compression dataset which does not need multiple input sentences. How do you compare your model with single sent input and two sent input? -Copy mechanism. What if there are multiple same words appeared in the source sentences to be copied? According to equation (5), you only copy one vector to the decoder. However, there is no this kind of issue for a hard copy mechanism. Besides, there is no comparison between the hard copy mechanism and this vector copy mechanism in the experiment section -Vocabulary size. This part is a bit off the main track of this paper. If there is no evidence showing this is the special property of vector copy mechanism, it would be trivial in this paper. -Experiments. On the DUC dataset, it compares the model with other up-to-date models, while on the Gigaword dataset paper only compares the model with the ABS Rush et al. (2015) and the GRU (?), which are quite weak baseline models. It is irresponsible to claim this model achieved the state-of-the-art performance in the context of summarization. Typos: (1) Tab. 1. -> Table 1. (2) Fig. 3.1.2.?","['Rating:###5: Marginally below acceptance threshold', 'Rating:###6: Marginally above acceptance threshold', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",5,4,Strengths: 1. The paper proposes a novel mechanism to improve the summarization performance by reading the input document twice. 2. The authors provide a clear and concise explanation of the proposed mechanism. Weaknesses: 1. The paper is not well-written. There are several typos and the explanations of the model/architecture are not really clear. 2. The model is not compared with other models on the same dataset. 3. The paper does not provide a clear justification for the proposed mechanism. 4. The model is not compared with other models on the same dataset. 5. The paper does not provide a clear justification for the proposed mechanism. 6. The model is not compared with other models on the same dataset. 7. The paper does not provide a clear justification for the proposed mechanism. 8. The model is not compared with other models on the same dataset. 9. The paper does not provide a clear justification for the proposed mechanism. 10. The model is not compared with other models on the same dataset. Rating: 4 Confidence: 3
Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks | OpenReview,"Modern convolutional networks, incorporating rectifiers and max-pooling, are neither smooth nor convex. Standard guarantees therefore do not apply. Nevertheless, methods from convex optimization such as gradient descent and Adam are widely used as building blocks for deep learning algorithms. This paper provides the first convergence guarantee applicable to modern convnets. The guarantee matches a lower bound for convex nonsmooth functions. The key technical tool is the neural Taylor approximation -- a straightforward application of Taylor expansions to neural networks -- and the associated Taylor loss. Experiments on a range of optimizers, layers, and tasks provide evidence that the analysis accurately captures the dynamics of neural optimization. The second half of the paper applies the Taylor approximation to isolate the main difficulty in training rectifier nets: that gradients are shattered. We investigate the hypothesis that, by exploring the space of activation configurations more thoroughly, adaptive optimizers such as RMSProp and Adam are able to converge to better solutions.","This paper develops a theoretical guarantee for the convergence of the training error. The result is quite general that covers the training of a wide range of neural network models. The key idea of the paper is approximate the training loss by its linear approximation. Since its linearity in the variables (thus convex), the authors plug in results that has been developed in the literature of online learning. This paper has good novelty in using the Taylor approximation thus greatly simplifying the analysis of the behaviour of the model. However, there are two problems about the main result of this paper, Theorem 2. 1. It is not clear if the Taylor optimum would converge or not. As noticed by the authors, the upper bound is path dependent. Appendix 3 tries to claim that this Taylor optimum indeed converges, but the proof is buggy. In the proof of Lemma 2, it is proved that the difference between two sequential Taylor optimum is approaching 0. Note that this is actually weaker than being Cauchy sequence and insufficient to guarantee convergence. 2. The lefthand side of Equation (3) (I will denote it by L3 in this review) is not equivalent to training error. An upper bound on this average error is not sufficient to guarantee the convergence of the training error neither. Take the gradient descent for example (thus each minibatch x_0^n is the whole training set), the convergence of the training error should be lim_{n -> infty} l(f_{w^n}(x_0^n), y^n). The convergence of L3 is necessary but not sufficient to imply the convergence of the training error. Another concern about Theorem 2 (but it is minor compared to the two problems mentioned above) is that to achieve the O(1/sqrt{n}) rate, the algorithm has to pick a particular learning rate. Larger or smaller learning rate (in the order of n) will lead to significantly worse regret. But in the experiments of the paper, the learning rates are not picked according to the theorem. Overall, this paper has a good motivation and good novelty. It could be further developed into a good paper. But due to the two problems and a buggy proof mentioned above, I think it is not ready for publish yet.  This paper adopts Taylor approximations of neural nets for separating convex and non-convex components of the optimization. This enables them to bound the training error by the Taylor optimum and regret (theorem 2). This is a nice theoretical result applicable to popular deep nets. The empirical studies back up the theoretical claim.  It is interesting to derive such a bound and show it satisfies a regret bound along with empirical evidence on the CIFAR-10 for cross entropy loss and auto encoder for MSE loss. At least empirically, by comparing the observed training loss and taylor loss, the better a particular optimizer performs (training loss statement, not a validation or observed test statement) the smaller the difference between these two. Also shown is the regret loss is satisfied at different scales of the network, by layer, neuron and whole network. The Taylor approximation can be used to investigate activation configurations of the network, and used to connect this to difficulty in optimizing at kinks in the loss surface, along with an empirical study of exploration of activation surface of the SGD/Adam/RMSprop optimizers, the more exploration the better the resulting training loss. Not that it impacts the paper but the weaker performance of the SGD could be related to the fixed learning rate, if we anneal this learning rate, which should improve performance, does this translate to more exploration and tightening between the actual loss and the Taylor loss? - It might be useful to use a cross validation set for some of the empirical studies, in the end we would like to say something about generalization of the resulting network - Is there a reason the subscript on the Jacobian changes to a_l in the  definition?","['Rating:###3: Clear rejection', 'Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']",3,4,Strengths: 1. The paper provides a theoretical guarantee for the convergence of the training error of a neural network. 2. The authors provide empirical evidence that the Taylor approximation is useful for understanding the training process of deep networks. 3. The paper is well-written and easy to follow. Weaknesses: 1. The paper is not very original. The Taylor approximation is a well-known tool in convex optimization. The authors do not provide any new insights into the Taylor approximation. 2. The paper is not very well-justified. The authors do not provide a clear explanation of why the Taylor approximation is useful for deep networks. 3. The paper is not very well-organized. The authors jump from one result to another without providing a clear connection between them. Rating: 6 Confidence: 3
Multi-modal Variational Encoder-Decoders | OpenReview,"Recent advances in neural variational inference have facilitated efficient training of powerful directed graphical models with continuous latent variables, such as variational autoencoders. However, these models usually assume simple, uni-modal priors — such as the multivariate Gaussian distribution — yet many real-world data distributions are highly complex and multi-modal. Examples of complex and multi-modal distributions range from topics in newswire text to conversational dialogue responses. When such latent variable models are applied to these domains, the restriction of the simple, uni-modal prior hinders the overall expressivity of the learned model as it cannot possibly capture more complex aspects of the data distribution. To overcome this critical restriction, we propose a flexible, simple prior distribution which can be learned efficiently and potentially capture an exponential number of modes of a target distribution. We develop the multi-modal variational encoder-decoder framework and investigate the effectiveness of the proposed prior in several natural language processing modeling tasks, including document modeling and dialogue modeling.","The authors introduce some new prior and approximate posterior families for variational autoencoders, which are compatible with the reparameterization trick, as well as being capable of expressing multiple modes. They also introduce a gating mechanism between prior and posterior. They show improvements on bag of words document modeling, and dialogue response generation. The original abstract is overly strong in its assertion that a unimodal latent prior p(z) cannot fit a multimodal marginal int_z p(x|z)p(x)dz with a DNN response model p(x|z) (*it cannot possibly capture more complex aspects of the data distribution*, *critical restriction*, etc). While the assertion that a unimodal latent prior is necessary to model multimodal observations is false, there are sensible motivations for the piecewise constant prior and posterior. For example, if we think of a VAE as a sort of regularized autoencoder where codes are constrained to *fill up* parts of the prior latent space, then there is a sphere-packing argument to be made that filling a Gaussian prior with Gaussian posteriors is a bad use of code space. Although the authors don*t explore this much, a hypercube-based tiling of latent code space is a sensible idea. As stated, I found the message of the paper to be quite sloppy with respect to the concept of *multi-modality.* There are 3 types of multimodality at play here: multimodality in the observed marginal distribution p(x), which can be captured by any deep latent Gaussian model, multimodality in the prior p(z), which makes sense in some situations (e.g. a model of MNIST digits could have 10 prior modes corresponding to latent codes for each digit class), and multimodality in the posterior z for a given observation x_i, q(z_i|x_i). The final type of multimodality is harder to argue for, except in so far as it allows the expression of flexibly shaped distributions without highly separated modes. I believe flexible posterior approximations are important to enable fine-grained and efficient tiling of latent space, but I don*t think these need to have multiple strong modes. I would be interested to see experiments demonstrating otherwise for real world data. I think this paper should be more clear about the different types of multi-modality and which parts of their analysis demonstrate which ones. I also found it unsatisfactory that the piecewise variable analysis did not show different components of the multi-modal prior corresponding to different words, but rather just a separation between the Gaussian and the piecewise variables. As I mention in my earlier questions, I found it surprising that the learned variance and mean for the Gaussian prior helps so dramatically with G-NVDM likelihood when the powerful networks transforming to and from latent space should make it scale-invariant. Explicitly separating out the contributions of a reimplemented base model, prior-posterior interpolation and the learned prior parameters would strengthen these experiments. Overall, the very strong improvements on the text modeling task over NVDM seem hard to understand, and I would like to see an ablation analysis of all the differences between that model and the proposed one. The fact that adding more constant components helps for document modeling is interesting, and it would be nice to see more qualitative analysis of what the prior modes represent. I also would be surprised if posterior modes were highly separated, and if they were it would be interesting to explore if they corresponded to e.g. ambiguous word-senses. The experiments on dialog modeling are mostly negative results, quantitatively. The observation that the the piecewise constant variables encode time-related words and the Gaussian variables encode sentiment is interesting, especially since it occurs in both sets of experiments. This is actually quite interesting, and I would be interested in seeing analysis of why this is the case. As above, I would like to see an analysis of the sorts of words that are encoded in the different prior modes and whether they correspond to e.g. groups of similar holidays or days. In conclusion, I think the piecewise constant variational family is a good idea, although it is not well-motivated by the paper. The experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in G-NVDM. The fact that H-NVDM performs better is interesting, though. This paper should better motivate the need for different types of multi-modality, and demonstrate that those sorts of things are actually being captured by the model. As it is, the paper introduces an interesting variational family and shows that it performs better for some tasks, but the motivation and analysis is not clearly focused. To demonstrate that this is a broadly applicable family, it would also be good to do experiments on a more standard datasets like MNIST. Even without an absolute log-likelihood improvement, if the method yielded interpretable multiple modes this would be a valuable contribution.  This paper proposes a piecewise constant parameterisation for neural variational models so that it could explore the multi-modality of the latent variables and develop more powerful neural models. The experiments of neural variational document models and variational hierarchical recurrent encoder-decoder models show that the introduction of the piecewise constant distribution helps achieve better perplexity on modelling documents and seemly better performance on modelling dialogues. The idea of having a piecewise constant prior for latent variables is interesting, but the paper is not well-written (even 14 pages long) and the design of the experiments fails to demonstrate the most of the claims. The detailed comments are as follows: --The author explains the limitations of the VAEs with standard Gaussian prior in the last paragraph of 3.1 and the last paragraph of 5.1. Hence, a multimodal prior would help the VAEs overcome the issues of optimisation. However, there is a lack of evidence showing the multimodality of the prior helps break the bottleneck. --In the last paragraph of 6.1, the author claimed the decoder parameter matrix is directly affected by the latent variables. But what the connects the decoder is a combination of a piecewise constant and Gaussian latent variables. No matter what is discovered in the experiments, it only shows z= is multimodal. However, z= can be multimodal as well. None of the claims in this paragraph stands. --In the quantitative evaluation of NVDM, there is an incremental model from z=z_gaussian to z=. As the prior is learned together with the variational posterior, a more flexible prior would alleviate the regularisation imposed by the KL term. Certainly, more parameters are applied as well, so a fair comparison would at least be z= and z= which equals to a double sized z_gaussian. --The results shown in Table 3 are implausible. I cannot believe the author used gradients to evaluate the model. --Eq. 5 is confusing, adding a multiplication sign might help. --3.1 can be deleted because people attending ICLR are familiar with VAEs. Typos: as well as the well as the generated prior-> as well as the generated prior  UPDATE: I have read the authors* rebuttal and also the other comments in this paper*s thread. My thoughts have not changed. The authors propose using a mixture prior rather than a uni-modal prior for variational auto-encoders. They argue that the simple uni-modal prior *hinders the overall expressivity of the learned model as it cannot possibly capture more complex aspects of the data distribution.* I find the motivation of the paper suspicious because while the prior may be uni-modal, the posterior distribution is certainly not. Furthermore, a uni-modal distribution on the latent variable space can certainly still lead to the capturing of complex, multi-modal data distributions. (As the most trivial case, take the latent variable space to be a uniform distribution; take the likelihood to be a point mass given by applying the true data distribution*s inverse CDF to the uniform. Such a model can capture any distribution.) In addition, multi-modality is arguably an overfocused concept in the literature, where the (latent variable) space is hardly anymore worth capturing from a mixture of simple distributions when it is often a complex nonlinear space. It is unclear from the experiments how much the influence of the prior*s multimodality influences the posterior to capture more complex phenomena, and whether this is any better than considering a more complex (but still reparameterizable) distribution on the latent space. I recommend that this paper be rejected, and encourage the authors to more extensively study the effect of different priors. I*d also like to make two additional comments: While there is no length restriction at ICLR, the 14 page document can be significantly condensed without loss of describing their innovation or clarity. I recommend the authors do so. Finally, I think it*s important to note the controversy in this paper. It was submitted with many significant incomplete details (e.g., no experiments, many missing citations, a figure placed inside that was pencilled in by hand, and several missing paragraphs). These details were not completed until roughly a week(?) later. I recommend the chairs discuss this in light of what should be allowed next year.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###4: Ok but not good enough - rejection', 'Rating:###3: Clear rejection']","['Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",4,5,"Strengths: 
1. The authors propose a new variational family of distributions that can be used to model complex data distributions. 
2. The authors provide a clear and concise description of the proposed distribution and its properties. 
3. The authors provide a clear and concise description of the experiments and the results. 

Weaknesses: 
1. The paper is not well-written. The text is not clear and the sentences are not well-structured. 
2. The paper lacks a clear motivation for the proposed distribution. The authors claim that the proposed distribution is necessary to model complex data distributions, but they do not provide a clear explanation of why this is the case. 
3. The paper lacks a clear explanation of the experiments. The authors claim that the proposed distribution improves the performance of the model, but they do not provide a clear explanation of how the proposed distribution is used in the experiments. 
4. The paper lacks a clear comparison to other models. The authors claim that the proposed distribution improves the performance of the model, but they do not provide a clear comparison to other models. 
5. The paper lacks a clear explanation of the results. The authors claim that the proposed distribution improves the performance of the model, but they do not provide a clear explanation of what this means. 

Rating: 4
Confidence: 3"
Semi-Supervised Classification with Graph Convolutional Networks | OpenReview,We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.,"The paper develops a simple and reasonable algorithm for graph node prediction/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups. Experiments are thorough and compare with many reasonable baselines on large and real benchmark datasets. Although, I am not quite aware of the literature on other methods and there may be similar alternatives as link and node prediction is an old problem. I still think the approach is quite simple and reasonably supported by good evaluations.  The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method. The paper is technically fine and clear, the algorithm seems to scale well, and the results on the different datasets compare very favorably with the different baselines. The algorithm is simple and training seems easy. Concerning the originality, the proposed algorithm is a simple adaptation of graph convolutional networks (ref Defferrard 2016 in the paper) to a semi-supervised transductive setting. This is clearly mentioned in the paper, but the authors could better highlight the differences and novelty wrt this reference paper. Also, there is no comparison with the family of iterative classifiers, which usually compare favorably, both in performance and training time, with regularization based approaches, although they are mostly used in inductive settings. Below are some references for this family of methods. The authors mention that more complex filters could be learned by stacking layers but they limit their architecture to one hidden layer. They should comment on the interest of using more layers for graph classification. Some references on iterative classification Qing Lu and Lise Getoor. 2003. Link-based classification. In ICML, Vol. 3. 496–503. Gideon S Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with weakly labeled data. The Journal of Machine Learning Research 11 (2010), 955–984. David Jensen, Jennifer Neville, and Brian Gallagher. 2004. Why collective inference improves relational classification. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 593–598. Joseph J Pfeiffer III, Jennifer Neville, and Paul N Bennett. 2015. Overcoming Relational Learning Biases to Accurately Predict Preferences in Large Scale Networks. In Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 853– 863. Stephane Peters, Ludovic Denoyer, and Patrick Gallinari. 2010. Iterative annotation of multi-relational social networks. In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on. IEEE, 96–103.  This paper proposes the graph convolutional networks, motivated from approximating graph convolutions. In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity. This model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin. The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared. It is surprising that such a simple model works so much better than all the baselines. Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this. Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not. Even though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple. Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things. So how would the proposed GCN compare against these methods? Overall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good. There are a few questions that still remain, but I feel this paper can be accepted.","['Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept', 'Rating:###7: Good paper, accept']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",7,3,"Strengths: 1. The paper presents a simple and reasonable algorithm for semi-supervised learning on graph-structured data. 2. The formulation is very intuitive and leads to a simple CNN based training. 3. The paper is well written and easy to follow. 4. The experiments are thorough and compare with many reasonable baselines on large and real benchmark datasets. Weaknesses: 1. The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method. The algorithm is simple and training seems easy. Concerning the originality, the proposed algorithm is a simple adaptation of graph convolutional networks (ref Defferrard 2016 in the paper) to a semi-supervised transductive setting. This is clearly mentioned in the paper, but the authors could better highlight the differences and novelty wrt this reference paper. Also, there is no comparison with the family of iterative classifiers, which usually compare favorably, both in performance and training time, with regularization based approaches, although they are mostly used in inductive settings. Below are some references on iterative classification Qing Lu and Lise Getoor. 2003. Link-based classification. In ICML, Vol. 3. 496–503. G"
Vocabulary Selection Strategies for Neural Machine Translation | OpenReview,Classical translation models constrain the space of possible outputs by selecting a subset of translation rules based on the input sentence. Recent work on improving the efficiency of neural translation models adopted a similar strategy by restricting the output vocabulary to a subset of likely candidates given the source. In this paper we experiment with context and embedding-based selection methods and extend previous work by examining speed and accuracy trade-offs in more detail. We show that decoding time on CPUs can be reduced by up to 90% and training time by 25% on the WMT15 English-German and WMT16 English-Romanian tasks at the same or only negligible change in accuracy. This brings the time to decode with a state of the art neural translation system to just over 140 words per seconds on a single CPU core for English-German.,"This paper conducts a comprehensive series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation. A range of techniques are investigated, ranging from very simple methods such as word co-occurences, to the relatively complex use of SVMs. The experiments are solid, comprehensive and very useful in practical terms. It is good to see that the best vocabulary selection method is very effective at achieving a very high proportion of the coverage of the full-vocabulary model (fig 3). However, I feel that the experiments in section 4.3 (vocabulary selection during training) was rather limited in their scope - I would have liked to see more experiments here. A major criticism I have with this paper is that there is little novelty here. The techniques are mostly standard methods and rather simple, and in particular, there it seems that there is not much additional material beyond the work of Mi et al (2016). So although the work is solid, the lack of originality lets it down. Minor comments: in 2.1, the word co-occurence measure - was any smoothing used to make this measure more robust to low counts?  This paper compares several strategies for guessing a short list of vocabulary for the target language in neural machine translation. The primary findings are that word alignment dictionaries work better than a variety of other techniques. My take on this paper is that to have a significant impact, it needs to make the case for why one might want vocabulary rather than characters or sub word units like BPE. I think there are likely many very good reasons to do this that could be argued for (synthesize morphology, deal with transliteration, etc), but most of these would suggest some particular models and experiments, which are of course not in this paper. As it is, I think this paper is a useful but minor contribution that shows that word alignment is a good way of getting short lists, but it does not strongly make the case that we should abandon work in other directions. Minor comments: In addition to the SVM approach for modeling vocabulary, the discriminative word lexicon of Mauser et al. (2009) and the neural version of Ha et al. (2014) are also worth mentioning. It would be useful to know what the coverage rate of the actual full vocabulary would be (rather than the 100k “full vocabulary”). Since presumably this technique could be used to work with much larger vocabularies. When reducing the vocabulary size for training, the Mi et al. (2016) technique of taking the union of all the vocabularies in a mini batch seems like a rather strange objective. If the vocabulary of a single sentence is used, the probabilistic semantics of the translation model can still be preserved since p(e | f, vocab(f)) = p(e | f) if p(vocab(f) | f) = 1, i.e., is deterministic, which it is here. Whereas the objective is no longer a sensible probability model in the mini batch vocabulary case. Thus, while it may be a bit more difficult to implement, it seems like it would at least be a sensible comparison to make.  In this paper, the authors present several strategies to select a small subset of target vocabulary to work with per source sentence, which results in significant speedup. The results are convincing and I think this paper offers practical values to general seq2seq approaches to language tasks. However, there is little novelty in this work: the authors further mostly extend the work of (Mi et al., 2016) with more vocabulary selection strategies and thorough experiments. This paper will fit better in an NLP venue.  This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training. It could be quite useful to practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks, and I am not sure ICLR is the ideal venue for this work. - Do the reported decoding times take into account the vocabulary reduction step? - Aside from machine translation, might there be applications to other settings such as language modeling, where large vocabulary is also a scalability challenge? - The proposed methods are helpful because of the difficulties induced by using a word-level model. But (at least in my opinion) starting from a character or even lower-level abstraction seems to be the obvious solution to the huge vocabulary problem.","['Rating:###5: Marginally below acceptance threshold', 'Rating:###4: Ok but not good enough - rejection', 'Rating:###4: Ok but not good enough - rejection', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct']",5,3,"Strengths:
1. The paper presents a comprehensive set of experiments on vocabulary selection strategies for neural machine translation. The results are solid and provide a good understanding of the trade-offs between decoding time and accuracy.
2. The paper is well-written and easy to follow.
3. The authors provide a good motivation for the work and the experiments are well-justified.

Weaknesses:
1. The paper does not bring any new ideas to the field. The methods presented are mostly standard and the experiments are mostly based on the work of Mi et al. (2016).
2. The paper does not provide a good motivation for why vocabulary selection is important. The authors mention that vocabulary selection can be useful for other tasks, but they do not provide any concrete examples.
3. The paper does not provide a good comparison to other methods. The authors mention that the methods presented are not compared to other methods, but they do not provide any details on how the methods were compared.
4. The paper does not provide a good discussion on the limitations of the methods presented. The authors mention that the methods are not suitable for all tasks, but they do not provide any details on how the methods perform on other tasks.
5. The paper does not provide a good conclusion. The authors summarize the main results, but they do not provide any final thoughts on the implications of the work.

Rating: 6
Confidence: 3"
Divide and Conquer with Neural Networks | OpenReview,"We consider the learning of algorithmic tasks by mere observation of input-output pairs. Rather than studying this as a black-box discrete regression problem with no assumption whatsoever on the input-output mapping, we concentrate on tasks that are amenable to the principle of divide and conquer, and study what are its implications in terms of learning. This principle creates a powerful inductive bias that we exploit with neural architectures that are defined recursively, by learning two scale-invariant atomic operators: how to split a given input into two disjoint sets, and how to merge two partially solved tasks into a larger partial solution. The scale invariance creates parameter sharing across all stages of the architecture, and the dynamic design creates architectures whose complexity can be tuned in a differentiable manner. As a result, our model is trained by backpropagation not only to minimize the errors at the output, but also to do so as efficiently as possible, by enforcing shallower computation graphs. Moreover, thanks to the scale invariance, the model can be trained only with only input/output pairs, removing the need to know oracle intermediate split and merge decisions. As it turns out, accuracy and complexity are not independent qualities, and we verify empirically that when the learnt complexity matches the underlying complexity of the task, this results in higher accuracy and better generalization in two paradigmatic problems: sorting and finding planar convex hulls.","I was holding off on this review hoping to get the missing details from the code at https://github.com/alexnowakvilla/DP, but at this time it*s still missing. After going over this paper couple of times I*m still missing the details necessary to reproduce the experiments. I think this would be a common problem for readers of this paper, so the paper needs to be improved, perhaps with a toy example going through all the stages of learning. As an example of the difficulty, take section 4.3. It talks about training *split block* which is a function that can assign each element to either partition 0 or partition 1. At this point I*m looking at it as a binary classification problem and looking for the parameters, loss, and how this loss is minimized. Instead I get a lot of unexpected information, such as *we must create artificial targets at every node of the generated tree from the available final target partition*. What are these artificial targets, and how do they relate to the problem of training the splitter? An example that explicitly goes through this construction would help with understanding.  The basic idea of this contribution is very nice and worth pursuing: how to use the powerful “divide and conquer” algorithm design strategy to learn better programs for tasks such as sorting or planar convex hull. However, the execution of this idea is not convincing and needs polishing before acceptance. As it is right now, the paper has a proof-of-concept feel that makes it great for a workshop contribution. My main concern is that the method presented is currently not easily applicable to other tasks. Typically, demonstrations of program induction from input-output examples on well known tasks serves the purpose of proving, that a generic learning machine is able to solve some well known tasks, and will be useful on other tasks due to its generality. This contribution, however, presents a learning machine that is very hand-tailored to the two chosen tasks. The paper essentially demonstrates that with enough engineering (hardcoding the recurrency structure, designing problem-specific rules of supervision at lower recurrency levels) one can get a partially trainable sorter or convex hull solver. I found the contribution relatively hard to understand. High level ideas are mixed with low-level tricks required to get the model to work and it is not clear either how the models operate, nor how much of them was actually learned, and how much was designed. The answer to the questions did hep, nut didn*t make it into the paper. Mixing the descriptions of the tricks required to solve the two tasks makes things even more confusing. I believe that the paper would be much more accessible if instead of promising a general solution it clearly stated the challenges faced by the authors and the possible solutions. Highlights: + Proof-of-concept of a partially-trainable implementation of the important “divide and conquer” paradigm ++ Explicit reasoning about complexity of induced programs - The solution isn’t generic enough to be applicable to unknown problems - the networks require tricks specific to each problem - The writing style pictures the method as very general, but falls back on very low level details specific to each task  I find this paper extremely hard to read. The main promise of the paper is to train models for combinatorial search procedures, especially for dynamic programming to learn where to split and merge. The present methodology is supposed to make use of some form of scale invariance property which is scarcely motivated for most problems this approach should be relevant for. However, the general research direction is fruitful and important. The paper would be much more readable if it would start with a clear, formal problem formulation, followed by some schematic view on the overall flow and description on which parts are supervised, which parts are not. Also a tabular form and sample of the various kinds problems solved by this method could be listed in the beginning as a motivation with some clear description on how they fit the central paradigm and motivate the rest of the paper in a more concrete manner. Instead, the paper is quite chaotic, switching between low-level and high level details, problem formulations and their solutions in a somewhat random, hard to parse order. Both split and merge phases seem to make a lot of discrete choices in a hierarchical manner during training. The paper does not explain how those discrete choices are backpropagated through the network in an unbiased manner, if that is the case at all. In general, the direction this paper is exciting, but the paper itself is a frustrating read in its present form. I have spent several hours on it without having to manage to achieve a clear mental image on how all the presented pieces fit together. I would revise my score if the paper would be improved greatly from a readability perspective, but I think it would require a major rewrite.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###4: Ok but not good enough - rejection', 'Rating:###3: Clear rejection']","['Confidence:###2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']",4,2,Strengths: 1. The paper presents a novel approach to learning algorithms by using a divide-and-conquer paradigm. 2. The authors provide a clear and concise description of the algorithm and its components. 3. The paper is well-structured and easy to follow. Weaknesses: 1. The paper is difficult to read due to the lack of clear explanations and the use of technical jargon. 2. The authors do not provide a clear motivation for the use of the divide-and-conquer paradigm and its relevance to the problem at hand. 3. The paper lacks empirical evidence to support its claims. Rating: 6 Confidence: 3
Perception Updating Networks: On architectural constraints for interpretable video generative models | OpenReview,We investigate a neural network architecture and statistical framework that models frames in videos using principles inspired by computer graphics pipelines. The proposed model explicitly represents *sprites* or its percepts inferred from maximum likelihood of the scene and infers its movement independently of its content. We impose architectural constraints that forces resulting architecture to behave as a recurrent what-where prediction network.,"This paper proposes a generative model of videos composed of a background and a set of 2D objects (sprites). Optimization is performed under a VAE framework. The authors* proposal of an outer product of softmaxed vectors (resulting in a 2D map that is delta-like), composed with a convolution, is a very interesting way to achieve translation of an image with differentiable parameters. It seems to be an attractive alternative to more complicated differentiable resamplers (such as those used by STNs) when only translation is needed. Below I have made some comments regarding parts of the text, especially the experiments, that are not clear. The experimental section in particular seems rushed, with some results only alluded to but not given, not even in the appendix. For an extremely novel and exotic proposal, showing only synthetic experiments could be excused. However, though there is some novelty in the method, it is disappointing that there isn*t even an attempt at trying to tackle a problem with real data. I suggest as an example aerial videos (such as those taken from drone platforms), since the planar assumption that the authors make would most probably hold in that case. I also suggest that the authors do another pass at proof-reading the paper. There are missing references (*Fig. ??*), unfinished sentences (caption of Fig. 5), and the aforementioned issues with the experimental exposition.  This paper presents an approach to modeling videos based on a decomposition into a background + 2d sprites with a latent hidden state. The exposition is OK, and I think the approach is sensible, but the main issue with this paper is that it is lacking experiments on non-synthetic datasets. As such, while I find the graphics inspired questions the paper is investigating interesting, I don*t think it is clear that this work introduces useful machinery for modeling more general videos. I think this paper is more appropriate as a workshop contribution in its current form.  This paper presents a generative model of video sequence data where the frames are assumed to be generated by a static background with a 2d sprite composited onto it at each timestep. The sprite itself is allowed to dynamically change its appearance and location within the image from frame to frame. This paper follows the VAE (Variational Autoencoder) approach, where a recognition/inference network allows them to recover the latent state at each timestep. Some results are presented on simple synthetic data (such as a moving rectangle on a black background or the “Moving MNIST” data. However, the results are preliminary and I suspect that the assumptions used in the paper are far too strong too be useful in real videos. On the Moving MNIST data, the numerical results are not competitive to state of the art numbers. The model itself is also not particularly novel and the work currently misses some relevant citations. The form of the forward model, for example, could be viewed as a variation on the DRAW paper by Gregor et al (ICML 2014). Efficient Inference in Occlusion-Aware Generative Models of Images by Huang & Murphy (ICLR) is another relevant work, which used a variational auto-encoder with a spatial transformer and an RNN-like sequence model to model the appearance of multiple sprites on a background. Finally, the exposition in this paper is short on many details and I don’t believe that the paper is reproducible from the text alone. For example, it is not clear what the form of the recognition model is… Low-level details (which are very important) are also not presented, such as initialization strategy.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###4: Ok but not good enough - rejection', 'Rating:###4: Ok but not good enough - rejection']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",4,3,"Strengths: 1. The paper proposes a novel approach to modeling videos by decomposing them into a background and a set of 2D sprites. 2. The authors propose a generative model of video sequence data where the frames are assumed to be generated by a static background with a 2D sprite composited onto it at each timestep. 3. The model is based on a VAE framework, which is a common and well-established approach to generative modeling. Weaknesses: 1. The paper lacks experiments on non-synthetic datasets. 2. The results presented are preliminary and do not seem to be competitive to state-of-the-art numbers. 3. The model itself is not particularly novel and the work currently misses some relevant citations. 4. The form of the forward model is not clearly explained. 5. The exposition in the paper is short on many details and is not reproducible from the text alone. Rating: 6 Confidence: 3"
Deep Error-Correcting Output Codes | OpenReview,"Existing deep networks are generally initialized with unsupervised methods, such as random assignments and greedy layerwise pre-training. This may result in the whole training process (initialization/pre-training + fine-tuning) to be very time consuming. In this paper, we combine the ideas of ensemble learning and deep learning, and present a novel deep learning framework called deep error-correcting output codes (DeepECOC). DeepECOC are composed of multiple layers of the ECOC module, which combines multiple binary classifiers for feature learning. Here, the weights learned for the binary classifiers can be considered as weights between two successive layers, while the outputs of the combined binary classifiers as the outputs of a hidden layer. On the one hand, the ECOC modules can be learned using given supervisory information, and on the other hand, based on the ternary coding design, the weights can be learned only using part of the training data. Hence, the supervised pre-training of DeepECOC is in general very effective and efficient. We have conducted extensive experiments to compare DeepECOC with traditional ECOC, feature learning and deep learning algorithms on several benchmark data sets. The results demonstrate that DeepECOC perform not only better than traditional ECOC and feature learning algorithms, but also state-of the-art deep learning models in most cases.","The paper proposes a greedy supervised layer-wise initialization strategy for (deep) multi-layer perceptrons. Layer weights are initialized by training linear SVMs for binary classification where the binary targets are constructed as error correcting codes (ECOC, including one-vs-all, one-vs-one and others). The thus pertained model (together with a softmax output layer) is then globally fine-tuned by backdrop with dropout. Note that as a heuristic greedy supervised layer-wise initialization strategy this work is very similar to the author’s other ICLR submission: « Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications ». The two works differ in the supervised initialization strategy employed. While layer-wise initialization strategies are worthy of further exploration, the paper doesn’t convey any insight as to what makes a better strategy. Experimental results are not sufficiently convincing by themselves alone to support a mostly incremental work; in particular I remain unconvinced that competing methods received full proper hyper-parameter tuning of their own. Results showing accuracies as bar graphs make it hard to read-off precise accuracies, and one cannot easily compare with known state-of-the-art performance references on benchmark problems (such as MNIST). CIFAR10 performance seem far from state-of-the-art. Explanations are unnecessarily detailed for standard algorithms (e.g. SVMs) and not sufficiently for aspects specific to the approach such as lesser known ECOC schemes. One important aspect remains unclear regarding the use of SVMs. Did you use linear SVMs as stated in section 3.1 (« In order to take the probabilistic outputs of the base classifiers as new representations of data, we adopt linear support vector machines (linear SVMs) as the binary classifiers ») or kernel SVMs as mentioned later « For all DeepECOC models, we used support vector machines (SVMs) with RBF kernel function». In the latter case, the paper lacks a description of how learned RBF-kernel SVMs are transferred to a deep network layer (does it yield 2 layers the firrst of which would be a large RBF neural layer?) Also in this case of kernel SVMs the computational cost is likely to skyrocket and the method will have scaling issues. Is this the reason why the method is too expensive to use on CIFAR10 from scratch, and prompts doing LBP first? If you used linear SVMs, did you use an efficient implementation specific to linear SVMS (as opposed to generic kernel SVM code with a linear kernel?). Finally for image datasets, a visual comparison of learned filters could help provide some qualitative insight.  Error correcting output coding is well established supervised learning approach. Stacking several layers of ECOC seems a natural way of extending the framework to deep learning. The main motivation of the authors here is to reduce the sample complexity of internal binary classifiers by using ternary coding and to learn discriminative representations at every layer thanks to the local supervision. The main idea of this work is interesting. However, the presentation can be improved and several sections (about SVMs for example) can be shortened. I am not convinced by the advantage of this approach compared to end-to-end training of neural networks of other layer-wise training of deep architecture. In particular, when the codes at every layer can are randomly generated the induced binary problems can be arbitrarily difficult and this could lead to learnability issues hence affecting the quality of the features. While this problem is counterbalanced by the decoding scheme (and error-correction) at the prediction layer, it is not clear how it should be dealt with in the intermediate layers. In addition, other approaches such as stacked RBMs or Autoencoders learn regularities capturing information about the distribution of the data at every layer. In comparison, it is not clear what the learned representation by an ECOC layer is unless it is supervised (One versus Rest). Overall, though interesting, I am not convinced by the proposed approach and the experimental section does not help mitigate the impression. More insights on the learned internal representations, and experiments using standard datasets could help.  The paper proposes a strategy to pre-train the successive layers of a multi-layer perceptron for classification tasks. It consists in training successively each layer to predict an ECOC corresponding to the classification problem. The first layer predicts this ECOC from the input data and each successive layer takes as output its preceding layer to predict another ECOC. Different regularization strategies are used during training (input noise, Dropout). The MLP is then fine-tuned using SGD. Comparisons are performed with baselines on different datasets. This is a preliminary work. The idea might be valuable but it should be pushed further. Concerning the writing, sections concerning well known notions (e.g. SVM) could be suppressed. Since ECOC are central to this contribution, a more detailed description of the different ECOC strategies would be helpful. The experiments do not compare the proposed model with state of the art classifiers. The experimental conditions should be made more precise, e.g., it is not clear whether the regularization strategies have been used for all the NN architectures or only for the ECOC based ones. Finally note that there are several papers that try to learn intermediate representations for classification problems, see e.g. Samy Bengio, Jason Weston, David Grangier: Label Embedding Trees for Large Multi-Class Tasks. NIPS 2010: 163-171","['Rating:###3: Clear rejection', 'Rating:###3: Clear rejection', 'Rating:###3: Clear rejection']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",3,4,Strengths: 1. The paper proposes a novel approach to deep learning by combining the ideas of ensemble learning and deep learning. 2. The authors provide a clear and concise presentation of the approach. Weaknesses: 1. The paper does not provide a clear motivation for the proposed approach. 2. The experimental results are not convincing. 3. The paper does not provide a clear comparison with state-of-the-art methods. 4. The writing is not clear and concise. 5. The paper does not provide a clear description of the different ECOC strategies. Rating: 4 Confidence: 3
Cooperative Training of Descriptor and Generator Networks | OpenReview,"This paper studies the cooperative training of two probabilistic models of signals such as images. Both models are parametrized by convolutional neural networks (ConvNets). The first network is a descriptor network, which is an exponential family model or an energy-based model, whose feature statistics or energy function are defined by a bottom-up ConvNet, which maps the observed signal to the feature statistics. The second network is a generator network, which is a non-linear version of factor analysis. It is defined by a top-down ConvNet, which maps the latent factors to the observed signal. The maximum likelihood training algorithms of both the descriptor net and the generator net are in the form of alternating back-propagation, and both algorithms involve Langevin sampling. We observe that the two training algorithms can cooperate with each other by jump-starting each other’s Langevin sampling, and they can be seamlessly interwoven into a CoopNets algorithm that can train both nets simultaneously.","The authors proposes an interesting idea of connecting the energy-based model (descriptor) and the generator network to help each other. The samples from the generator are used as the initialization of the descriptor inference. And the revised samples from the descriptor is in turn used to update the generator as the target image. The proposed idea is interesting. However, I think the main flaw is that the advantages of having that architecture are not convincingly demonstrated in the experiments. For example, readers will expect quantative analysis on how initializing with the samples from the generator helps? Also, the only quantative experiment on the reconstruction is also compared to quite old models. Considering that the model is quite close to the model of Kim & Bengio 2016, readers would also expect a comparison to that model. ** Minor - I*m wondering if the analysis on the convergence is sound when considering the fact that samples from SGLD are biased samples (with fixed step size). - Can you explain a bit more on how you get Eqn 8? when p(x|y) is also dependent on W_G?  This paper introduces CoopNets, an algorithm which trains a Deep-Energy Model (DEM, the “descriptor”) with the help of an auxiliary directed bayes net, e.g. “the generator”. The descriptor is trained via standard maximum likelihood, with Langevin MCMC for sampling. The generator is trained to generate likely samples under the DEM in a single, feed-forward ancestral sampling step. It can thus be used to shortcut expensive MCMC sampling, hence the reference to “cooperative training”. The above idea is interesting and novel, but unfortunately is not sufficiently validated by the experimental results. First and foremost, two out of the three experiments do not feature a train /test split, and ignore standard training and evaluation protocols for texture generation (see [R1]). Datasets are also much too small. As such these experiments only seem to confirm the ability of the model to overfit. On the third in-painting tasks, baselines are almost non-existent: no VAEs, RBMs, DEM, etc which makes it difficult to evaluate the benefits of the proposed approach. In a future revision, I would also encourage the authors to answer the following questions experimentally. What is the impact of the missing rejection step in Langevin MCMC (train with, without ?). What is the impact of the generator on the burn-in process of the Markov chain (show sample auto-correlation). How bad is approximation of training the generator from ({ ilde{Y}, hat{X}) instead of ({ ilde{Y},  ilde{X}) ? Run comparative experiments. The paper would also greatly benefit from a rewrite focusing on clarity, instead of hyperbole (“pioneering work” in reference to closely related, but non-peer reviewed work) and prose (“tale of two nets”). For example, the authors fail to specify the exact form of the energy function: this seems like a glaring omission. PROS: + Interesting and novel idea CONS: - Improper experimental protocols - Missing baselines - Missing diagnostic experiments [R1] Heess, N., Williams, C. K. I., and Hinton, G. E. (2009). Learning generative texture models with extended fields of-experts.  This paper proposed a new joint training scheme for two probabilistic models of signals (e.g. images) which are both deep neural network based and are termed generator and descriptor networks. In the new scheme, termed cooperative training, the two networks train together and assist each other: the generator network provides samples that work as initial samples for the descriptor network, and the descriptor network updates those samples to help guide training of the generator network. This is an interesting approach for coupling the training of these two models. The paper however is quite weak on the empirical studies. In particular: - The training datasets are tiny, from sets of 1 image to 5-6. What is the reason for not using larger sets? I think the small datasets are leading to over training and are really masking the true value of the proposed cooperative training approach. - For most of the experiments presented in the paper it is hard to assess the specific value brought by the proposed cooperative training approach because baseline results are missing. There are comparisons provided for face completion experiments - but even there comparisons with descriptor or generator network trained separately or with other deep auto-encoders are missing. Thus it is hard to conclude if and how much gain is obtained by cooperative training over say individually training the descriptor and generator networks. Another comment is that in the “related work” section, I think relation with variational auto encoders (Kingma and Welling 2013) should be included. Despite limitations mentioned above, I think the ideas presented in the paper are intuitively appealing and worth discussing at ICLR. Paper would be considerably strengthened by adding more relevant baselines and addressing the training data size issues.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###3: Clear rejection', 'Rating:###6: Marginally above acceptance threshold']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",4,3,"Strengths:

1. The paper proposes a novel approach to training two deep neural networks in a cooperative manner. The idea is interesting and worth discussing.
2. The authors provide a clear and concise description of the proposed approach.

Weaknesses:

1. The paper lacks empirical evidence to support the proposed approach. The training datasets are too small and the baselines are missing.
2. The paper lacks clarity in its presentation. The authors use unclear notation and do not provide enough details about the energy function.
3. The paper lacks a clear evaluation metric. The authors use a simple reconstruction error metric which is not sufficient to evaluate the performance of the proposed approach.
4. The paper lacks a clear discussion of the limitations of the proposed approach. The authors do not discuss the potential issues with the proposed approach, such as the potential for overfitting.
5. The paper lacks a clear comparison to other approaches. The authors do not compare their approach to other deep learning approaches for image generation.

Rating: 6
Confidence: 3"
The Power of Sparsity in Convolutional Neural Networks | OpenReview,"Deep convolutional networks are well-known for their high computational and memory demands. Given limited resources, how does one design a network that balances its size, training time, and prediction accuracy? A surprisingly effective approach to trade accuracy for size and speed is to simply reduce the number of channels in each convolutional layer by a fixed fraction and retrain the network. In many cases this leads to significantly smaller networks with only minimal changes to accuracy. In this paper, we take a step further by empirically examining a strategy for deactivating connections between filters in convolutional layers in a way that allows us to harvest savings both in run-time and memory for many network architectures. More specifically, we generalize 2D convolution to use a channel-wise sparse connection structure and show that this leads to significantly better results than the baseline approach for large networks including VGG and Inception V3.","The paper is about channel sparsity in Convolution layer. The paper is well written and it elaborately discussed and investigated different approaches for applying sparsity. The paper contains detailed literature review. In result section, it showed the approach gives good results using 60% sparsity with reducing number of parameters, which can be useful in some embedded application with limited resource i.e. mobile devices. The main point is that the paper needs more detailed investigation on different dropout schedule. As mentioned implementation details section, they deactivate the connections by applying masks to parameter tensors, which is not helpful in speeding up the training and computation in convolution layer. They can optimize implementation to reduce computation time.  This paper aims to improve efficiency of convolutional networks by using a sparse connection structure in the convolution filters at each layer. Experiments are performed using MNIST, CIFAR-10 and ImageNet, comparing the sparse connection kernels against dense convolution kernels with about the same number of connections, showing the sparse structure (with more feature maps) generally performs better for similar numbers of parameters. Unfortunately, any theoretical efficiencies are not realized, since the implementation enforces the sparse structure using a zeroing mask on the weights. In addition, although the paper mentions that this method can be implemented efficiently and take advantage of contiguous memory reads/writes of current architectures, I still find it unclear whether this would be the case: The number of activation units is no smaller than when using a dense convolution of same dimension, and these activations (inputs and outputs) must be loaded/stored. The fact that the convolution is sparse saves only on the multiply/addition operation cost, not memory access for the activations, which can often be the larger amount of time spent. The section on incremental training is interesting, but feels short and preliminary, and any gains here also have yet to be realized. The precision is no better than for the original network, and as mentioned above, the implementation of the sparse structure is no faster than the original. Overall, the method and evaluations show that the basic approach has promise. However, it is unclear how real gains (in either speed or accuracy) might actually be found with it. Without this last step, it still seems incomplete to me for a conference paper.  The paper experiments with channel to channel sparse neural networks. The paper is well written and the analysis is useful. The sparse connection is not new but has not been experimented on large-scale problems like ImageNet. One of the reasons for that is the unavailability of fast implementations of randomly connected convolutional layers. The results displayed in figures 2, 3, 4, and, 5 show that sparse connections need the same number of parameters as the dense networks to reach to the best performance on the given tasks, but can provide better performance when there is a limited budget for the #parameters and #multiplyAdds. This paper is definitely informative but it does not reach to the conference acceptance level, simply because the idea is not new, the sparse connection implementation is poor, and the results are not very surprising.","['Rating:###7: Good paper, accept', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###4: Ok but not good enough - rejection']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",7,3,"Strengths: 
1. The paper is well written and the analysis is useful.
2. The paper experiments with channel to channel sparse neural networks.
Weaknesses: 
1. The sparse connection is not new and has not been experimented on large-scale problems like ImageNet.
2. The implementation of the sparse structure is poor.
3. The results are not very surprising.
Rating: 6
Confidence: 3"
A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Samples | OpenReview,"Most machine learning classifiers, including deep neural networks, are vulnerable to adversarial examples. Such inputs are typically generated by adding small but purposeful modifications that lead to incorrect outputs while imperceptible to human eyes. The goal of this paper is not to introduce a single method, but to make theoretical steps toward fully understanding adversarial examples. By using concepts from topology, our theoretical analysis brings forth the key reasons why an adversarial example can fool a classifier ( ) and adds its oracle ( , like human eyes) in such analysis. By investigating the topological relationship between two (pseudo)metric spaces corresponding to predictor and oracle , we develop necessary and sufficient conditions that can determine if is always robust (strong-robust) against adversarial examples according to . Interestingly our theorems indicate that just one unnecessary feature can make not strong-robust, and the right feature representation learning is the key to getting a classifier that is both accurate and strong robust.","Under my point of view, the robustness of a classifier against adversarial noise it is interesting if we find any relationship between that robustness and generalization to new unseen test samples. I guess that this relationship is direct in most of the problems but perhaps classifier C1 could be more robust than C2 against adv. noise but not better for new unseen samples from the task in consideration. Best results on new unseen samples are normally related to robustness against the common distortions of the data, e.g. invariance to scale, rotation… than robustness to adv. noise. I can not see any direct conclusion from table 5 results. Essentially i am not convinced about the necessity to measure the robustness against adversarial noise.  This paper aims at making three contributions: - Charecterizing robustness to adversarials in a topological manner. - Connecting the topological characterization to more quantitative measurements and evaluating deep networks. - Using Siamese network training to create models robust to adversarial in a practical manner and evaluate their properties. In my opinion the paper would improve greatly if the first, topological analysis attempt would be removed from the paper altogether. A central notion of the paper is the abstract characterization of robustness. The main weakness is the notion of strong robustness itself, which is an extremely rigid notion. It requires the partitioning of the predictor function by class to match the exact partitioning of the oracle. This robustness is almost never the case in real life: it requires that the predictor is almost perfect. The main flaw however is that the output space is assumed to have discrete topology and continuity is assumed for the classifier. Continuity of the classifier wrt. a discrete output is also never really satisfied. However, if the output space is assumed to be continues values with an interesting topology (like probabilities), then the notion of strong robustness becomes so constrained and strict, that it has even less practical sense and relevance. Based on those definition, several uninteresting, trivial consequences follow. They seem to be true, with inelegant proofs, but that matters little as they seem irrelevant for any practical purposes. The second part is a well executed experiment by training a Siamese architecture with an explicit additional robustness constraint. The approach seems to be working very well, but is compared only to a baseline (stability training) which performs worse than the original model without any trainings for adversarials. This is strange as adversarial examples have been studied extensively in the past year and several methods claimed improvements over the original model not trained for robustness. The experimental section and approach would look interesting, if it were compared with a stronger baseline, however the empty theoretical definitions and analysis attempts make the paper in its current form unappealing.  This paper theoretically analyzes the adversarial phenomenon by modeling the topological relationship between the feature space of the trained and the oracle discriminate function. In particular, the (complicated) discriminant function (f) is decomposed into a feature extractor (g) and a classifier (c), where the feature extractor (g) defines the feature space. The main contribution of this paper is to propose abstract understanding and analysis for adversarial phenomenon, which is interesting and important. However, this paper also has the following problems. 1) It is not clear how the classifier c can affect the overall robustness to adversarial noises. The classifier c seems absent from the analysis, which somehow indicates that the classifier does not matter. (Please correct me if it is not true) This is counter-intuitive. For example, if we always take the input space as the feature space and the entire f as the classifier c, the strong robustness can always hold. I am also wondering if the metric d has anything to do with the classifier c. 2) A very relevant problem is how to decompose f into g and c. For examples, one can take any intermediate layer or the input space as the feature space for a neural network. Will this affect the analysis of the adversarial robustness? 3) The oracle is a good concept. However, it is hard to explicitly define it. In this paper, the feature space of the oracle is just the input image space, and the inf-norm is used as the metric. This implementation makes the algorithm in Section 4 quite similar to existing methods (though there are some detailed differences as mentioned in the discussion). Due to the above problems, I feel that some aspects of the paper are not ready. If the problems are resolved or better clarified, I believe a higher rating can be assigned to this paper. In addition, the main text of this paper is somehow too long, the arguments can be more focused if the main paper become more concise.  A promising attempt to quantify the difference between human annotator and DNN. The contribution of defining topological equivalence of two metric spaces is significant to understand the origin of adversarial examples. Appreciate the examples showed in Figure2 and Figure 3, straightforward and intuitively helpful. More interestingly, a useful measure is provided for quantify the robustness of DNN model. Researchers can use this measure to compare different DNN models. Some critiques: It might be nicer to think more about the detailed mechanics of human vision. In the early state of primary visual cortex, feature space might be linear, L2 norm might be sufficient to capture the key difference between DNN and human metrics. However, when the information flow goes deep, single one metric might not be sufficient when the Oracle feature space is highly curved. Tracking and reducing the metric distance in cognitive level would be extremely hard. This paper offers a pioneering advance to tackle this problem. Hope more work can be done to further our understanding towards the limitation of DNN.","['Rating:###5: Marginally below acceptance threshold', 'Rating:###3: Clear rejection', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###10: Top 5% of accepted papers, seminal paper']","['Confidence:###2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",5,2,Strengths: 1. The paper presents a theoretical framework for understanding the robustness of deep neural networks against adversarial examples. 2. The authors provide a clear and concise presentation of the theoretical framework. 3. The paper is well-structured and easy to follow. Weaknesses: 1. The paper is too long. The main text is too lengthy and some parts seem to be repetitive. 2. The theoretical framework is too abstract. The authors seem to be trying to prove some trivial results. 3. The paper does not provide any practical results. The authors only provide a theoretical framework and some experiments that are not very interesting. Rating: 6 Confidence: 3
Inference and Introspection in Deep Generative Models of Sparse Data | OpenReview,"Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model.","The paper claims improved inference for density estimation of sparse data (here text documents) using deep generative Gaussian models (variational auto-encoders), and a method for deriving word embeddings from the model*s generative parameters that allows for a degree of interpretability similar to that of Bayesian generative topic models. To discuss the contributions I will quickly review the generative story in the paper: first a K-dimensional latent representation is sampled from a multivariate Gaussian, then an MLP (with parameters  heta) predicts unnormalised potentials over a vocabulary of V words, the potentials are exponentiated and normalised to make the parameters of a multinomial from where word observations are repeatedly sampled to make a document. Here intractable inference is replaced by the VAE formulation where an inference network (with parameters phi) independently predicts for each document the mean and variance of a normal distribution (amenable to reparameterised gradient computation). The first, and rather trivial, contribution is to use tf-idf features to inject first order statistics (a global information) into local observations. The authors claim that this is particularly helpful in the case of sparse data such as text. The second contribution is more interesting. In optimising generative parameters ( heta) and variational parameters (phi), the authors turn to a treatment which is reminiscent of the original SVI procedure. That is, they see the variational parameters phi as *global* variational parameters, and the predicted mean mu(x) and covariance Sigma(x) of each observation x are treated as *local* variational parameters. In the original VAE, local parameters are not directly optimised, instead they are indirectly optimised via optimisation of the global parameters utilised in their prediction (shared MLP parameters). Here, local parameters are optimised holding generative parameters fixed (line 3 of Algorithm 1). The optimised local parameters are then used in the gradient step of the generative parameters (line 4 of Algorithm 1). Finally, global variational parameters are also updated (line 5). Whereas indeed other authors have proposed to optimise local parameters, I think that deriving this procedure from the more familiar SVI makes the contribution less of a trick and easier to relate to. Some things aren*t entirely clear to me. I think it would have been nice if the authors had shown the functional form of the gradient used in step 3 of Algorithm 1. The gradient step for global variational parameters (line 5 of Algorithm 1) uses the very first prediction of local parameters (thus ignoring the optimisation in step 3), this is unclear to me. Perhaps I am missing a fundamental reason why that has to be the case (either way, please clarify). The authors argue that this optimisation turns out helpful to modelling sparse data because there is evidence that the generative model p_ heta(x|z) suffers from poor initialisation. Please, discuss why you expect the initialisation problem to be worse in the case of sparse data. The final contribution is a neat procedure to derive word embeddings from the generative model parameters. These embeddings are then used to interpret what the model has learnt. Interestingly, these word embeddings are context-sensitive once that the latent variable models an entire document. About Figures 2a and 2b: the caption says that solid lines indicate validation perplexity for M=1 (no optimisation of local parameters) and dashed lines indicate M=100 (100 iterations of optimisation of local parameters), but the legends of the figures suggest a different reading. If I interpret the figures based on the caption, then it seems that indeed deeper networks exposed to more data benefit from optimisation of local parameters. Are the authors pretty sure that in Figure 2b models with M=1 have reached a plateau (so that longer training would not allow them to catch up with M=100 curves)? As the authors explain in the caption, x-axis is not comparable on running time, thus the question. The analysis of singular values seems like an interesting way to investigate how the model is using its capacity. However, I can barely interpret Figures 2c and 2d, I think the authors could have walked readers through them. As for the word embedding I am missing an evaluation on a predictive task. Also, while illustrative, Table 2b is barely reproducible. The text reads *we create a document comprising a subset of words in the the context’s Wikipedia page.* which is rather vague. I wonder whether this construct needs to be carefully designed in order to get Table 2b. In sum, I have a feeling that the inference technique and the embedding technique are both useful, but perhaps they should have been presented separately so that each could have been explored in greater depth.  First I would like to apologize for the delay in reviewing. Summary : In this paper a variational inference is adapted to deep generative models, showing improvement for non-negative sparse dataset. The authors offer as well a method to interpret the data through the model parameters. The writing is generally clear. The methods seem correct. The introspection approach appears to be original. I found very interesting the experiment on the polysemic word embedding. I would however have like to see how the obtained embedding would perform with respect to other more common embeddings in solving a supervised task. Minor : Eq. 2: too many closing parentheses  This paper introduces three tricks for training deep latent variable models on sparse discrete data: 1) tf-idf weighting 2) Iteratively optimizing variational parameters after initializing them with an inference network 3) A technique for improving the interpretability of the deep model The first idea is sensible but rather trivial as a contribution. The second idea is also sensible, but is conceptually not novel. What is new is the finding that it works well for the dataset used in this paper. The third idea is interesting, and seems to give qualitatively reasonable results. The quantitative semantic similarity results don’t seem that convincing, but I am not very familiar with the relevant literature and therefore cannot make a confident judgement on this issue.  This paper presents a small trick to improve the model quality of variational autoencoders (further optimizing the ELBO while initializing it from the predictions of the q network, instead of just using those directly) and the idea of using Jacobian vectors to replace simple embeddings when interpreting variational autoencoders. The idea of the Jacobian as a natural replacement for embeddings is interesting, as it does seem to cleanly generalize the notion of embeddings from linear models. It*d be interesting to see comparisons with other work seeking to provide context-specific embeddings, either by clustering or by smarter techniques (like Neelakantan et al, Efficient non-parametric estimation of multiple embeddings per word in vector space, or Chen et al A Unified Model for Word Sense Representation and Disambiguation). With the evidence provided in the experimental section of the paper it*s hard to be convinced that the Jacobian of VAE-generated embeddings is substantially better at being context-sensitive than prior work. Similarly, the idea of further optimizing the ELBO is interesting but not fully explored. It*s unclear, for example, what is the tradeoff between the complexity of the q network and steps further optimizing the ELBO, in terms of compute versus accuracy. Overall the ideas in this paper are good but I*d like to see them a little more fleshed out.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###7: Good paper, accept', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",6,4,"Strengths: 1. The paper introduces a new method for variational inference in deep generative models. 2. The authors provide a clear and simple method for deriving word embeddings from the model parameters. Weaknesses: 1. The paper is not very well written. The text is often unclear and sometimes confusing. 2. The experimental results are not very convincing. The paper claims that the Jacobian of VAE-generated embeddings is substantially better at being context-sensitive than prior work, but the evidence provided in the experimental section of the paper is not very convincing. 3. The paper does not provide a clear evaluation of the proposed method. The authors do not provide a clear evaluation of the proposed method on a supervised task. 4. The paper does not provide a clear discussion of the results. The authors do not provide a clear discussion of the results and do not explain why the proposed method is better than prior work. Rating: 6 Confidence: 3"
Revisiting Classifier Two-Sample Tests | OpenReview,"The goal of two-sample tests is to assess whether two samples, and , are drawn from the same distribution. Perhaps intriguingly, one relatively unexplored method to build two-sample tests is the use of binary classifiers. In particular, construct a dataset by pairing the examples in with a positive label, and by pairing the examples in with a negative label. If the null hypothesis `` ** is true, then the classification accuracy of a binary classifier on a held-out subset of this dataset should remain near chance-level. As we will show, such emph{Classifier Two-Sample Tests} (C2ST) learn a suitable representation of the data on the fly, return test statistics in interpretable units, have a simple null distribution, and their predictive uncertainty allow to interpret where and differ. The goal of this paper is to establish the properties, performance, and uses of C2ST. First, we analyze their main theoretical properties. Second, we compare their performance against a variety of state-of-the-art alternatives. Third, we propose their use to evaluate the sample quality of generative models with intractable likelihoods, such as Generative Adversarial Networks (GANs). Fourth, we showcase the novel application of GANs together with C2ST for causal discovery.","I would like first to apologize for the delay. Summary: A framework for two-samples statistical test using binary classification is proposed. It allows multi-dimensional sample testing and an interpretability that other tests lack. A theoritical analysis is provided and various empirical tests reported. A very interesting approach. I have however two main concerns. The clarity of the presentation is obscured by too much content. It would be more interesting if the presentation could be somewhat self-contained. You could consider making 2 papers out of this paper. Seriously, you cram a lot of experiments in this paper. But the setting of the experiments is not really explained. We are supposed to have read Jitkrittum et al., 2016, Radford et al., 2016, Yu et al., 2015, etc. All this is okay but reduces your public to a very few. For example, if I am not mistaken, you never explained what SCF is, despite the fact that its performances are reported. As a second point, given also that the number of submissions to this conference are exploding, I would like to challenge you with the following question: Why is this work significant to the representation learning community?  The submission considers the setting of 2-sample testing from the perspective of evaluating a classifier. For a classifier between two samples from the same distribution, the distribution of the classification accuracy follows a simple form under the null hypothesis. As such, a straightforward threshold can be derived for any classifier. Finding a more powerful test then amounts to training a better classifier. One may then focus efforts, e.g. on deep neural networks, for which statistics such as the MMD may be very difficult to characterize. + The approach is sound and very general + The paper is timely in that deep learning has had huge impacts in classification and other prediction settings, but has not had as big an impact on statistical hypothesis testing as kernel methods have - The discussion of the relationship to kernel-MMD has not always been as realistic as it could have been. For example, the kernel-MMD can also be seen as a classifier based approach, so a more fair discussion could be provided. Also, the form of kernel-MMD used in the comparisons is a bit contradictory to the discussion as well * The linear kernel-MMD is used which is less powerful than the quadradic kernel-MMD (the authors have justified this from the perspective of computation time) * The kernel-MMD is argued against due to its unwieldy distribution under the null, but the linear time kernel-MMD (see also Zaremba et al., NIPS 2013) has a Gaussian distribution under the null. Arthur Gretton*s comment from Dec 14 during the discussion period was very insightful and helpful. If these insights and additional experiments comparing the kernel-MMD to the classifier threshold on the blobs dataset could be included, that would be very helpful for understanding the paper. The open review format gives an excellent opportunity to assign proper credit for these experiments and insights by citing the comment.  ## Paper summary The paper reconsiders the idea of using a binary classifier to do two-sample testing. The idea is to split the sample into two disjoint training and test sets, train a classifier on the training set, and use the accuracy on the test set as the test statistic. If the accuracy is above chance level, one concludes that the two samples are from different distributions i.e., reject H0. A theoretical result on an asymptotic approximate test power is provided. One implication is that the test is consistent, assuming that the classifier is better than coin tossing. Experiments on toy problems, evaluation of GANs, and causal discovery verify the effectiveness of the test. In addition, when the classifier is a neural net, examining the first linear filter layer allows one to see features which are most activated. The result is an interpretable visual indicator of how the two samples differ. ## Review summary The paper is well written and easy to follow. The idea of using a binary classifier for a two-sample testing is not new, as made clear in the paper. The main contributions are the analysis of the asymptotic test power, the use of modern deep nets as the classifier in this context, and the empirical studies on various tasks. The empirical results are satisfactorily convincing. Although not much discussion is made on why the method works well in practice, overall contributions have a potential to start a new direction of research on model criticisms of generative models, as well as visualization of where a model fails. I vote for an acceptance. ## Major comments / questions My main concern is on Theorem 1 (asymptotic test power) and its assumptions. But, I understand that these can be fixed as discussed below. * Under H0, the distribution of the test statistic (i.e., sum of 0-1 classification results) follows Binomial(nte, 1/2) as stated. However, under H1, terms in the sum are independent but *not* identical Bernoulli random variable. This is because each term depends on a data point z_i, which can be from either P or Q. So, in the paragraph in Sec3.1: *... the random variable n_te hat{t} follows a Binomial(nte, p)...* is not correct. Essentially p depends on z_i. It should follow a Poisson binomial distribution. * In the same paragraph, for the same reason, the alternative distribution of Binomial(nte, p=p_{risk}) is probably not correct. I guess you mention it to use Moivre-Laplace to get the asymptotic normality. Anyway, I see no reason why you would need this statement as the Binomial is not required in the proof, but only its asymptotic normality. A variant of the central limit theorem (instead of the Moivre-Laplace theorem) for independent, non-identical variables would still allow you to conclude the asymptotic normality of the Poisson binomial (with some conditions). See for example http://stats.stackexchange.com/questions/5347/how-can-i-efficiently-model-the-sum-of-bernoulli-random-variables * The statement in Theorem 1 should be more precise. For instance, *Assume the assumptions on the classifier given in the previous paragraph. Then, for large n, the power of C2ST is approximate Phi(..).* The current version is *The power of C2ST is Phi(..).* * The proof of Theorem 1 should be more precise regarding which quantities are exact, which are approximate. Both the null and alternative normal distributions are approximate for finite nte, for instance. * It is unclear to me why the paper includes independence tests in the experiments. It does imply that the proposed test can be used to do an independence test. But, isn*t this also true for other two-sample tests where x,y can be stacked together? This seems ad-hoc and raises further questions regarding the consistency, what type of dependency can be detected, etc. These points are not discussed. * Comment: By using classification accuracy as a proxy, one should expect the test power, for a given sample size n, to be lower than a direct statistic like MMD. A vague analogy would be the t-test (based on the values of the data) vs. the sign test (based on only whether x_i < y_j, not the actual values). The classifier test is in some sense reminiscent of the sign test i.e., passing data points through a classifier and binarizing the output. For sufficiently large n, the test can still correctly detect the difference as shown empirically. ## Minor comments / questions * Section 2, in the paragraph on the four steps of testing, the random variable for the statistic T is undefined. * In the last paragraph of Sec.2: *kernel two-sample tests require the prescription of a manually-engineered representation of the data under study, and return values in units that are difficult to interpret.* This is too vague. Manually-engineered representation? Wouldn*t a neural net require even more of the explicitness of the representation? * In sec.3.2, it is better to also state the assumptions on the classifier in a slightly less technical way. Specifically, under H0, you assume that the classifier is not biased, and under H1 you assume that it can learn well (better than coin tossing).","['Rating:###7: Good paper, accept', 'Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###7: Good paper, accept']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",7,3,"Strengths: 1. The paper is well written and easy to follow. 2. The idea of using a classifier to do two-sample testing is not new, as made clear in the paper. 3. The main contributions are the analysis of the asymptotic test power, the use of modern deep nets as the classifier in this context, and the empirical studies on various tasks. 4. The empirical results are satisfactorily convincing. 5. The paper is timely in that deep learning has had huge impacts in classification and other prediction settings, but has not had as big an impact on statistical hypothesis testing as kernel methods have. Weaknesses: 1. The paper is too long. 2. The paper is too long. 3. The paper is too long. 4. The paper is too long. 5. The paper is too long. Rating: 6 Confidence: 3"
Adaptive Feature Abstraction for Translating Video to Language | OpenReview,"Previous models for video captioning often use the output from a specific layer of a Convolutional Neural Network (CNN) as video representations, preventing them from modeling rich, varying context-dependent semantics in video descriptions. In this paper, we propose a new approach to generating adaptive spatiotemporal representations of videos for a captioning task. For this purpose, novel attention mechanisms with spatiotemporal alignment is employed to adaptively and sequentially focus on different layers of CNN features (levels of feature ``abstraction**), as well as local spatiotemporal regions of the feature maps at each layer. Our approach is evaluated on three benchmark datasets: YouTube2Text, M-VAD and MSR-VTT. Along with visualizing the results and how the model works, these experiments quantitatively demonstrate the effectiveness of the proposed adaptive spatiotemporal feature abstraction for translating videos to sentences with rich semantics.","This paper presents a model for video captioning with both soft and hard attention, using a C3D network for the encoder and a RNN for the decoder. Experiments are presented on YouTube2Text, M-VAD, and MSR-VTT. While the ideas of image captioning with soft and hard attention, and video captioning with soft attention, have already been demonstrated in previous work, the main contribution here is the specific architecture and attention over different layers of the CNN. The work is well presented and the experiments clearly show the benefit of attention over multiple layers. However, in light of previous work in captioning, the contribution and resulting insights is too incremental for a conference paper at ICLR. Further experiments and analysis of the main contribution would strengthen the paper, but I would recommend resubmission to a more suitable venue.  The authors apply the image captioning architecture of Xu et al. 2015 to video captioning. The model is extended to have attention over multiple layers of the ConvNet instead of just a single layer. Experiments on YouTube2Text, M-VAD and MSR-VTT show that this works better than only using one of the layers at a time. I think this is solid work on the level of a well-executed course project or a workshop paper. The model makes sense, it is adequately described, and the experiments show that attending over multiple layers works better than attending over any one layer in isolation. Unfortunately, I don*t think there is enough to get excited about here from a technical perspective and it*s not clear what value the paper brings to the community. Other aspects of the paper, such as including the hard attention component, don*t seem to add to the paper but take up space. If the authors want to contribute a detailed, focused exploration of multi-level features this could become a more valuable paper, but in that case I would expect a much more thorough exploration of the choices and tradeoffs of different schemes without too many spurious aspects such as video features, hard attention, etc.  1) Summary This paper proposes a video captioning model based on a 3D (space+time) convnet (C3D) encoder and a LSTM decoder. The authors investigate the benefits of using attention mechanisms operating both at the spatio-temporal and layer (feature abstraction) levels. 2) Contributions + Well motivated and implemented attention mechanism to handle the different shapes of C3D feature maps (along space, time, and feature dimensions). + Convincing quantitative and qualitative experiments on three challenging datasets (Youtube2Text, M-VAD, MSR-VTT) showing clearly the benefit of the proposed attention mechanisms. + Interesting comparison of soft vs hard attention showing a slight performance advantage for the (simpler) soft attention mechanism in this case. 3) Suggestions for improvement Hypercolumns comparison: As mentioned during pre-review questions, it would be interesting to compare to the hypercolumns of https://arxiv.org/abs/1411.5752, as they are an alternative to the proposed attention mechanisms, with the same purpose of leveraging different feature abstraction levels. Minor clarifications in the text and figures as agreed with the authors in our pre-review discussions. 4) Conclusion Although the novelty with existing video captioning approaches is limited, the paper is relevant to ICLR, as the proposed simple but efficient implementation and benefits of spatio-temporal + feature abstraction attention are clearly validated in this work.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###4: Ok but not good enough - rejection', 'Rating:###7: Good paper, accept']","['Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",4,5,"Strengths: 1) Summary This paper proposes a video captioning model based on a 3D (space+time) convnet (C3D) encoder and a LSTM decoder. The authors investigate the benefits of using attention mechanisms operating both at the spatio-temporal and layer (feature abstraction) levels. 2) Contributions + Well motivated and implemented attention mechanism to handle the different shapes of C3D feature maps (along space, time, and feature dimensions). + Convincing quantitative and qualitative experiments on three challenging datasets (Youtube2Text, M-VAD, MSR-VTT) showing clearly the benefit of the proposed attention mechanisms. + Interesting comparison of soft vs hard attention showing a slight performance advantage for the (simpler) soft attention mechanism in this case. 3) Suggestions for improvement Hypercolumns comparison: As mentioned during pre-review questions, it would be interesting to compare to the hypercolumns of https://arxiv.org/abs/1411.5752, as they are an alternative to the proposed attention mechanisms, with the same purpose of leveraging different feature abstraction levels. Minor clarifications in the text and figures as agreed with the authors in our pre-review discussions. 4) Conclusion Although the novelty with existing video captioning approaches is limited, the paper is relevant to ICLR, as the proposed simple but efficient implementation and benefits of spatio-temporal + feature abstraction attention are clearly validated in this work. Weaknesses: 1) Summary The paper"
OMG: Orthogonal Method of Grouping With Application of K-Shot Learning | OpenReview,"Training a classifier with only a few examples remains a significant barrier when using neural networks with large number of parameters. Though various specialized network architectures have been proposed for these k-shot learning tasks to avoid overfitting, a question remains: is there a generalizable framework for the k-shot learning problem that can leverage existing deep models as well as avoid model overfitting? In this paper, we proposed a generalizable k-shot learning framework that can be used on any pre-trained network, by grouping network parameters to produce a low-dimensional representation of the parameter space. The grouping of the parameters is based on an orthogonal decomposition of the parameter space. To avoid overfitting, groups of parameters will be updated together during the k-shot training process. Furthermore, this framework can be integrated with any existing popular deep neural networks such as VGG, GoogleNet, ResNet, without any changes in the original network structure or any sacrifices in performance. We evaluate our framework on a wide range of intra/inter-dataset k-shot learning tasks and show state-of-the-art performance.","This paper proposes a k-shot learning framework that can be used on existing pre-trained networks by grouping filters that produce similar activations. The grouped filters are learned together to address overfitting when only few training samples are available. The idea of the paper is interesting there are some encouraging results, but the current version doesn*t seem ready for publication: Performance: The method should be compared with other state-of-the-art k-shot learning methods (e.g., Matching Networks by Vinyals et al., 2016). It*s not clear how this method compares against them. Missing explanation: Experimental setting for k-shot learning should be more detailed. Measure: Accuracy difference does not look like a good idea for comparing the baseline method and the proposed one. Just raw accuracies would be fine. Many grammatical errors and inappropriate formatting of citations, such as: M. et al. (2011) ImageNet (Alex et al. (2012)) Judy et al. (2013): this reference appears three times in the reference section.  This paper proposes a regularization technique for k-shot learning based on orthogonal grouping of units in a neural network. The units within a group are forced to be maximally similar, at the same time the units from different groups are encouraged to be orthogonal. While I like the motivation of the approach, the empirical analysis provided in the paper doesn’t look particularly convincing. My main concerns are the following: 1. The method is sensitive to the values of alpha and beta and a poor choice of those hyperparameters can lead to a quite drastic drop in performance comparing the minor gains one gets when alpha and beta are set properly. 2. It seems strange that the best performance is obtained when the group*s size ratio is 0.5. From the figures in the paper, it follows that usually, one has more “orthogonal” groups in a filter bank. I have an impression that the empirical evidence doesn’t align well with the motivation of the proposed approach. 3. The paper contains a significant amount of typos and incorrectly formatted references. There are also several places in the manuscript that I found hard to understand due to unusual phrasing. I would like to thank the authors for answering/addressing my pre-review questions. I would be grateful if the authors could provide more clarifications of the following: 1. Question 2: I’m not sure if modifying  heta_{map} alone would result in any learning at all. Do I understand correctly that  heta_{map} is only used to define groups? If so, then I don’t see how the proposed method can be used in the purely unsupervised regime. 2. Question 3: I was not referring to the fixed clustering based on the filter of the pre-trained network. One can perform that clustering at every step of the k-shot learning process. I’m not sure I understand why the authors visualize grouping of _filters_ while in the actual algorithm they group _activations_. Overall, the paper is quite interesting but needs a stronger empirical justification of the approach as well as a better presentation of the material.  The authors of this work propose a learnable approach to reducing the dimensionality of learned filters in deep neural networks. This is an interesting approach, but the presented work looks a bit raw. 1. There are many typos in this manuscript. 2. The experimental results are rather weak and don*t show much improvement in accuracy. Instead the authors could position this work as a compression mechanism and would have to compare to low rank approximation of filters for DNNs. Yet this is not done. 3. Aside from compression, OMG can be viewed as a form of regularization to reduce the unnecessary capacity of the network to improve generalization. Again, this is not addressed in enough detail. 4. If the authors care to compare their approach to other 1-shot learning methods, then they would have to evaluate their approach with siamese and triplet learning networks. This isn*t done.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###4: Ok but not good enough - rejection', 'Rating:###4: Ok but not good enough - rejection']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature']",4,4,"Strengths: 1. The idea of orthogonal grouping of units in a neural network is interesting. 2. The authors provide a clear and well-structured paper. Weaknesses: 1. The experimental results are weak. The authors do not compare their approach to other 1-shot learning methods. 2. The paper is not well-formatted. There are many typos and grammatical errors. 3. The authors do not provide a clear motivation for the choice of alpha and beta. 4. The paper does not provide a clear explanation of the group*s size ratio. 5. The paper does not provide a clear explanation of the choice of the number of groups. 6. The paper does not provide a clear explanation of the choice of the number of iterations. 7. The paper does not provide a clear explanation of the choice of the learning rate. 8. The paper does not provide a clear explanation of the choice of the optimizer. 9. The paper does not provide a clear explanation of the choice of the regularization strength. 10. The paper does not provide a clear explanation of the choice of the batch size. Rating: 4
Confidence: 2"
Rethinking Numerical Representations for Deep Neural Networks | OpenReview,"With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms. We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and VGG, achieves an average speedup of 7.6x with less than 1% degradation in inference accuracy relative to a state-of-the-art baseline platform representing the most sophisticated hardware using single-precision floating point. To facilitate the use of such customized precision, we also present a novel technique that drastically reduces the time required to derive the optimal precision configuration.","The paper studies the impact of using customized number representations on accuracy, speed, and energy consumption of neural network inference. Several standard computer vision architectures including VGG and GoogleNet are considered for the experiments, and it is concluded that floating point representations are preferred over fixed point representations, and floating point numbers with about 14 bits are sufficient for the considered architectures resulting in a small loss in accuracy. The paper provides a nice overview of floating and fixed point representations and focuses on an important aspect of deep learning that is not well studied. There are several aspects of the paper that could be improved, but overall, I am leaned toward weak accept assuming that the authors address the issues below. 1- The paper is not clear that it is only focusing on neural network inference. Please include the word *inference* in the title / abstract to clarify this point and mention that the findings of the paper do not necessarily apply to neural network training as training dynamics could be different. 2- The paper does not discuss the possibility of adopting quantization tricks during training, which may result in the use of fewer bits at inference. 3- The paper is not clear whether in computing the running time and power consumption, it includes all of the modules or only multiply-accumulate units? Also, how accurate are these numbers given different possible designs and the potential difference between simulation and production? Please elaborate on the details of simulation in the paper. 4- The whole discussion about *efficient customized precision search* seem unimportant to me. When such important hardware considerations are concerned, even spending 20x simulation time is not that important. The exhaustive search process could be easily parallelized and one may rather spend more time at simulation at the cost of finding the exact best configuration rather than an approximation. That said, weak configurations could be easily filtered after evaluating just a few examples. 5- Nvidia*s Pascal GP100 GPU supports FP16. This should be discussed in the paper and relevant Nvidia papers / documents should be cited. More comments: - Parts of the paper discussing *efficient customized precision search* are not clear to me. - As future work, the impact of number representations on batch normalization and recurrent neural networks could be studied.  This paper explores the performance-area-energy-model accuracy tradeoff encountered in designing custom number representations for deep learning inference. Common image-based benchmarks: VGG, Googlenet etc are used to demonstrate that fewer than1 6 bits in a custom floating point representation can lead to improvement in runtime performance and energy efficiency with only a small loss in model accuracy. Questions: 1. Does the custom floating point number representation take into account support for de-normal numbers? 2. Is the custom floating point unit clocked at the same frequency as the baseline 32-bit floating point unit? If not, what are the different frequencies used and how would this impact the overall system design in terms of feeding the data to the floating point units from the memory Comments: 1. I would recommend using the IEEE half-precision floating point (1bit sign, 5bit exponent, and 10bit mantissa) as a baseline for comparison. At this point, it is well known in both the ML and the HW communities that 32-bit floats are an overkill for DNN inference and major HW vendors already include support for IEEE half-precision floats. 2. In my opinion, the claim that switching to custom floating point lead to a YY.ZZ x savings in energy is misleading. It might be true that the floating-point unit itself might consume less energy due to smaller bit-width of the operands, however a large fraction of the total energy is spent in data movement to/from the memories. As a result, reducing the floating point unit’s energy consumption by a certain factor will not translate to the same reduction in the total energy. A reader not familiar with such nuances (for example a typical member of the ML community), may be mislead by such claims. 3. On a similar note as comment 2, the authors should explicitly mention that the claimed speedup is that of the floating point unit only, and it will not translate to the overall workload speedup. Although the speedup of the compute unit is roughly quadratic in the bit-width, the bandwidth requirements scale linearly with bit-width. As a result, it is possible that these custom floating point units may be starved on memory bandwidth, in which case the claims of speedup and energy savings need to be revisited. 4. The authors should also comment on the complexities and overheads introduced in data accesses, designing the various system buses/ data paths when the number representation is not byte-aligned. Moving to a custom 14-bit number representation (for example) can improve the performance and energy-efficiency of the floating point unit, but these gains can be partially eroded due to the additional overhead in supporting non-byte aligned memory accesses.  The paper provides a first study of customized precision hardware for large convolutional networks, namely alexnet, vgg and googlenet. It shows that it is possible to achieve larger speed-ups using floating-point precision (up to 7x) when using fewer bits, and better than using fixed-point representations. The paper also explores predicting custom floating-point precision parameters directly from the neural network activations, avoiding exhaustive search, but i could not follow this part. Only the activations of the last layer are evaluated, but on what data ? On all the validation set ? Why would this be faster than computing the classification accuracy ? The results should be useful for hardware manufacturers, but with a catch. All popular convolutional networks now use batch normalization, while none of the evaluated ones do. It may well be that the conclusions of this study will be completely different on batch normalization networks, and fixed-point representations are best there, but that remains to be seen. It seems like something worth exploring. Overall there is not a great deal of novelty other than being a useful study on numerical precision trade-offs at neural network test time. Training time is also something of interest. There are a lot more researchers trying to train new networks fast than trying to evaluate old ones fast. I am also no expert in digital logic design, but my educated guess is that this paper is marginally below the acceptance threshold.","['Rating:###6: Marginally above acceptance threshold', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']",6,3,"Strengths: 1. The paper explores the impact of custom number representations on accuracy, speed, and energy consumption of neural network inference. 2. The paper provides a nice overview of floating and fixed point representations and focuses on an important aspect of deep learning that is not well studied. Weaknesses: 1. The paper is not clear that it is only focusing on neural network inference. Please include the word *inference* in the title / abstract to clarify this point and mention that the findings of the paper do not necessarily apply to neural network training as training dynamics could be different. 2. The paper does not discuss the possibility of adopting quantization tricks during training, which may result in the use of fewer bits at inference. 3. The paper is not clear whether in computing the running time and power consumption, it includes all of the modules or only multiply-accumulate units? Also, how accurate are these numbers given different possible designs and the potential difference between simulation and production? Please elaborate on the details of simulation in the paper. 4. The whole discussion about *efficient customized precision search* seem unimportant to me. When such important hardware considerations are concerned, even spending 20x simulation time is not that important. The exhaustive search process could be easily parallelized and one may rather spend more time at simulation at the cost of finding the exact best configuration rather than an approximation. That said, weak configurations could be easily filtered after evaluating just a few examples."
Calibrating Energy-based Generative Adversarial Networks | OpenReview,"In this paper, we propose to equip Generative Adversarial Networks with the ability to produce direct energy estimates for samples. Specifically, we propose a flexible adversarial training framework, and prove this framework not only ensures the generator converges to the true data distribution, but also enables the discriminator to retain the density information at the global optimal. We derive the analytic form of the induced solution, and analyze the properties. In order to make the proposed framework trainable in practice, we introduce two effective approximation techniques. Empirically, the experiment results closely match our theoretical analysis, verifying the discriminator is able to recover the energy of data distribution.","This paper addresses one of the major shortcomings of generative adversarial networks - their lack of mechanism for evaluating held-out data. While other work such as BiGANs/ALI address this by learning a separate inference network, here the authors propose to change the GAN objective function such that the optimal discriminator is also an energy function, rather than becoming uninformative at the optimal solution. Training this new objective requires gradients of the entropy of the generated data, which are difficult to approximate, and the authors propose two methods to do so, one based on nearest neighbors and one based on a variational lower bound. The results presented show that on toy data the learned discriminator/energy function closely approximates the log probability of the data, and on more complex data the discriminator give a good measure of quality for held out data. I would say the largest shortcomings of the paper are practical issues around the scalability of the nearest neighbors approximation and accuracy of the variational approximation, which the authors acknowledge. Also, since entropy estimation and density estimation are such closely linked problems, I wonder if any practical method for EGANs will end up being equivalent to some form of approximate density estimation, exactly the problem GANs were designed to circumvent. Nonetheless, the elegant mathematical exposition alone makes the paper a worthwhile contribution to the literature. Also, some quibbles about the writing - it seems that something is missing in the sentence at the top of pg. 5 *Finally, let*s whose discriminative power*. I*m not sure what the authors mean to say here. And the title undersells the paper - it makes it sound like they are making a small improvement to training an existing model rather than deriving an alternative training framework.  The authors present a method for changing the objective of generative adversarial networks such that the discriminator accurately recovers density information about the underlying data distribution. In the course of deriving the changed objective they prove that stability of the discriminator is not guaranteed in the standard GAN setup but can be recovered via an additional entropy regularization term. The paper is clearly written, including the theoretical derivation. The derivation of the additional regularization term seems valid and is well explained. The experiments also empirically seem to support the claim that the proposed changed objective results in a *better* discriminator. There are only a few issues with the paper in its current form: - The presentation albeit fairly clear in the details following the initial exposition in 3.1 and the beginning of 3.2 fails to accurately convey the difference between the energy based view of training GANs and the standard GAN. As a result it took me several passes through the paper to understand why the results don*t hold for a standard GAN. I think it would be clearer if you state the connections up-front in 3.1 (perhaps without the additional f-gan perspective) and perhaps add some additional explanation as to how c() is implemented right there or in the experiments (you may want to just add these details in the Appendix, see also comment below). - The proposed procedure will by construction only result in an improved generator and unless I misunderstand something does not result in improved stability of GAN training. You also don*t make such a claim but an uninformed reader might get this wrong impression, especially since you mention improved performance compared to Salimans et al. in the Inception score experiment. It might be worth-while mentioning this early in the paper. - The experiments, although well designed, mainly convey qualitative results with the exception of the table in the appendix for the toy datasets. I know that evaluating GANs is in itself not an easy task but I wonder whether additional more quantitative experiments could be performed to evaluate the discriminator performance. For example: one could evaluate how well the final discriminator does separate real from fake examples, how robust its classification is to injected noise (e.g. how classification accuracy changes for noised training data). Further one might wonder whether the last layer features learned by a discriminator using the changed objective are better suited for use in auxiliary tasks (e.g. classifying objects into categories). - Main complaint: It is completely unclear what the generator and discriminators look like for the experiments. You mention that code will be available soon but I feel like a short description at least of the form of the energy used should also appear in the paper somewhere (perhaps in the appendix).  The submission explores several alternatives to provide the generator function in generative adversarial training with additional gradient information. The exposition starts by describing a general formulation about how this additional gradient information (termed K(p_gen) could be added to the generative adversarial training objective function (Equation 1). Next, the authors prove that the shape of the optimal discriminator does indeed depend on the added gradient information (Proposition 3.1), which is unsurprising. Finally, the authors propose three particular alternatives to construct K(p_gen): the negative entropy of the generator distribution, the L2 norm of the generator distribution, and a constant function (which resembles the EBGAN objective of Zhao et al, 2016). The exposition moves then to an experimental evaluation of the method, which sets K(p_gen) to be the approximate entropy of the generator distribution. At this point, my intuition is that the objective function under study is the vanilla GAN objective, plus a regularization term that encourages diversity (high entropy) in the generator distribution. The hope of the authors is that this regularization will transform the discriminator into an estimate of the energy landscape of the data distribution. The experimental evaluation proceeds by 1) showing the contour plots of the obtained generator distribution for a 2D problem, 2) studying the generation diversity in MNIST digits, and 3) showing some samples for CIFAR-10 and CelebA. The 2D problem results are convincing, since one can clearly observe that the discriminator scores translate into unnormalized values of the density function. The MNIST results offer good intuition also: the more prototypical digits are assigned larger scores (unnormalized densities) by the discriminator, and the less prototypical digits are assigned smaller scores. The sample experiments from Section 5.3 are less convincing, since no samples from baseline models are provided for comparison. To this end, I would recommend the authors to clarify three aspects. First, we have seen that entropy regularization leads to a discriminator that estimates the energy landscape of the data distribution. But, how does this regularization reshape the generator function? It would be nice to see the mean MNIST digit according to the generator, and some other statistics if possible. Second, how do the samples produced by the proposed methods compare (visually speaking) to the state-of-the art? Third, what are the *shortcomings* of this method versus vanilla GAN? Too much computational overhead? What are the qualitative and quantitative differences between the two entropy estimators proposed in the manuscript? Overall, a clearly written paper. I vote for acceptance. As an open question to the authors: What breakthroughs should we pursue to derive a GAN objective where the discriminator is an estimate of the data density function, after training?","['Rating:###8: Top 50% of accepted papers, clear accept', 'Rating:###7: Good paper, accept', 'Rating:###8: Top 50% of accepted papers, clear accept']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",8,4,"Strengths: 1. The paper presents a novel approach to training GANs by incorporating additional gradient information. The authors propose three different ways to incorporate this information and provide a theoretical analysis of the resulting objective functions. 2. The paper is well-written and easy to follow. The authors provide a clear and concise explanation of the proposed method and its theoretical underpinnings. 3. The experimental results are well-designed and provide a good intuition about the effectiveness of the proposed method. Weaknesses: 1. The paper does not provide a clear motivation for the proposed method. The authors mention that the discriminator should be able to estimate the energy landscape of the data distribution, but it is not clear why this is a desirable property. 2. The paper does not provide a clear comparison to the state-of-the-art. The authors mention that the proposed method is better than the standard GAN objective, but it is not clear what the standard GAN objective is. 3. The paper does not provide a clear explanation of the limitations of the proposed method. The authors mention that the method may be computationally expensive, but it is not clear what the computational cost is. Rating: 7 Confidence: 4"
Machine Solver for Physics Word Problems | OpenReview,"We build a machine solver for word problems on the physics of a free falling object under constant acceleration of gravity. Each problem consists of a formulation part, describing the setting, and a question part asking for the value of an unknown. Our solver consists of two long short-term memory recurrent neural networks and a numerical integrator. The first neural network (the labeler) labels each word of the problem, identifying the physical parameters and the question part of the problem. The second neural network (the classifier) identifies what is being asked in the question. Using the information extracted by both networks, the numerical integrator computes the solution. We observe that the classifier is resilient to errors made by the labeler, which does a better job of identifying the physics parameters than the question. Training, validation and test sets of problems are generated from a grammar, with validation and test problems structurally different from the training problems. The overall accuracy of the solver on the test cases is 99.8%.","The authors describe a system for solving physics word problems. The system consists of two neural networks: a labeler and a classifier, followed by a numerical integrator. On the dataset that the authors synthesize, the full system attains near full performance. Outside of the pipeline, the authors also provide some network activation visualizations. The paper is clear, and the data generation procedure/grammar is rich and interesting. However, overall the system is not well motivated. Why did they consider this particular problem domain, and what challenges did they specifically hope to address? Is it the ability to label sequences using LSTM networks, or the ability to classify what is being asked for in the question? This has already been illustrated, for example, by work on POS tagging and by memory networks for the babi tasks. A couple of standard architectural modifications, i.e. bi-directionality and a content-based attention mechanism, were also not considered.  This paper build a language-based solver for simple physics problems (a free falling object under constant velocity). Given a natural language query sampled from a fixed grammar, the system uses two LSTM models to extract key components, e.g., physical parameters and the type of questions being asked, which are then sent to a numerical integrator for the answer. The overall performance in the test set is almost perfect (99.8%). Overall I found this paper quite interesting to read (and it is well written). However, it is not clear how hard the problem is and how much this approach could generalize over more realistic (and complicated) situations. The dataset are a bit small and might not cover the query space. It might be better to ask AMT workers to come up with more complicated queries/answers. The physics itself is also quite easy. What happens if we apply the same idea on billiards? In this case, even we have a perfect physics simulator, the question to be asked could be very deep and requires multi-hop reasoning. Finally, given the same problem setting (physics solver), in my opinion, a more interesting direction is to study how DNN can take the place of numerical integrator and gives rough answers to the question (i.e., intuitive physics). It is a bit disappointing to see that DNN is only used to extract the parameters while still a traditional approach is used for core reasoning part. It would be more interesting to see the other way round.  The paper uses neural networks to answer falling body physics questions by 1. Resolving the parameters of the problem, and 2. Figure out which quantity is in question, compute it using a numerical integrator and return it as an answer. Learning and inference are performed on artificially generated questions using a probabilistic grammar. Overall, the paper is clearly written and seems to be novel in its approach. The main problems I see with this work are: 1. The task is artificial, and it*s not clear how hard it is. The authors provide no baseline nor do they compare it to any real world problem. Without some measure of difficulty it*s hard to tell if a much simple approach will do better, or if the task even makes sense. 2. The labler LSTM uses only 10 hidden units. This is remarkably small for language modeling problems, and makes one further wonder about the difficulty of the task. The authors provide no reasoning for this choice.","['Rating:###4: Ok but not good enough - rejection', 'Rating:###4: Ok but not good enough - rejection', 'Rating:###5: Marginally below acceptance threshold']","['Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct', 'Confidence:###4: The reviewer is confident but not absolutely certain that the evaluation is correct']",4,4,"Strengths: 1. The paper is clearly written and the authors provide a good overview of the system. 2. The system is novel and seems to be effective. Weaknesses: 1. The task is artificial and it*s not clear how hard it is. The authors provide no baseline nor do they compare it to any real world problem. Without some measure of difficulty it*s hard to tell if a much simple approach will do better, or if the task even makes sense. 2. The labler LSTM uses only 10 hidden units. This is remarkably small for language modeling problems, and makes one further wonder about the difficulty of the task. The authors provide no reasoning for this choice. Rating: 6
Confidence: 3"
Shift Aggregate Extract Networks | OpenReview,"The Shift Aggregate Extract Network SAEN is an architecture for learning representations on social network data. SAEN decomposes input graphs into hierarchies made of multiple strata of objects. Vector representations of each object are learnt by applying *shift*, *aggregate* and *extract* operations on the vector representations of its parts. We propose an algorithm for domain compression which takes advantage of symmetries in hierarchical decompositions to reduce the memory usage and obtain significant speedups. Our method is empirically evaluated on real world social network datasets, outperforming the current state of the art.","the paper proposed a method mainly for graph classification. The proposal is to decompose graphs objects into hierarchies of small graphs followed by generating vector embeddings and aggregation using deep networks. The approach is reasonable and intuitive however, experiments do not show superiority of their approach. The proposed method outperforms Yanardag et al. 2015 and Niepert et al., 2016 on social networks graphs but are quite inferior to Niepert et al., 2016 on bio-informatics datasets. the authors did not report acccuracy for Yanardag et al. 2015 which on similar bio-ddatasets for example NCI1 is 80%, significantly better than achieved by the proposed method. The authors claim that their method is tailored for social networks graph more is not supported by good arguments? what models of graphs is this method more suitable?  The paper contributes to recent work investigating how neural networks can be used on graph-structured data. As far as I can tell, the proposed approach is the following: 1. Construct a hierarchical set of *objects* within the graph. Each object consists of multiple *parts* from the set of objects in the level below. There are potentially different ways a part can be part of an object (the different pi labels), which I would maybe call *membership types*. In the experiments, the objects at the bottom level are vertices. At the next level they are radius 0 (just a vertex?) and radius 1 neighborhoods around each vertex, and the membership types here are either *root*, or *element* (depending on whether a vertex is the center of the neighborhood or a neighbor). At the top level there is one object consisting of all of these neighborhoods, with membership types of *radius 0 neighborhood* (isn*t this still just a vertex?) or *radius 1 neighborhood*. 2. Every object has a representation. Each vertex*s representation is a one-hot encoding of its degree. To construct an object*s representation at the next level, the following scheme is employed: a. For each object, sum the representation of all of its parts having the same membership type. b. Concatenate the sums obtained from different membership types. c. Pass this vector through a multi-layer neural net. I*ve provided this summary mainly because the description in the paper itself is somewhat hard to follow, and relevant details are scattered throughout the text, so I*d like to verify that my understanding is correct. Some additional questions I have that weren*t clear from the text: how many layers and hidden units were used? What are the dimensionalities of the representations used at each layer? How is final classification performed? What is the motivation for the chosen *ego-graph* representation? The proposed approach is interesting and novel, the compression technique appears effective, and the results seem compelling. However, the clarity and structure of the writing is quite poor. It took me a while to figure out what was going on---the initial description is provided without any illustrative examples, and it required jumping around the paper to figure for example how the pi labels are actually used. Important details around network architecture aren*t provided, and very little in the way of motivation is given for many of the choices made. Were other choices of decomposition/object-part structures investigated, given the generality of the shift-aggregate-extract formulation? What motivated the choice of *ego-graphs*? Why one-hot degrees for the initial attributes? Overall, I think the paper contains a useful contribution on a technical level, but the presentation needs to be significantly cleaned up before I can recommend acceptance.  Some of the key details in this paper are very poorly explained or not even explained at all. The model sounds interesting and there may be something good here, but it should not be published in it*s current form. Specific comments: The description of the R_l,pi convolutions in Section 2.1 was unclear. Specifically, I wasn*t confident that I understood what the labels pi represented. The description of the SAEN structure in section 2.2 was worded poorly. My understanding, based on Equation 1, is that the *shift* operation is simply a summation of the representations of the member objects, and that the *aggregate* operation simply concatenates the representations from multiple relations. In the *shift* step, it seems more appropriate to average over the object*s member*s representations h_j, rather than sum over them. The compression technique presented in Section 2.3 requires that multiple objects at a level have the same representation. Why would this ever occur, given that the representations are real valued and high-dimensional? The text is unintelligible: *two objects are equivalent if they are made by same sets of parts for all the pi-parameterizations of the R_l,pi decomposition relation.* The *ego graph patterns* in Figure 1 and *Ego Graph Neural Network* used in the experiments are never explained in the text, and no references are given. Because of this, I cannot comment on the quality of the experiments.","['Rating:###5: Marginally below acceptance threshold', 'Rating:###5: Marginally below acceptance threshold', 'Rating:###3: Clear rejection']","['Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###3: The reviewer is fairly confident that the evaluation is correct', 'Confidence:###2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper']",5,3,"Strengths: 1. The paper proposes a novel approach to learning representations on social network data. 2. The authors provide a clear and concise summary of the proposed approach. Weaknesses: 1. The paper is poorly written. The text is unclear and difficult to follow. 2. The authors do not provide a clear motivation for the proposed approach. 3. The paper does not provide a clear explanation of the *ego graph* representation. 4. The authors do not provide a clear explanation of the *pi labels*. 5. The paper does not provide a clear explanation of the *shift*, *aggregate*, and *extract* operations. 6. The authors do not provide a clear explanation of the *R_l,pi convolution*. 7. The paper does not provide a clear explanation of the compression technique. 8. The authors do not provide a clear explanation of the *ego graph patterns*. 9. The paper does not provide a clear explanation of the *Ego Graph Neural Network*. 10. The paper does not provide a clear explanation of the final classification step. Rating: 4
Confidence: 2"
