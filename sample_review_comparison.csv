Original Review,Zero-Shot Output,Fine-Tuned Output
"Game of tic-tac-toe is considered. 1029 tic-tac-toe board combinations are chosen so that a single move will result into victory of either the black or the white player. There are 18 possible moves - 2 players x 9 locations. A CNN is trained from a visual rendering of the game board to these 18 possible outputs. CAM technique is used to visualize the salient regions in the inputs responsible for the prediction that CNN makes. Authors find that predictions correspond to the winning board locations. Authors claim that this: 1. is a very interesting finding. 2. CNN has figured out game rules. 3. Cross modal supervision is applicable to higher-level semantics. I don*t think (2) be can be claimed because the knowledge of game rules is not tested by any experiment. There is only *one* stage of a game - i.e. last move that is considered. Further, the results are on the training set itself - the bare minimum requirement of any implicit or explicit representation of game rules is the ability to act in previously unseen states (i.e. generalization). Even if the CNN did generalize, I would avoid making any claims about knowledge of game rules. For (3), author*s definition of cross-modal seems to be training from images to games moves. In image-classification we go from images --> labels (i.e. between two different domains). We already know CNNs can perform such mappings. CNNs have been used to map images to actions such as in DQN my Mnih et al., or DDPG by Lillicrap et al. and a lot of other classical work such as ALVIN. It*s unclear what points authors are trying to make. For (1): how interesting is an implicit attention mechanism is a subjective matter. The authors claim a difference between the concepts of *what do do* and *what will happen*. They claim by supervising for *what will happen*, the CNN can automatically learn about *what to do*. This is extensively studied in the model predictive control literature. Where model is *what will happen next*, and the model is used to infer a control law - *what to do*. However, in the experimental setup presented in the paper what will happen and what to do seem to be the exact same things. For further analysis of what the CNN has learnt I would recommend: (a) Visualizing CAM with respect to incorrect classes. For eg, visualize the CAM with respect to player would lose (instead of winning). (b) Split the data into train/val and use the predictions on the val-set for visualization. These would be much more informative about what kind of *generalizable* features the CNN pays attention to. In summary, understanding why CNN*s make what decisions they make is a very interesting area of research. While the emergence of an implicit attention mechanism may be considered to be an interesting finding by some, many claims made by the authors are not supported by experiments (see comments above).

Summary === This paper presents tic-tac-toe as toy problem for investigating CNNs. A dataset is created containing tic-tac-toe boards where one player is one move away from winning and a CNN is trained to label boards according to (1) the player who can win (2 choices) and (2) the position they may move to win (9 choices), resulting in 18 labels. The CNN evaluated in this paper performs perfectly at the task and the paper*s goal is to inspect how the CNN works. The fundamental mechanism for this inspection is Class Activation Mapping (CAM) (Zhou et. al. 2016), which identifies regions of implicit attention in the CNN. These implicit attention maps (localization heat maps) are used to derive actions (which square each player should move). The attention maps (1) attend to squares in the tic-tac-toe board rather than arbitrary blobs, despite the fact that one square in a board has uniform color, and (2) they can be used to pick correct (winning) actions. This experiment are used to support assertions that the network understands (1) chess (tic-tac-toe) boards (2) a rule for winning tic-tac-toe (3) that there are two players. Some follow up experiments indicate similar results under various renderings of the tic-tac-toe boards and an incomplete training regime. More Clarifying Questions === * I am not quite sure precisely how CAM is implemented here. In the original CAM one must identify a class of interest to visualize (e.g., cat or dog). I don*t think this paper identifies such a choice. How is one of the 18 possible classes chosen for creating the CAM visualization and through that visualization choosing an action? * How was the test set for this dataset for the table 1 results created? How many of the final 1029 states were used for test and was the distribution of labels the same in train and test? * How is RCO computed? Is rank correlation or Pearson correlation used? If Pearson correlation is used then it may be good to consider rank correlation, as argued in *Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?* by Das et. al. in EMNLP 2016. In table 1, what does the 10^3 next to RCO mean? Pros === * The proposed method, deriving an action to take from the result of a visualization technique, is very novel. * This paper provides an experiment that clearly shows a CNN relying on context to make accurate predictions. * The use of a toy tic-tac-toe domain to study attention in CNNs (implicit or otherwise) is a potentially fruitful setting that may lead to better understanding of implicit and maybe explicit attention mechanisms. Cons === * This work distinguishes between predictions about *what will happen* (will the white player win?) and *what to do* (where should the white player move to win?). The central idea is generalization from *what will happen* to *what to do* indicates concept learning (sec. 2.1). Why should an ability to act be any more indicative of a learned concept than an ability to predict future states. I see a further issue with the presentation of this approach and a potential correctness problem: 1. (correctness) In the specific setting proposed I see no difference between *what to do* and *what will happen.* Suppose one created labels dictating *what to do* for each example in the proposed dataset. How would these differ from the labels of *what will happen* in the proposed dataset? In this case *what will happen* labels include both player identity (who wins) and board position (which position they move to win). Wouldn*t the *what to do* labels need to indicate board position? They could also chosen to indicate player identity, which would make them identical to the *what will happen* labels (both 18-way softmaxes). 2. (presentation) I think this distinction would usually be handled by the Reinforcement Learning framework, but the proposed method is not presented in that framework or related to an RL based approach. In RL *what will happen* is the reward an agent will receive for making a particular action and *what to do* is the action an agent should take. From this point of view, generalization from *what will happen* to *what to do* is not a novel thing to study. Alternate models include: * A deep Q network (Mnih. et. al. 2015) could predict the value of every possible action where an action is a (player, board position) tuple. * The argmax of the current model*s softmax could be used as an action prediction. The deep Q network approach need not be implemented, but differences between methods should be explained because of the uniqueness of the proposed approach. * Comparison to work that uses visualization to investigate deep RL networks is missing. In particular, other work in RL has used Simonyan et. al. (arXiv 2013) style saliency maps to investigate network behavior. For example, *Dueling Network Architectures for Deep Reinforcement Learning* by Wang et. al. in (ICML 2016) uses saliency maps to identify differences between their state-value and advantage networks. In *Graying the black box: Understanding DQNs* by Zahavy et. al. (ICML 2016) these saliency maps are also used to analyze network behavior. * In section 2.3, saliency maps of Simonyan et. al. are said to not be able to activate on grid squares because they have constant intensity, yet no empirical or theoretical evidence is provided for this claim. On a related note, what precisely is the notion of information referenced in section 2.3 and why is it relevant? Is it entropy of the distribution of pixel intensities in a patch? To me it seems that any measure which depends only on one patch is irrelevant because the methods discussed (e.g., saliency maps) depend on context as well as the intensities within a patch. * The presentation in the paper would be improved if the results in section 7 were presented along with relevant discussion in preceding sections. Overall Evaluation === The experiments presented here are novel, but I am not sure they are very significant or offer clear conclusions. The methods and goals are not presented clearly and lack the broader relevant context mentioned above. Furthermore, I find the lines of thought mentioned in the Cons section possibly incorrect or incomplete. As detailed with further clarifying questions, upon closer inspection I do not see how some aspects of the proposed approach were implemented, so my opinion may change with further details.

1029 tic-tac-toe boards are rendered (in various ways). These 1029 boards are legal boards where the next legal play can end the game. There are 18 categories of such boards -- 9 for the different locations of the next play, and 2 for the color of the next play. The supervision is basically saying *If you place a black square in the middle right, black will win* or *if you place a white square in the upper left, white will win*. A CNN is trained to predict these 18 categories and can do so with 100% accuracy. The focus of the paper is using Zhou et al*s Class Activation Mapping to show where the CNN focuses when making it*s decision. As I understand it, an input to CAM is the class of interest. So let*s say it is class 1 (black wins with a play to the bottom right square, if I*ve deciphered figure 2 correctly. Figure 2 should really be more clear about what each class is). So we ask CAM to determine the area of focus of the CNN for deciding whether class 1 is exhibited. The focus ends up being on the empty bottom right square (because certainly you can*t exhibit class 1 if the bottom right square is occupied). The CNN also needs to condition its decision on other parts of the board -- it needs to know whether there will be 3 in a row from some direction. But maybe that conditioning is weaker? That*s kind of interesting but I*m not sure about the deeper statements about discovering game rules that the paper hints at. I*m also not sure about the connection of this work to weakly supervised learning or multi-modal learning. The paper is pretty well written, overall, with some grammatical mistakes, but I simply don*t see the surprising discovery of this work. I also have some concerns about how contrived this scenario is -- using a big, expressive CNN for such a simple game domain and using a particular CNN visualization method. I am not an expert in reinforcement learning (which isn*t happening in this paper, but is in related works on CNN game playing), so maybe I*m not appreciating the paper appropriately.","This paper presents an interesting approach to learning chess game concepts under weak supervision using convolutional neural networks (CNNs). Here are the strengths and weaknesses of the paper:

**Strengths:**

1. **Novelty**: The paper explores a new and under-explored area of research, which is learning chess game concepts under weak supervision. This novelty can attract attention from researchers and practitioners in the field.
2. **Practicality**: The authors use a simple and well-known game, Tic-tac-toe, to demonstrate the feasibility of their approach. This makes the paper more accessible and easier to understand.
3. **Methodological contributions**: The paper discusses three important aspects of deep learning: (1) understanding the relationship between visual inputs and actions, (2) the applicability of cross-modal supervision, and (3) the limitations of class activation mapping. These discussions provide valuable insights into the strengths and weaknesses of these techniques.
4. **Availability of resources**: The authors offer pre-processed datasets and pre-trained models, which can be useful for future research and experimentation.

**Weaknesses:**

1. **Limited scope**: The paper focuses on a single game, Tic-tac-toe, which may not be representative of more complex games like chess.","Here are the strengths and weaknesses of this paper:

**Strengths:**

1. **Novelty**: The paper explores a new and under-explored area of research, which is the application of convolutional neural networks to learn chess game concepts under weak supervision. This novelty can attract attention from the research community and stimulate further investigation.
2. **Practicality**: The paper focuses on a simple and well-known game, tic-tac-toe, which makes it easier to implement and evaluate the proposed methodology. This practicality can help to demonstrate the effectiveness of the approach.
3. **Clear research questions**: The paper clearly articulates three research questions that are relevant to the field, which provides a clear direction for the investigation.
4. **Availability of resources**: The authors offer to release the codes, pre-processed datasets, and pre-trained models, which can facilitate replication and extension of the work.

**Weaknesses:**

1. **Limited scope**: The paper focuses on a single game, tic-tac-toe, which may not be representative of more complex games like chess. This limited scope may limit the generalizability of the results.
2. **Lack of theoretical foundation**: The paper does not provide a clear theoretical foundation for the proposed methodology"
"This paper proposes an unsupervised graph embedding learning method based on random walk and skip-thought model. They show promising results compared to several competitors on four chemical compound datasets. Strength: 1, The idea of learning the graph embedding by applying skip-thought model to random walk sequences is interesting. 2, The paper is well organized. Weakness: 1, As the current datasets are small (e.g., the average number of nodes per graph is around 30), it would be great to explore larger graph datasets to further investigate the method. 2, Comparisons with recent work like LINE and node2vec are missing. You can compare them easily by applying the same aggregation strategy to their node embeddings. Detailed Questions: 1, The description about how to split the random walk sequence into 3 sub-sequences is missing. Also, the line “l_min >= (n_k - 1), … >= l_max” in section 2.2.2 is a mistake. 2, Can you provide the standard deviations of the 5-fold cross validation in Table 2? I’m curious about how stable the algorithm is.

This paper studies the graph embedding problem by using the encoder-decoder method. The experimental study on real network data sets show the features extracted by the proposed model is good for classification. Strong points of this paper: 1. The idea of using the methods from natural language processing to graph mining is quite interesting. 2. The organization of the paper is clear Weak points of this paper: 1. Comparisons with state-of-art methods (Graph Kernels) is missing. 2. The problem is not well motivated, are there any application of this. What is the different from the graph kernel methods? The comparison with graph kernel is missing. 3. Need more experiment to demonstrate the power of their feature extraction methods. (Clustering, Search, Prediction etc.) 4. Presentation of the paper is weak. There are lots of typos and unclear statements. 5. The author mentioned about the graph kernel things, but in the experiment they didn*t compare them. Also, only compare the classification accuracy by using the proposed method is not enough.

Authors take the skip-graph architecture (Kiros 2015) and apply it to classifying labeled graphs (molecular graphs). They do it by creating many sentences by walking the graph randomly, and asking the model to predict previous part and next part from the middle part. Activations of the decoder part of this model on a walk generated from a new graph are used as features for a binary classifier use to predict whether the molecule has anti-cancer properties. Paper is well written, except that evaluation section is missing details of how the embedding is used for actual classification (ie, what classifier is used) Unfortunately I*m not familiar with the dataset and how hard it is to achieve the results they demonstrate, that would be the important factor to weight on the papers acceptance.

The paper presents a method to learn graph embeddings in a unsupervised way using random walks. It is well written and the execution appears quite accurate. The area of learning whole graph representations does not seem to be very well explored in general, and the proposed approach enjoys having very few competitors. In a nutshell, the idea is to linearize the graph using random walks and to compute the embedding of the central segment of each walk using the skip-thought criterion. Being not an expert in biology, I can not comment whether or not this makes sense, but the gains reported in Table 2 are quite significant. An anonymous public comment compared this work to a number of others in which the problem of learning representations of nodes is considered. While this is arguably a different goal, one natural baseline would be to pool these representations using mean- or max- pooling. It would very interesting to do such a comparison, especially given that the considered approach heavily relies on pooling (see Figure 3(c)) To sum up, I think it is a nice paper, and with more baselines I would be ready to further increase the numerical score.","Here are the strengths and weaknesses of the paper ""Skip-graph: Learning graph embeddings with an encoder-decoder model"" based on the provided abstract:

**Strengths:**

1. **Novel approach**: The paper proposes a novel method for learning graph embeddings using an encoder-decoder model, which is a departure from the traditional supervised techniques used in graph learning.
2. **Generic feature representation**: The proposed method can learn a generic feature representation for graphs, which can be applied to various graph classification tasks.
3. **Evaluation on real-world datasets**: The paper evaluates the proposed model on several real-world datasets, demonstrating its effectiveness against state-of-the-art techniques.
4. **Applicability to graph classification**: The method can be applied to various graph classification tasks, making it a versatile approach.

**Weaknesses:**

1. **Lack of interpretability**: The paper does not provide much insight into the learned feature representations, making it difficult to understand the underlying structure of the graph embeddings.
2. **Assumes a specific task**: The proposed method is specifically designed for graph classification, which may limit its applicability to other graph-based tasks.
3. **Random walk sequence prediction**: The method relies on predicting a random walk sequence, which may not be a realistic assumption","Here are the strengths and weaknesses of the paper:

**Strengths:**

1. **Novel approach**: The paper proposes a novel approach to learning graph embeddings using an encoder-decoder model, which is a departure from the traditional supervised methods used in graph classification tasks.
2. **Generic feature representation**: The proposed method can learn a generic feature representation for graphs, which can be used for a wide range of graph classification tasks.
3. **Evaluation on real-world datasets**: The paper evaluates the proposed method on several real-world datasets, which provides a good indication of its effectiveness.
4. **Comparison to state-of-the-art techniques**: The paper compares the proposed method to state-of-the-art techniques, which provides a clear understanding of its performance relative to existing methods.

**Weaknesses:**

1. **Lack of interpretability**: The paper does not provide much insight into the interpretability of the learned graph embeddings. It is unclear how the learned representations capture the structural and functional similarities between subgraphs.
2. **Limited experimental results**: The paper only evaluates the proposed method on a few datasets, which may not be representative of the overall performance on more diverse datasets.
3. **No discussion of limitations**: The paper does not discuss the limitations of the proposed method, such"
"The paper studies the impact of using customized number representations on accuracy, speed, and energy consumption of neural network inference. Several standard computer vision architectures including VGG and GoogleNet are considered for the experiments, and it is concluded that floating point representations are preferred over fixed point representations, and floating point numbers with about 14 bits are sufficient for the considered architectures resulting in a small loss in accuracy. The paper provides a nice overview of floating and fixed point representations and focuses on an important aspect of deep learning that is not well studied. There are several aspects of the paper that could be improved, but overall, I am leaned toward weak accept assuming that the authors address the issues below. 1- The paper is not clear that it is only focusing on neural network inference. Please include the word *inference* in the title / abstract to clarify this point and mention that the findings of the paper do not necessarily apply to neural network training as training dynamics could be different. 2- The paper does not discuss the possibility of adopting quantization tricks during training, which may result in the use of fewer bits at inference. 3- The paper is not clear whether in computing the running time and power consumption, it includes all of the modules or only multiply-accumulate units? Also, how accurate are these numbers given different possible designs and the potential difference between simulation and production? Please elaborate on the details of simulation in the paper. 4- The whole discussion about *efficient customized precision search* seem unimportant to me. When such important hardware considerations are concerned, even spending 20x simulation time is not that important. The exhaustive search process could be easily parallelized and one may rather spend more time at simulation at the cost of finding the exact best configuration rather than an approximation. That said, weak configurations could be easily filtered after evaluating just a few examples. 5- Nvidia*s Pascal GP100 GPU supports FP16. This should be discussed in the paper and relevant Nvidia papers / documents should be cited. More comments: - Parts of the paper discussing *efficient customized precision search* are not clear to me. - As future work, the impact of number representations on batch normalization and recurrent neural networks could be studied.

This paper explores the performance-area-energy-model accuracy tradeoff encountered in designing custom number representations for deep learning inference. Common image-based benchmarks: VGG, Googlenet etc are used to demonstrate that fewer than1 6 bits in a custom floating point representation can lead to improvement in runtime performance and energy efficiency with only a small loss in model accuracy. Questions: 1. Does the custom floating point number representation take into account support for de-normal numbers? 2. Is the custom floating point unit clocked at the same frequency as the baseline 32-bit floating point unit? If not, what are the different frequencies used and how would this impact the overall system design in terms of feeding the data to the floating point units from the memory Comments: 1. I would recommend using the IEEE half-precision floating point (1bit sign, 5bit exponent, and 10bit mantissa) as a baseline for comparison. At this point, it is well known in both the ML and the HW communities that 32-bit floats are an overkill for DNN inference and major HW vendors already include support for IEEE half-precision floats. 2. In my opinion, the claim that switching to custom floating point lead to a YY.ZZ x savings in energy is misleading. It might be true that the floating-point unit itself might consume less energy due to smaller bit-width of the operands, however a large fraction of the total energy is spent in data movement to/from the memories. As a result, reducing the floating point unit’s energy consumption by a certain factor will not translate to the same reduction in the total energy. A reader not familiar with such nuances (for example a typical member of the ML community), may be mislead by such claims. 3. On a similar note as comment 2, the authors should explicitly mention that the claimed speedup is that of the floating point unit only, and it will not translate to the overall workload speedup. Although the speedup of the compute unit is roughly quadratic in the bit-width, the bandwidth requirements scale linearly with bit-width. As a result, it is possible that these custom floating point units may be starved on memory bandwidth, in which case the claims of speedup and energy savings need to be revisited. 4. The authors should also comment on the complexities and overheads introduced in data accesses, designing the various system buses/ data paths when the number representation is not byte-aligned. Moving to a custom 14-bit number representation (for example) can improve the performance and energy-efficiency of the floating point unit, but these gains can be partially eroded due to the additional overhead in supporting non-byte aligned memory accesses.

The paper provides a first study of customized precision hardware for large convolutional networks, namely alexnet, vgg and googlenet. It shows that it is possible to achieve larger speed-ups using floating-point precision (up to 7x) when using fewer bits, and better than using fixed-point representations. The paper also explores predicting custom floating-point precision parameters directly from the neural network activations, avoiding exhaustive search, but i could not follow this part. Only the activations of the last layer are evaluated, but on what data ? On all the validation set ? Why would this be faster than computing the classification accuracy ? The results should be useful for hardware manufacturers, but with a catch. All popular convolutional networks now use batch normalization, while none of the evaluated ones do. It may well be that the conclusions of this study will be completely different on batch normalization networks, and fixed-point representations are best there, but that remains to be seen. It seems like something worth exploring. Overall there is not a great deal of novelty other than being a useful study on numerical precision trade-offs at neural network test time. Training time is also something of interest. There are a lot more researchers trying to train new networks fast than trying to evaluate old ones fast. I am also no expert in digital logic design, but my educated guess is that this paper is marginally below the acceptance threshold.","Here are the strengths and weaknesses of this paper:

**Strengths:**

1. **Practical contribution**: The paper presents a practical contribution to the field of deep learning, exploring the implications of numeric representation and precision on computational efficiency.
2. **Experimental results**: The authors provide concrete experimental results, demonstrating the effectiveness of their approach on two popular DNN models (GoogLeNet and VGG) and a baseline platform.
3. **Novel technique**: The paper introduces a novel technique for deriving optimal precision configurations, which can be useful for future DNN platforms.
4. **Relevance to industry**: The work is relevant to the industry, as it addresses the growing demand for efficient DNNs in production-grade applications.

**Weaknesses:**

1. **Limited scope**: The paper focuses on a specific aspect of DNNs (numeric representation and precision) and may not provide a comprehensive understanding of the broader implications of DNNs.
2. **Assumptions**: The authors assume that the benefits of narrow-precision representations will be significant, but they do not provide a thorough analysis of the trade-offs between precision, accuracy, and computational efficiency.
3. **Lack of theoretical foundation**: The paper does not provide a theoretical foundation for the choice of","Here are the strengths and weaknesses of this paper:

**Strengths:**

1. **Relevant to current trends**: The paper addresses a pressing issue in deep learning, which is the increasing computational demand and the need for more efficient models.
2. **Practical experimentation**: The authors provide experimental results on real-world models (GoogLeNet and VGG) to demonstrate the effectiveness of their approach.
3. **Novel technique**: The paper introduces a new technique for deriving the optimal precision configuration, which is a valuable contribution to the field.
4. **Clear goals and objectives**: The authors clearly state their goals and objectives, making it easy to understand the paper's purpose and scope.

**Weaknesses:**

1. **Limited scope**: The paper focuses on a specific aspect of deep learning (numeric representation and precision) and may not address other important factors that affect model performance, such as architecture, training data, or optimization algorithms.
2. **Assumptions about hardware**: The paper assumes that the custom numeric representations will be implemented on production-grade hardware, which may not be the case for all environments (e.g., edge devices or embedded systems).
3. **Lack of theoretical foundations**: The paper does not provide a thorough theoretical analysis of the implications of"
"The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition. quality: I am missing a more thorough evaluation of the fixation prediction performance. The center bias performance in Table 1 differs significantly from the on in Table 2. All the state-of-the-art models reported in Table 2 have a performance worse than the center bias performance reported in Table 1. Is there really no other model better than the center bias? Additionally I am missing details on how central bias and human performance are modelled. Is human performance cross-validated? You claim that your *results are very close to human performance (the difference is only 3.2%). This difference is actually larger than the difference between Central Bias and your model reported in Table 1. Apart from this, it is dangerous to compare AUC performance differences due to e.g. saturation issues. clarity: the explanation for Table 3 is a bit confusing, also it is not clear why the CONV5 and the FC6 models differ in how the saliency map is used. At least one should also evaluate the CONV5 model when multiplying the input with the saliency map to see how much of the difference comes from the different ways to use the saliency map and how much from the different features. Other issues: You cite Kümmerer et. al 2015 as a model which *learns ... indirectly rather than from explicit information of where humans look*, however the their model has been trained on fixation data using maximum-likelihood. Apart from these issues, I think the paper make a very interesting contribution to spatio-temporal fixation prediction. If the evaluation issues given above are sorted out, I will happily improve my rating.

This work proposes to a spatiotemporal saliency network that is able to mimic human fixation patterns, thus helping to prune irrelevant information from the video and improve action recognition. The work is interesting and has shown state-of-the-art results on predicting human attention on action videos. It has also shown promise for helping action clip classification. The paper would benefit from a discussion on the role of context in attention. For instance, if context is important, and people give attention to context, why is it not incorporated automatically in your model? One weak point is the action recognition section, where the comparison between the two (1)(2) and (3) seems unfair. The attention weighted feature maps in fact reduce the classification performance, and only improve performance when doubling the feature and associated model complexity by concatenating the weighted maps with the original features. Is there a way to combine the context and attention without concatenation? The rational for concatenating the features extracted from the original clip, and the features extracted from the saliency weighted clip seems to contradict the initial hypothesis that `eliminating or down-weighting pixels that are not important* will improve performance. The authors should also mention the current state-of-the-art results in Table 4, for comparison. # Other comments: # Abstract - Typo: `mixed with irrelevant ...* ``Time consistency in videos ... expands the temporal domain from few frames to seconds** - These two points are not clear, probably need a re-write. # Contributions - 1) `The model can be trained without having to engineer spatiotemporal features* - you would need to collect training data from humans though.. # Section 3.1 The number of fixation points is controlled to be fixed for each frame - how is this done? In practice we freeze the layers of the C3D network to values pretrained by Tran etal. What happens when you allow gradients to flow back to the C3D layers? Is it not better to allow the features to be best tuned for the final task? The precise way in which the features are concatenated needs to be clarified in section 3.4. Minor typo: `we added them trained central bias*

This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet (in particular, C3D) to extract visual features. The visual features are then passed to LSTM. The hidden state at each time step in LSTM is used to generate the parameters in a Gaussian mixture model. Finally, the visual attention map is generated from the Gaussian mixture model. Overall, the idea in this paper is reasonable and the paper is well written. RNN/LSTM has been used in lots of vision problem where the outputs are discrete sequences, there has not been much work on using RNN/LSTM for problems where the output is continuous like in this paper. The experimental results have demonstrated the effectiveness of the proposed approach. In particular, it outperforms other state-of-the-art on the saliency prediction task on the Hollywood2 datasets. It also shows improvement over baselines (e.g. C3D + SVM) on the action recognition task. My only *gripe* of this paper is that this paper is missing some important baseline comparisons. In particular, it does not seem to show how the *recurrent* part help the overall performance. Although Table 2 shows RMDN outperforms other state-of-the-art, it might be due to the fact that it uses strong C3D features (while other methods in Table 2 use traditional handcrafted features). Since saliency prediction is essentially a dense image labeling problem (similar to semantic segmentation). For dense image labeling, there has been lots of methods proposed in the past two years, e.g. fully convolution neural network (FCN) or deconvnet. A straightforward baseline is to simply take FCN and apply it on each frame. If the proposed method still outperforms this baseline, we can know that the *recurrent* part really helps.","Here are the strengths and weaknesses of the paper:

**Strengths:**

1. **Novel approach**: The paper proposes a novel approach to spatiotemporal attention, which is a significant contribution to the field of computer vision.
2. **Human fixation data**: The use of human fixation data as training data is a unique aspect of the paper, which allows the model to learn attentional patterns from human behavior.
3. **Hierarchical temporal modeling**: The proposed model uses a hierarchical approach to model time consistency, which is a common challenge in spatiotemporal attention.
4. **State-of-the-art performance**: The paper reports state-of-the-art performance on the Hollywood2 dataset, which is a significant achievement.
5. **Generalization to other datasets**: The model's ability to generalize to other datasets, such as UCF101, is a testament to its robustness and effectiveness.

**Weaknesses:**

1. **Limited evaluation**: The paper only evaluates the model on two datasets, Hollywood2 and UCF101. It would be beneficial to evaluate the model on more diverse datasets to assess its generalizability.
2. **Lack of interpretability**: The paper does not provide much insight into the learned attentional patterns or the features that","Here are the strengths and weaknesses of the paper:

**Strengths:**

1. **Novel approach**: The paper proposes a novel approach to spatiotemporal visual attention, which is a significant contribution to the field of computer vision.
2. **Human fixation data**: The use of human fixation data as training data is a unique aspect of the paper, which allows the model to learn attentional patterns that are grounded in human perception.
3. **Hierarchical temporal modeling**: The proposed model's hierarchical temporal modeling approach is a significant improvement over previous methods, which often rely on simple temporal convolutional layers.
4. **State-of-the-art performance**: The paper reports state-of-the-art performance on the Hollywood2 dataset, which is a significant achievement.
5. **Generalization to other datasets**: The model's ability to generalize to other datasets, such as UCF101, is a strong indication of its robustness and versatility.

**Weaknesses:**

1. **Lack of interpretability**: The paper does not provide much insight into the attentional mechanisms learned by the model, which makes it difficult to understand why the model is attending to certain parts of the image or video.
2. **Over-reliance on human fixation data**: The use of human"
"The authors introduce some new prior and approximate posterior families for variational autoencoders, which are compatible with the reparameterization trick, as well as being capable of expressing multiple modes. They also introduce a gating mechanism between prior and posterior. They show improvements on bag of words document modeling, and dialogue response generation. The original abstract is overly strong in its assertion that a unimodal latent prior p(z) cannot fit a multimodal marginal int_z p(x|z)p(x)dz with a DNN response model p(x|z) (*it cannot possibly capture more complex aspects of the data distribution*, *critical restriction*, etc). While the assertion that a unimodal latent prior is necessary to model multimodal observations is false, there are sensible motivations for the piecewise constant prior and posterior. For example, if we think of a VAE as a sort of regularized autoencoder where codes are constrained to *fill up* parts of the prior latent space, then there is a sphere-packing argument to be made that filling a Gaussian prior with Gaussian posteriors is a bad use of code space. Although the authors don*t explore this much, a hypercube-based tiling of latent code space is a sensible idea. As stated, I found the message of the paper to be quite sloppy with respect to the concept of *multi-modality.* There are 3 types of multimodality at play here: multimodality in the observed marginal distribution p(x), which can be captured by any deep latent Gaussian model, multimodality in the prior p(z), which makes sense in some situations (e.g. a model of MNIST digits could have 10 prior modes corresponding to latent codes for each digit class), and multimodality in the posterior z for a given observation x_i, q(z_i|x_i). The final type of multimodality is harder to argue for, except in so far as it allows the expression of flexibly shaped distributions without highly separated modes. I believe flexible posterior approximations are important to enable fine-grained and efficient tiling of latent space, but I don*t think these need to have multiple strong modes. I would be interested to see experiments demonstrating otherwise for real world data. I think this paper should be more clear about the different types of multi-modality and which parts of their analysis demonstrate which ones. I also found it unsatisfactory that the piecewise variable analysis did not show different components of the multi-modal prior corresponding to different words, but rather just a separation between the Gaussian and the piecewise variables. As I mention in my earlier questions, I found it surprising that the learned variance and mean for the Gaussian prior helps so dramatically with G-NVDM likelihood when the powerful networks transforming to and from latent space should make it scale-invariant. Explicitly separating out the contributions of a reimplemented base model, prior-posterior interpolation and the learned prior parameters would strengthen these experiments. Overall, the very strong improvements on the text modeling task over NVDM seem hard to understand, and I would like to see an ablation analysis of all the differences between that model and the proposed one. The fact that adding more constant components helps for document modeling is interesting, and it would be nice to see more qualitative analysis of what the prior modes represent. I also would be surprised if posterior modes were highly separated, and if they were it would be interesting to explore if they corresponded to e.g. ambiguous word-senses. The experiments on dialog modeling are mostly negative results, quantitatively. The observation that the the piecewise constant variables encode time-related words and the Gaussian variables encode sentiment is interesting, especially since it occurs in both sets of experiments. This is actually quite interesting, and I would be interested in seeing analysis of why this is the case. As above, I would like to see an analysis of the sorts of words that are encoded in the different prior modes and whether they correspond to e.g. groups of similar holidays or days. In conclusion, I think the piecewise constant variational family is a good idea, although it is not well-motivated by the paper. The experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in G-NVDM. The fact that H-NVDM performs better is interesting, though. This paper should better motivate the need for different types of multi-modality, and demonstrate that those sorts of things are actually being captured by the model. As it is, the paper introduces an interesting variational family and shows that it performs better for some tasks, but the motivation and analysis is not clearly focused. To demonstrate that this is a broadly applicable family, it would also be good to do experiments on a more standard datasets like MNIST. Even without an absolute log-likelihood improvement, if the method yielded interpretable multiple modes this would be a valuable contribution.

This paper proposes a piecewise constant parameterisation for neural variational models so that it could explore the multi-modality of the latent variables and develop more powerful neural models. The experiments of neural variational document models and variational hierarchical recurrent encoder-decoder models show that the introduction of the piecewise constant distribution helps achieve better perplexity on modelling documents and seemly better performance on modelling dialogues. The idea of having a piecewise constant prior for latent variables is interesting, but the paper is not well-written (even 14 pages long) and the design of the experiments fails to demonstrate the most of the claims. The detailed comments are as follows: --The author explains the limitations of the VAEs with standard Gaussian prior in the last paragraph of 3.1 and the last paragraph of 5.1. Hence, a multimodal prior would help the VAEs overcome the issues of optimisation. However, there is a lack of evidence showing the multimodality of the prior helps break the bottleneck. --In the last paragraph of 6.1, the author claimed the decoder parameter matrix is directly affected by the latent variables. But what the connects the decoder is a combination of a piecewise constant and Gaussian latent variables. No matter what is discovered in the experiments, it only shows z=<z_gaussian, z_piecewise> is multimodal. However, z=<z_gaussian1, z_gaussian2> can be multimodal as well. None of the claims in this paragraph stands. --In the quantitative evaluation of NVDM, there is an incremental model from z=z_gaussian to z=<z_gaussian, z_piecewise>. As the prior is learned together with the variational posterior, a more flexible prior would alleviate the regularisation imposed by the KL term. Certainly, more parameters are applied as well, so a fair comparison would at least be z=<z_gaussian, z_piecewise> and z=<z_gaussian1, z_gaussian2> which equals to a double sized z_gaussian. --The results shown in Table 3 are implausible. I cannot believe the author used gradients to evaluate the model. --Eq. 5 is confusing, adding a multiplication sign might help. --3.1 can be deleted because people attending ICLR are familiar with VAEs. Typos: as well as the well as the generated prior-> as well as the generated prior

UPDATE: I have read the authors* rebuttal and also the other comments in this paper*s thread. My thoughts have not changed. The authors propose using a mixture prior rather than a uni-modal prior for variational auto-encoders. They argue that the simple uni-modal prior *hinders the overall expressivity of the learned model as it cannot possibly capture more complex aspects of the data distribution.* I find the motivation of the paper suspicious because while the prior may be uni-modal, the posterior distribution is certainly not. Furthermore, a uni-modal distribution on the latent variable space can certainly still lead to the capturing of complex, multi-modal data distributions. (As the most trivial case, take the latent variable space to be a uniform distribution; take the likelihood to be a point mass given by applying the true data distribution*s inverse CDF to the uniform. Such a model can capture any distribution.) In addition, multi-modality is arguably an overfocused concept in the literature, where the (latent variable) space is hardly anymore worth capturing from a mixture of simple distributions when it is often a complex nonlinear space. It is unclear from the experiments how much the influence of the prior*s multimodality influences the posterior to capture more complex phenomena, and whether this is any better than considering a more complex (but still reparameterizable) distribution on the latent space. I recommend that this paper be rejected, and encourage the authors to more extensively study the effect of different priors. I*d also like to make two additional comments: While there is no length restriction at ICLR, the 14 page document can be significantly condensed without loss of describing their innovation or clarity. I recommend the authors do so. Finally, I think it*s important to note the controversy in this paper. It was submitted with many significant incomplete details (e.g., no experiments, many missing citations, a figure placed inside that was pencilled in by hand, and several missing paragraphs). These details were not completed until roughly a week(?) later. I recommend the chairs discuss this in light of what should be allowed next year.","**Strengths:**

1. **Addressing a significant limitation**: The paper identifies a significant limitation of existing variational autoencoders (VAEs) and proposes a solution to overcome it, which is a major strength.
2. **Practical application**: The authors demonstrate the effectiveness of the proposed prior distribution in several natural language processing (NLP) tasks, such as document modeling and dialogue modeling, which is a practical application of the technique.
3. **Flexibility**: The proposed prior distribution is flexible and can be learned efficiently, which is a desirable property in machine learning models.
4. **Relevance to real-world problems**: The paper addresses a real-world problem (complex and multi-modal distributions) and provides a solution that can be applied to various NLP tasks.

**Weaknesses:**

1. **Lack of theoretical justification**: The paper does not provide a rigorous theoretical justification for the proposed prior distribution. While it is mentioned that the prior can capture an exponential number of modes, the underlying mathematical framework is not fully explained.
2. **Limited experimental results**: The paper presents results for only two NLP tasks (document modeling and dialogue modeling), which may not be representative of the broader applicability of the proposed method.
3. **Assumptions about","Here are the strengths and weaknesses of this paper:

**Strengths:**

1. **Novelty**: The paper proposes a new prior distribution that can capture complex and multi-modal distributions, which is a significant contribution to the field of variational inference.
2. **Applicability**: The proposed prior can be applied to various natural language processing tasks, such as document modeling and dialogue modeling, which is a broad and relevant application area.
3. **Flexibility**: The multi-modal variational encoder-decoder framework is flexible and can be used to model complex distributions, which is a key advantage over traditional uni-modal priors.
4. **Efficiency**: The paper suggests that the proposed prior can be learned efficiently, which is an important consideration in large-scale machine learning applications.

**Weaknesses:**

1. **Lack of experimental results**: The paper does not provide detailed experimental results to support the effectiveness of the proposed prior distribution. It would be helpful to see more concrete evidence of the prior's ability to capture complex distributions.
2. **Limited scope**: The paper focuses on natural language processing tasks, which may limit its applicability to other domains. It would be interesting to see if the proposed prior can be applied to other areas, such as image or speech processing"
